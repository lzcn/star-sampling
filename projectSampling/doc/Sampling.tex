\documentclass[letterpaper]{article}
% Required Packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
%%%%%%%%%%
% PDFINFO for PDFLATEX
% Uncomment and complete the following for metadata
\pdfinfo{
/Title (Core Sampling for Top t Retrial Processing)
/Author (Zhi Lu)
/Keywords ()
}
\title{Core Sampling for Top t Retrial Processing}
\author{}
%
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm,amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage[mathscr]{eucal}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfigure}
% for algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
\newcommand{\Scah}[3]{h^{(#1,#2)}_{#3}(\V{x}_{i_#1})}
%% -------
%% Tensor
%% -------
\newcommand{\T}[1]{\boldsymbol{\mathscr{\MakeUppercase{#1}}}}

\newcommand{\KT}[1]{\left\llbracket #1 \right\rrbracket}

%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\bm{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\VnC}[3]{\V{#1}^{(#2)}_{#3}}
% norm one of r-th cloumn
\newcommand{\Nrocl}[2]{\norm{\VnC{a}{#1}{*#2}}{1}}

\newcommand{\Varow}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\Vacol}[1]{\V{a}^{(#1)}_{*r}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\bm{\mathbf{\MakeUppercase{#1}}}}}
%Matrix with superscript
\newcommand{\Mn}[2]{\M{#1}^{(#2)}}

% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}

%% ------------
%% reference
%% ------------

% reference:definition
\newcommand{\Def}[1]   {Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]   {Equation~\ref{eq:#1}}
% reference:figure
\newcommand{\Fig}[1]   {Figure~\ref{fig:#1}}
\newcommand{\Figs}[2]  {Figure~\ref{fig:#1}$\sim$~\ref{fig:#2}}
% reference:table
\newcommand{\Table}[1] {Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]  {Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1] {Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1] {Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1] {Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newcommand{\Coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WreightR}{\Nrocl{1}{r}\ldots\Nrocl{N}{r}}
\newcommand{\predx}{\widehat{x}_{\V{i}}}
\newcommand{\predxn}{\widehat{x}_{\V{i},n}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
\maketitle

\section{Introduction}

In recommendation system,
it is an essential task to find the most relevant items when a specific user is given.
Since the item size is always in vast scale, it demands a fast retrial algorithm.
Generally, all users and items are trained to a joint latent spacea with the same dimensionality.
The most widely used technique is based on the factorization framework,
like the matrix factorization framework\cite{KoYe09}.
Matrix factorization is a way to map both users and items to the latent space
that modeling the user-item interaction with the inner products.
For multi-items recommendation system, the factorization model is then tensoer based.
Take the personalized tag recommendation system for example,
many facotrization models have been used to tackle the interaction
between the tag and post(the user-item pair),
like Higher-Order-Singular-Value-Decomposition(HOSVD) model\cite{SyNa08},
Tucker Decomposition(TD) model\cite{Rendle_RTF} and
Pairwise-Interaction-Tensor-Factorization(PITF) model\cite{Rendle_PITF}.
Meantime, fashion outfit recommendation\cite{HuYiLa15} also utilizes the factorization model to
map user and $N$ categories of items into latent sapce.

In those works, after factor matrices(a set of user or item vectors in latent space) have been trained,
finding the top-$t$ mostly related tuples is still a time-consuming task.
We study those tensor factorization models and
proposed a sampling-based origthm to estimate the top-$t$ most relevant targerts.
The tensor factorization we considered is the CANDECOMP/PARAFAC decomposition\cite{KoBa09},
since the PITF is a special case of the CP decomposition
and TD factorization take so much computations that used less at present.
We generalzie our algorithm to search in all-tuples ,
in which given a particular user or post is a degeneration.
The problem we summarized is called the top-t retrial problem:

\begin{definition}\label{def:DefinitionTopt}
(top-t Retrial.) Suppose $S_1,S_2,..S_N$ are $N$ different categories.
Vectors $\V{t}_{i_1},\V{t}_{i_2},...,\V{t}_{i_N}$ are instances
in each category with same dimensionality.
Under the interaction function $f(\V{t}_{i_1},\V{t}_{i_2},...,\V{t}_{i_N})$,
find $t$ tuples $(i_1,i_2,...,i_N)$ that have the largest value over all.
\end{definition}

The prediction $f$ is made by multiplying over all feature matrices.
\begin{equation}\label{eq:CPDecomposition}
    \T{X} := \Mn{A}{1}\times_{L_2}\Mn{A}{N}\cdots\times_{L_N}\Mn{A}{N}
\end{equation}
where $\T{X}$ the $N$-order prediction tensor with size $L_1\times L_2\times\cdots\times L_N$ and
\[
\M{A}^{(n)} =
\begin{bmatrix}
    \VnC{a}{n}{1},\VnC{a}{n}{2},\cdots,\VnC{a}{n}{r}
\end{bmatrix}  \in R^{L_n\times R}
\]
the factor matrix. The column size $R$ represents the dimensionality of latent space.
When factor matrices are trained, the prediction $x_\V{i}$ is:
\begin{equation}\label{eq:ValueInTensor}
x_\V{i} = \sum_{r=1}^{R}\Sca{a}{1}{r}\cdot\Sca{a}{2}{r}\cdots\Sca{a}{N}{r}
\end{equation}
where $\V{i}$ is a shorthand for multi-index $\Coord$.
Our work is inspired by the Maximum All-pairs Dot-product(MAD) search~\cite{BaPiKoSe15}.

\begin{table}[t]
  \label{table:Notation}
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Notations & Explanation \\
    \hline
    $\T{A}$ & tensor \\
    $\M{A}$ & matrix \\
    $\Mn{A}{n}$ & $n$-th factor matrix of tensor\\
    $\V{A}_{*r},\V{A}_r$ & $k$-th column of matrix \\
    $\V{A}_{i*}$ & $i$-th row of matrix \\
    $\V{A}$ & vector \\
    $a_{ir}$ & element of matrix, a scalar\\
    \hline
  \end{tabular}
  \caption{Notation}
\end{table}

\subsection{Notations}

We use the norm $\norm{*}{1}$ operation refereed in \cite{BaPiKoSe15}.
For a vector $\V{v}\in R^n$ or a matrix $\M{M}\in R^{m\times n}$.
It is defined as:
\[
    \norm{\V{v}}{1} = \sum_{i=1}^{n}|v_i|
    \ \  {\rm and} \ \
    \norm{\M{M}}{1} = \sum_{i=1}^{m}\sum_{j=1}^{n}|m_{ij}|
\]
Some frequently used notations are listed in ~\Table{Notation}.


\section{Related work}
If we restrict the number of factor matrices to $2$,
it becomes the matrix multiplication task.
The state of art\cite{BaPiKoSe15} is in MAD search.
And Maximum Inner Product Search(MIPS)\cite{Cohen97,Ram12}
is a special case of MAD search.
Howere finding the top-$t$ tuples based on CP factorization model is not well studied yet.
We study those similar works with probabilistic method that
to sample the coordinate $\V{i}$  proportionating to value of $x_{\V{i}}$.
CP factorization is the special case of TD factorization with a all-one diagonal core tensor.
Borrowing this concept, we proposed the core sampling to handle the CP model.

\section{Core Sampling}
All factor matrices are adjoin to the latent space.
So we represent the latent space by $R$ vertexes, and call it the core partition.
It is derived by the graph presentation of matrices.

\subsection{Graph Presentation of Factor Matrices}
Firstly, consider a matrix $\M{A}$ in size $M \times N$,
it is presented by a weighted bipartite graph shown in ~\Fig{GraphMatrix}.
The left partition has $M$ vertexes called $v^l_m$ and the right $N$ called $v^r_n$.
The weight of edge $e_{mn} = v^l_mv^r_n$ equals to $a_{mn}$.
Further, the $N$ factor matrices in ~\Eqn{CPDecomposition}
have the same latent space dimensionality,
for which we use only one partition called core partition to represent.
So those $N$ factor matrices can be represented by a weighted (N+1)-partite graph
that shown in ~\Fig{GraphMatrices}.
The vertexes in core partition are called $v^c_r$,
and the other partitions $v^{n}_{i_n}$ respectively.
Similarly, the weight of edge $e_{i_nr} = v^{n}_{i_n}v^c_r$ equals to $a^{(n)}_{i_nr}$.
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{fig_graph_matrix}\\
  \caption{Graph Presentation of Matrix $\M{A} \in R^{M \times N}$}
  \label{fig:GraphMatrix}
\end{figure}
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{fig_graph_factor_matrices}\\
  \caption{Graph Presentation of Factor Matrices}
  \label{fig:GraphMatrices}
\end{figure}
\subsection{Sampling Mechanism}

We start our sampling algorithm from core partition
by assigning a particular value to each vertexes.
In each one round, we sample the vertexes $v^c_r$ according to the value,
after which $N$ vertexes $v^{n}_{i_n}$ from other partitions are sampled
based on the weight of edge.
At the end of each round,
we assign a particular score to the coordinate $\Coord$ occurred.

\begin{algorithm}[t]
    \caption{Core Sampling with factor matrixes}
    \label{alg:CoreSampling}
    Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples.
    \begin{algorithmic}[1]
    \For{$r\in{1,2,\ldots,R}$}
    \State $w_r \leftarrow \Nrocl{1}{r}\norm{\Vacol{2}}{1}\ldots\Nrocl{N}{r}$
    \label{line:Weight}
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
    \label{line:CorePartition}
    \For {$n = 1,...,N$}
    \label{line:ItemPartitionFor}
    \State Sample $i_n$ with probability $|\Sca{a}{n}{r}|/\norm{\Vacol{n}}{1}$
    \EndFor
    \label{line:ItemPartitionEnd}
    \State
    \label{line:Scoring}
        $\M{X}_{\V{i},\ell} \leftarrow sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})$
    \If {$\V{i}=\Coord$ has not been sampled}
    \State  Creat $\predx \leftarrow \M{X}_{\V{i},\ell} $
    \Else
    \State $\predx \leftarrow \predx + \M{X}_{\V{i},\ell}$
    \EndIf
    \EndFor
    \label{line:ScoringEnd}
    \end{algorithmic}
\end{algorithm}

\subsubsection{Probabilities}
The weights we designed for vertexes in core partition are:
\[
    w_r = \WreightR
\]
shown in ~\AlgLine{CoreSampling}{Weight}.

In each round, we sample the index $r$
with probability $w_r/\norm{\V{w}}{1}$
(~\AlgLine{CoreSampling}{CorePartition}).
Then repeat $N$ times,
walk form the vertex $v^c_r$ to other partitions with probability
$|\Sca{a}{n}{r}|/\norm{\Vacol{n}}{1}$
(~\AlgLines{CoreSampling}{ItemPartitionFor}{ItemPartitionEnd}).

\subsubsection{Scores}
We stress that the score we designed is used for estimation.
However, for finding task, the step of scoring is redundance.
Instead, we just record the coordinate that has been sampled.

We use scores the for completeness of our algrothms.
Each time a coordinate $\Coord $ occurred, a score of is assigneg to it.
\[
\M{X}_{\V{i},\ell}  = sgn(\Sca{a}{1}{r}\cdot\Sca{a}{2}{r}\cdots\Sca{a}{N}{r})
\]
where $\ell$ is the iteration.
If this coordinate has not been sampled previously,
create a container $\predx = \M{X}_{\V{i},\ell}$.
Otherwise, increase $\predx$ by $\M{X}_{\V{i},\ell}$
(~\AlgLines{CoreSampling}{Scoring}{ScoringEnd}).
For $\V{i}$ that not be sampled in $\ell$-th turn,
we also assume that $\M{X}_{\V{i},\ell}=0$,
so that the final sacore
\begin{equation}\label{eq:score}
\predx = \sum_{\ell} \M{X}_{\V{i},\ell}
\end{equation}

\subsection{Extract Top-$t$ Largest Values}
After sampling, we get a set of coordinates with scores $\predx$
or jsut a set of coordinates which have been sampled.
Denotes it by
\[
    \Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}
\]
The size of $\Psi_p$ is $P$.

There are two strategies for extracting top-$t$ largest values.
The naive strategy is computing the actual values in $\Psi_p$,
and extract the top-$t$ largest values.
We call it the full-computing strategy.
The other one is called pre-sorting strategy, which cooperates with the scores $\predx$.
It sort the coordinates in $\Psi_p$ according to scores $\widehat{x}_{\V{i}_p}$
and extract the top-$t'$ coordinates
\[
    \Psi_{t'} = \{ \V{i} | \predx \geq \widehat{x}_{\V{i'}},
                           \forall i'\in \Omega_{s} \backslash \Omega_{t'}
                \}
\]  where $t'>t$.
Then we compute the actual value $x_{\V{i}}$ of $\V{i}$ in set $\Psi_{t'}$.
And the final top-$t$ largest value's coordinates will be
\[
    \Psi_{t} =
                \{ \V{i} | x_{\V{i}}\geq x_{\V{i'}},
                           \forall i' \in \Omega_{t'} \backslash \Omega_{t}
                \}
\]
The use of budget $t'$ is a tradeoff between accuracy and computation.

The accuracy of full-computing strategy is determined by the probability of coordinate $\V{i}$.
For one coordinate, it is a Binomial distribution  $\sim B(s,p(\epsilon_{\V{i}}))$.
So the higher probability $p(\epsilon_{\V{i}})$ of largest elements is,
the more likely it will be sampled finally,
and thus the higher the accuracy will be.
The pre-sorting strategy do save computation,
however, the accuracy will less than the full-computing in statistical.
That ideal case is that the final score keeps the same order of actual values.

\subsection{Extension of Core Sampling}

We now consider the first factor the occurrence of coordinate
which determine the upper bound of the final accuracy.
The effort for significantly improving the final accuracy significantly
should to focus on $p(\epsilon_{\V{i}})$.
So we introduce the extension version of core sampling to reach a higher accuracy bound.

Instead of sampling one index $r$, we sampled the indexes pair $(r,r')$ at a time.
To implement this mechanism,
we introduce the extension matrix which expend the feature dimension from $R$ to $R^2$.
Suppose $\V{v}$ is an vector with dimension $R$ and the extension vector $\V{v}^e$,
the rule given below:
\[
    \V{v}^e(r+r'R) = \V{v}(r)\V{v}(r')\ {\rm for\ }r,r' = {1,2,...,R}
\]
After such expansion, we get $N$ expanded factor matrices
\begin{equation}\label{eq:ExtensionMatrices}
    \M{E}^{(n)}=[\VnC{e}{n}{1},\ldots,\VnC{e}{n}{R^2}]
\end{equation}
of each $\M{A}^{(n)}$.
Apply core sampling to process these expanded factor matrices
will make the probability of coordinate proportioning to $x_{\V{i}}^2$.
We can make the probability of coordinate proportioning to $n$-the power the its value
by extending the feature vector to $R^n$.
For convenience, we use $core^n$ to
represent the algorithm with coordinate probability proportioning to $x^n_{\V{i}}$.
That is the $core^1$ stands for the original core sampling, and $core^2$ for the use of extension matrices.
The theoretical analysis and optimization to save the storage of extension version will be
introduced in folowing sections.

\section{Theoretical Analysis}

Until now, we just give the result without any proofs.
In this section,
we will show the probabilities of coordinates and the expectation of scores in our algorithms,
and give some useful error bounds.
We suppose all factor matrices are nonnegative.
This premise of the following compact conclusions.

\subsection{Probability of Coordinates}
In one round of sampling,
we sample one index in core partition and $N$ indexes in other partitions.
We use the notation $\epsilon_{\V{i},r}$ to represent this random event
and $\epsilon_{\V{i}}$ for the event that $\V{i}$ occures.
The $p(\cdot)$ stands for the probability of events.
\begin{lemma}\label{lem:Probability}
    Suppose all facotor matrices are nonnegative.
    In core sampling, $p(\epsilon_{\V{i}})$ equals $x_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
The probability $\epsilon_{\V{i}}$ is a marginal distribution of $\epsilon_{\V{i},r}$:
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{r=1}^{R} p(\epsilon_{\V{i},r}) \\
& = \sum_{r=1}^{R} p({\rm pick\ }r)p({\rm pick\ }i_1|r)\cdots p({\rm pick\ }i_N|r)\\
& = \sum_{r=1}^{R} \frac{\WreightR}{\norm{\V{W}}{1}}
    \frac{|\Sca{a}{1}{r}|}{\Nrocl{1}{r}}\ldots\frac{|\Sca{a}{N}{r}|}{\Nrocl{N}{r}}\\
& = \sum_{r=1}^{R} \frac{\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}}{\norm{\V{W}}{1}}
  = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}
The actual probability vector $\V{w}$ is according to the context.
\begin{lemma}\label{lem:ExtensionProbability}
    Suppose all facotor matrices are nonnegative.
    In extension core sampling, $p(\epsilon_{\V{i}})$ equals $x^2_{\V{i}}/\norm{\V{w}}{1}$,
    even with extra sampling in core partition.
\end{lemma}

\begin{proof}
It is easy to notice that
for each column of the extension matrices in~\Eqn{ExtensionMatrices} satisfies:
\[
    \norm{\VnC{e}{n}{*l}}{1} = \sum\nolimits_{i_n}|\Sca{a}{n}{r}\Sca{a}{n}{r'}|,l=r+r'R
\]
According to central sampling,
we first sample the indexes pair $(r,r'),l = r+r'R$
with probability $w_l/\norm{\V{w}}{1}$.
Then sample the other indexes $v^{n}_{i_n}$
with probability $e^{(n)}_{i_nl}/\norm{\VnC{e}{n}{*l}}{1}$.
By definition, we know
\[
w_l = \norm{\VnC{e}{1}{*l}}{1}\cdots \norm{\VnC{e}{N}{l}}{1}, l = 1,\ldots,R^2
\]
and
\[
e^{(n)}_{i_nl} = \Sca{a}{n}{r}\Sca{a}{n}{r'}, l = r+r'R
\]
So, let $l = r+r'R$
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{l}    p(\epsilon_{\V{i},l})
  = \sum_{r,r'} p(\epsilon_{\V{i},r,r'})\\
& = \sum_{l} \frac{w_l}{\V{w}}
             \frac{|e^{(1)}_{i_1l}|}{\norm{\VnC{e}{1}{*l}}{1}}\cdots
             \frac{|e^{(N)}_{i_Nl}|}{\norm{\VnC{e}{N}{*l}}{1}} \\
& = \frac{1}{\norm{\V{W}}{1}}
    \sum_{r,r'} |\Sca{a}{1}{r} \cdots\Sca{a}{N}{r}\Sca{a}{1}{r'}\cdots\Sca{a}{N}{r'}| \\
& = \frac{1}{\norm{\V{W}}{1}}
    (\sum_{r}|\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}|)^2\\
& = \frac{x_{\V{i}}^2}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}
And it is noticed that for hign order extension, $p(\epsilon_{\V{i}})$ equals to $x^n_{\V{i}}/\norm{\V{w}}{1}$,
\subsection{Expectation of Scores}
In this part, we show the expectation of scores is an estimation of actual value.
\begin{lemma}\label{lem:Expectation}
The expectation of score $\widehat{x}_{\V{i}}$ in core sampling equals to $sx_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
    The final score shown in ~\Eqn{score},
    with the independence of $\M{X}_{\V{i},\ell}$ in different iterations $\ell$,
    the expectation of final score in core sampling is:
\begin{align*}
\mathbb{E}[\predx]
& = \sum_{\ell}^s \mathbb{E}[\M{X}_{\V{i},\ell}]\\
& = \sum_{\ell=1}^{s}\sum_{r=1}^{R} p(\epsilon_{\V{i},r})\M{X}_{\V{i},\ell}\\
& = s\sum_{r=1}^{R} \frac{|\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}|}{\norm{\V{W}}{1}}sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})\\
& = s\sum_{r=1}^{R} \frac{\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}}{\norm{\V{W}}{1}}
= \frac{sx_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}

\subsection{Error Bounds}
\begin{theorem}\label{theo:ObservationBound}
Fix error probability $\sigma \in (0,1)$.
The demond of samples of coordinate $\V{i}$ being observated as least once is
\[
    s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})
\]
\end{theorem}
\begin{proof}
Since it is a Binomial distribution  $\sim B(s,p(\epsilon_{\V{i}}))$ with
small $p(\epsilon_{\V{i}})$ and large $s$.
The Posson approximation is used with $\lambda = sp(\epsilon_{\V{i}})$.
So $Prob(x\geq1)=1-Prob(x=0)=1-e^{-sp(\epsilon_{\V{i}})} \geq 1-\sigma$.
That gives $s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})$.
\end{proof}

\begin{theorem}\label{theo:Bound}
Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
In core sampling, if the number of samples
\[
    s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x_{\V{i}})
\]
then
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
\]
\end{theorem}

\begin{proof}
This result is closely follows the proof of Lemma 3 from \cite{BaPiKoSe15}.
Since  $ \M{X}_{\V{i},\ell},\cdots,\M{X}_{\V{i},s} $
are independent random variables taking values in $\{0,1\}$.
So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are a special case).
The Chernoff bounds for the sum of Poisson trials shows for any $0 <\delta <1 $:
\[
    Pr[|\predx - \mu|\geq\delta\mu)] \leq 2\exp{(-\mu\delta^2/3)}
\]
where $\mu=\mathbb{E}[\predx]=(sx_{\V{i}})/\norm{\V{W}}{1}$.
And by the choice of $s$, we have
$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
Then
\[
    Pr[|\predx-sx_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx_{\V{i}}/\norm{\V{W}}{1}] \leq \sigma
\]
multipling by $\norm{\V{W}}{1}/s$ inside the ${\rm Pr}[\cdot]$ gives
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
\]
\end{proof}
The ~\Theo{ObservationBound} and ~\Theo{Bound} show that shows that
to reach a good performance of finding of estimate the vaule $x_{\V{i}}$
the demand samles $s\geq \norm{\V{w}}{1}/x_{\V{i}}^n$.

\section{Core Sampling for Queries}
In recommendation system,
it is also demanded to find the most $n$ relevant items for a set of users.
For consistency,
say $\M{A}^{(1)}$ stands for the querying users,
and the rest $N-1$ matrices $\M{A}^{(n)},n=2,\ldots,N$ are items matrices.
Each row in matrix $\M{A}^{(1)}$ represents a particular user $\V{u}$.
And the $N-1$ order tenor in ~\Eqn{RankTensorCP}
represents the predicted ranking score of this user for all item combinations.

\begin{equation}\label{eq:RankTensorCP}
\T{X}_{\V{u}} := \KT{ \V{u},\Mn{A}{2}\dots,\Mn{A}{N}}
\end{equation}
It is a exception to replace one factor matrix by a vector in previous algorithm.
When we have a lots of queries to deal,
the previous user will have many useful information for the next user to process.
The most noted one in sampling processing is walking form core partition to other partitions.
And the biggest difference between two users in the weight of vertexes in core partition,
in other word,
the frequency number $(c_1,c_2,\ldots,c_R)$ of each $v^c_r$ is expected to be sampled.
We define $R$ pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_R$ to save occurred coordinates.
The algorithm for query sampling is show in ~\Alg{QuerySampling}.
In ~\AlgLine{QuerySampling}{Indexes},
the user-oriented indexes $r$ is depending on the specific choice of sampling algorithm.

\begin{algorithm}[t]
    \caption{Finding top k-tuples for a query}\label{alg:QuerySampling}
        Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.
        $\M{A}^{(1)}$ is the query matrix.\\
        Let $s$ be the number of samples, $k'$ be the budget.
    \begin{algorithmic}[1]
    \State Initialize $R$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_R$
    \State Initialize $c_r = 0,r= 1,\ldots,R$
    \For {$i_1 =1,2,\ldots,L_1$}
    \State Let query $\V{u}=\VnC{a}{1}{i_1*}$
    \For {$\ell = 1,\ldots,s$}
    \State Sample the user-oriented index $r$.
    \label{line:Indexes}
    \State  Increment $c_r$.
    \EndFor
    \For {$r= 1,\ldots,R$}
    \If {$c_r > |\V{g}_r|$ }
    \State Sample $c_r - |\V{g}_r|$ coordinates into $\V{g}_r$.
    \EndIf
    \State Use $c_r$ coordinates in $\V{g}_r$ and compute scores.
    \EndFor
    \State Pre-sorting for finding top-$k$ tuples for query $\V{u}$.
    \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Fast Implementation}
In section, we introduce some implementing details get improvements.
\subsection{Core Partiton Sampling}
Core sampling samples one $r$ in one round like
~\AlgLine{CoreSampling}{CorePartition} and ~\AlgLine{QuerySampling}{Indexes}.
In practice, instead of sample $r$ once a time,
we sample the frequency $c_r$,
so that the expectation $c_r$ is $sw_r/\norm{\V{w}}{1}$.
\begin{equation*}c_r=
    \left\{
      \begin{array}{ll}
        \lfloor sw_r/\norm{w}{1} \rfloor,
        & \hbox{$p=\lceil sw_r/\norm{w}{1} \rceil - sw_r/\norm{w}{1}$} \\\\
        \lceil sw_r/\norm{w}{1} \rceil,
        & \hbox{$p=\lfloor sw_r/\norm{w}{1} \rfloor - sw_r/\norm{w}{1}$}
      \end{array}
    \right.
\end{equation*}
It requires $O(R)$ complexity comparing $O(s\log{R})$ in core partition sampling phase.

The other benefit of sampling the frequency is when sample number $s$ is large,
$c_r$ will be corresponding large.
So we use the alias method\cite{Vose91}
for efficiently sampling from a discrete probability distribution.
It requires $O(n)$ for initialization, where $n$ is the distribution size, and
const time for each generation.
In our algorithm $c_r$ is the number of generations,
and the probability distribution ${\Sca{a}{n}{r}}/\Nrocl{n}{r}$.
\subsection{Extension of Matrices}
Use the extension matrices will be storage-costing.
Utilizing the frequency sampling, we do not to save the extension matrices in memory.
Firstly, we compute the probability vector in core partition, whichi takes $O(R^2)$ in storage.
And for each time we sample $c_l$ times coordiantes,
we only need to compute the probability vectors $e^{(n)}_{i_nl}/\norm{\V{w}}{1}$ once.
Teh storage it taks totally $O(L_1 + L_2 + \cdots + L_N)$,
that makes high order extension possible.
\section{Experiments Results}
In this section, we will analysis the recall
and time consuming of our algorithms on real data sets.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{fig_ml_2k_recall}\\
  \caption{Recall for different methods in ml-2k data sets.}
  \label{fig:ml_2k_recall}
\end{figure}
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{fig_ml_10m_recall}\\
  \caption{Recall for different methods in ml-10m data sets.}
  \label{fig:ml_10m_recall}
\end{figure}


\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{fig_lastfm_recall}\\
  \caption{Recall for different methods in lastfm data sets.}
  \label{fig:lastfm_recall}
\end{figure}
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale = 0.4]{fig_delicious_recall}\\
  \caption{Recall for different methods in delicious data sets.
            }
  \label{fig:delicious_recall}
\end{figure}

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale = 0.4,clip=true,viewport = 0 0 10 10]{fig_times}\\
  \caption{Time for different data sets using the different sampling approaches.
           The costs of time is not depended on the top $t$,
           when full-computing strategy is used.}
  \label{fig:Time}
\end{figure}


\subsection{Data and Preprocessing}
We evaluate our algorithms on some real datesets:
DeliciousBookmarks
\footnote{http://www.delicious.com}(delicious for short),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens+IMDb\footnote{http://www.imdb.com }/
Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ml-2k).
Those data sets are released in the framework of the 2nd International Workshop (HetRec2011).
And one larger data sets of MovieLens\cite{Harper2015}:ml-10m.

We use 5-core of each data set in which all user, item and tag occurred at least 5 times.
The statistics of preprocessed data is shown in ~\Table{Data}.
\begin{table}[ht]
  \label{table:Data}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    DataSet     & User & Item    & Tag    & top-1   & top-1000\\
    \hline
    ml-2k       & 456  &  1973   &  1222  & 6.6813  & 2.5663 \\
    ml-10m      & 993  &  3298   &  2555  & 67.1037 & 26.747 \\
    lastfm      & 1348 &  6927   &  2132  & 78.1688 & 30.8263\\
    delicious   & 1681 &  29540  &  7251  & 3.9153  & 2.7896 \\
    \hline
  \end{tabular}
  \caption{Data Statistics}
\end{table}
We use the algorithm in \cite{Rendle_RTF} to train each data set under CP decomposition model.
The factor size $R$ is $64$.
After training, we get three factor matrices for user, item and tag of each data set.

\subsection{Recall and Time Consuming}
We evaluate those different methods' performance on time and recall of top-$t$ values.
We both use first order score for central sampling and the extension version.
We use the maximum budget which is the full-strategy to show the performance of our algorithms.
For each number of samples, we do $10$ times experiments independently, and the average recall is used.
The recall of each data set is show in ~\Figs{ml_2k_recall}{delicious_recall} .
And the time consuming for each algorithms in shown in ~\Fig{Time}.



\subsection{Sampling for Queries}
We use the pools to save the computation when it sampling for queries.
We use the extension matrices to do retrial task with $s=10^5$ and $k=100$.
the recall and time of each queries is shown in ~\Fig{Queries}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale = 0.4]{fig_queries}\\
  \caption{Recall and time for queries in date set lastfm.
           For better visualization,
           we use every 10 queries in processing.
           The $k=100$ and samples $s=10^5$.}
  \label{fig:Queries}
\end{figure}
\subsection{Estimation of Values}
In ~\Theo{Bound}, we gives the error bound of estimate the actual value.
We use the $core^3$ to dealing with the data set lastfm.
And the score $\widehat{x}_{\V{i}}$ is an estimation of $sx^3_{\V{i}}/\norm{\V{w}}{1}$.
We draw the result of pairs $(\norm{\V{w}}{1}\widehat{x}_{\V{i}}/s,x^3_{\V{i}})$.
We only show the $10^4$ pairs with largest scores in ~\Fig{Est}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale = 0.4]{fig_lastfm_est}\\
  \caption{The pairs of estimation value $\norm{\V{w}}{1}\widehat{x}_{\V{i}}/s$ and actual $x^3_{\V{i}}$.
           We draw the top $1e4$ pairs with largest scores and use $s=10^8$.}
  \label{fig:Est}
\end{figure}
\section{Conclusion}
In this paper propose a method for finding top-$t$ values for given CP-models.
After which we point out that we can adjust this method to get the $n$-the order estimation
and reach the high probability of finding one coordiante.
\bibliography{IIP}
\bibliographystyle{aaai}
\end{document}
