\documentclass[letterpaper]{article}
% Required Packages
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algpseudocode}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
%/Title (Core Sampling for Top t Retrial Processing)
%/Author (Zhi Lu,Yang Hu,Zeng Bing)
%/Keywords ()
}
\title{Star Sampling for Approximate Maximum Search in Tensor}
%\author{Zhi Lu \and Yang Hu \and Bing Zeng\\
%School of Electronic Engineering, University of Electronic Science and Technology of China\\
%Email: zhilu@std.uestc.edu.cn\\
%Email: \{yanghu,eezeng\}@uestc.edu.cn
%}
\author{AAAI Press\\
Association for the Advancement of Artificial Intelligence\\
2275 East Bayshore Road, Suite 160\\
Palo Alto, California 94303\\
}
%% -------
%% Scalar
%% -------
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
% entries in factor matrices 
\newcommand{\anr}[2]{\Sca{a}{#1}{#2}}
% entries in extension factor matrices 
\newcommand{\enr}[2]{\Sca{e}{#1}{\V{#2}}}
% score for {#1}-turn
\newcommand{\score}[1]{\xi_{\V{i},#1}}
%% -------
%% Tensor
%% -------
\newcommand{\T}[1]{\mathcal{#1}}
\newcommand{\KT}[1]{\llbracket #1 \rrbracket}
%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\boldsymbol{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\ColVec}[3]{\V{#1}^{(#2)}_{#3}}
\newcommand{\NormColA}[2]{\norm{\ColVec{a}{#1}{*#2}}{1}}
\newcommand{\NormColE}[2]{\norm{\ColVec{e}{#1}{*\V{#2}}}{1}}
\newcommand{\RowVecA}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\ColVecA}[1]{\V{a}^{(#1)}_{*r}}
% others
\newcommand{\coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WeightR}{\NormColA{1}{r}\ldots\NormColA{N}{r}}
\newcommand{\predx}{\hat{x}_{\V{i}}}
\newcommand{\predxn}{\hat{x}_{\V{i},n}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\boldsymbol{{\MakeUppercase{#1}}}}}
\newcommand{\FacMat}[2]{\M{#1}^{(#2)}}
% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}
%% ------------
%% reference
%% ------------
% reference:definition
\newcommand{\Def}[1]{Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]{Eq.(\ref{eq:#1})}
% reference:figure
\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\Figs}[2]{Figure~\ref{fig:#1}$\sim$\ref{fig:#2}}
% reference:table
\newcommand{\Table}[1]{Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]{Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1]{Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1]{Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
\maketitle
\section{Abstract}
Factorization models have been extensively used for 
recovering the missing entries of a matrix or tensor. 
However, direct computing all entries 
using the learned factorization models is prohibitive 
when the size of the matrix/tensor is large. 
On the other hand, in many applications, 
such as collaborative filtering, 
we are only interested in a few entries that are the largest among all entries. 
In this work, we propose a sampling-based approach for finding the top entries of a tensor 
which is decomposed by the CANDECOMP/PARAFAC model. 
We develop an algorithm to sample the entries with probabilities proportional to their values. 
We further extend it to make the sampling proportional to the $k$-th power of the values, 
amplifying the focus on the top ones. 
We provide theoretical analysis of the sampling algorithm and evaluate its performance on several real-world data sets. 
Experimental results indicate that the proposed approach is orders of magnitude faster than exhaustive computing. 
When applied to the special case of searching in a matrix, 
it also requires fewer samples than the state-of-the-art method.

\section{Introduction}
Matrix and tensor completion has received considerable attention in recent years. 
Many problems in application can be formulated 
as recovering the missing entries of a matrix or tensor. 
In some scenarios, such as image and video in-painting~\cite{Ankita14}, 
all lost entries are needed to be filled in. 
In some others, however, it is difficult and also unnecessary to recover them all. 
For example, for recommender systems, the size of the matrix or tensor, 
which is determined by the numbers of users and items, is usually rather large. 
It is computational expensive to compute all of the unknown values. 
On the other hand, for recommendation purpose, 
we are only interested in a few entries that are the largest within a sub-array of the matrix or tensor. 
The largest entries of a matrix/tensor are not only the central concerns for personalized recommendation, 
but also meaningful in many other cases. 
In a similarity matrix, the top entries correspond to pairs of items that are most similar, 
which are of interest for applications like link prediction in graph~\cite{LibenNowell07}, 
duplicate detection~\cite{Ke2010} as well as information retrieval~\cite{Salton03IR}. 
And in neuroimage meta-analysis, 
the largest matrix/tensor entries may suggest the most probable associations between brain functions and behaviors.

In this work, 
we study the problem of efficiently identifying the top entries of a tensor 
without exhaustive computing. 
The tensor, as a multi-way generalization of the matrix, 
has been exploited more and more recently. 
Take recommender systems for example. 
While traditional focus is on the user-item matrix/tensor 
is required for data representation in many emerging settings 
such as context-aware recommendation~\cite{Rendle_PITF,KoYe09}, 
%where contextual information like time and location is considered, 
and set-based recommendation~\cite{HuYiLa15},
where the object to be recommended is a set of items that interact with each other. 
Following the most common paradigm, we assume that the tensor can be decomposed into some factors, 
which can be estimated from the observed entries by some learning algorithms. 
Specifically, we focus on the CANDECOMP/PARAFAC decomposition model.

CP decomposition~\cite{KoBa09} is a widely used technique for exploring and 
extracting the underlying structure of multi-way data. 
Given a $N$-order tensor $\T{X}\in\mathbb{R}^{L_1\times \cdots\times L_N}$, 
CP decomposition approximates it by $N$ factor matrices $\FacMat{A}{1},\FacMat{A}{2},\ldots,\FacMat{A}{N}$, 
such that
\begin{align}
\label{eq:CPDecomposition}
\T{X}&\approx\KT{\FacMat{A}{1},\FacMat{A}{2},\cdots,\FacMat{A}{N}} \\ \notag
&=\sum_{r=1}^R\ColVecA{1}\circ\ColVecA{2}\circ\cdots\circ\ColVecA{N}
\end{align}
where each factor matrix 
$\FacMat{A}{n}=[\ColVec{a}{n}{*1}\ColVec{a}{n}{*2}\cdots\ColVec{a}{n}{*R}], n=1,\ldots,N$
is of size $L_n\times R$ with $\ColVec{a}{n}{*r}\in\mathbb{R}^{L_n}$ 
being the $r$-th column.
The symbol ``$\circ$'' represents the vector outer product. 
$R$ is the tensor rank, indicating the number of latent factors. 
Elementwise, \Eqn{CPDecomposition} is written as
\begin{align}
\label{eq:CPValue}
x_\V{i} \approx \sum_{r=1}^{R}\anr{1}{r}\anr{2}{r}\cdots\anr{N}{r}
\end{align}
where $\V{i}$ is short for the index tuple $(i_1,i_2,\ldots,i_N)$.

Given $N$ factor matrices $\textbf{A}^{(1)},\ldots,\textbf{A}^{(N)}$ and a parameter $t$, 
we would like to find $t$ index tuples $\{\boldsymbol{i}_1,\ldots,\boldsymbol{i}_t\}$ 
which correspond to the $t$ largest $x_{\boldsymbol{i}}$.
This problem subsumes many existing problems in the literature.
When $N=2$, it is exactly the MAD (Maximum All-pairs Dot-product Search) problem~\cite{BaPiKoSe15}
that finds the largest entries in the product of two matrices. 
And MAD contains the MIPS (Maximum Inner Product Search)~\cite{Cohen97} problem 
as the special case with one matrix being a single column.

The most obvious approach is to compute the entries exhaustively. 
However, this becomes prohibitive as the sizes of the factor matrices grow. 
There is some literature in approximate matrix multiplication. 
But these methods are not suited even for MAD, 
since only a few entries among the millions are of interest. 
The more efficient solution is to directly search the top ones. 
This has been extensively studied for the MIPS problem.
Popular approaches include LSH (Locality Sensing Hashing)~\cite{Andoni08,ALSH14},
space partition techniques like k-d tree 
and the sampling approachs are also widely used~\cite{Drineas2006,John15}.
Recently, Ballard et al. proposed a randomized approach called diamond sampling~\cite{BaPiKoSe15} to the MAD problem. 
They selected diamonds, i.e. four-cycles, from a weighted tripartite representation of the two factor matrices, 
with the probability of selecting a diamond corresponding to index pair $(i_1,i_2)$ 
being proportional to $(\RowVecA{1}\cdot\RowVecA{2})^2$,
Where we ues $\RowVecA{n}$ to represnt the $i_n$ row of factor matrix.
For tensor, there hasn't been any study conducted yet.

Inspired by the work of diamond sampling,
we apply index sampling methods to the case of tensor, 
whose entries are computed by the CP decomposition model. 
We design a strategy to sample the index $\V{i}$ proportional to the magnitude of the corresponding entries. 
We further extend it to make the sampling proportional to the $k$-th power of the entries,
amplifying the focus on the largest ones. 
For the application of recommender systems, 
an algorithm that reuse the samples for different users is presented. 
We provide theoretical analysis for our sampling algorithms, 
and derive concentration bounds on the behavior. 
We evaluate the sampling methods on several real-world datasets. 
The results show that they are orders of magnitude faster than exact computing. 
When compared to previous approach for matrix sampling, our methods require much fewer samples.

\section{The Sampling Method}
\begin{figure}[!ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[viewport = 0 20 240 130]{./img/fig_graph_factor_matrices}\\
  \caption{Graph representation of factor matrices}
  \label{fig:GraphMatrices}
\end{figure}
\subsection{Basic Sampling Method}
The idea of the randomized approach is 
to sample the entries of the tensor according to some probability distribution 
so that the larger the entry, the more probable it is picked. 
To achieve this, we use a (N+1)-partite graph to represent the factor matrices
$\FacMat{A}{1},\ldots,\FacMat{A}{N}$ in \Eqn{CPDecomposition}.
Consider one of the factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n \times R}$,
it is represented by a bipartite graph shown within the dash-dot box on ~\Fig{GraphMatrices}.
We first use $R$ nodes indexed by $r\in[R]$ to denote the $R$ columns of it,
$[R]$ denotes $\{1,\ldots,R\}$.
These nodes are also shared by other factor matrices.
They constitute the core partition.
We then use $L_n$ nodes, indexed by $i_n\in [L_n]$,
to represent the rows of $\FacMat{A}{n}$.
They constitute a peripheral partition. 
Edge $(i_n,r)$ exists if the entry $\anr{n}{r}$ is nonzero.

To sample an entry, we sequentially pick each of its indices
starting from the core partition. 
We first assign some weights to the nodes in the core partition. 
For $r\in[R]$, let
\begin{equation}
    w_r = \WeightR
\end{equation}
where $\|\cdot\|$ denotes 1-norm of the vector.
We sample the nodes in core partition with probability $w_r/\norm{\V{W}}{1}$. 
After one core node is chosen, 
$N$ peripheral nodes, one from each peripheral partition, are picked, 
where the index $i_n$ is drawn with probability $|\anr{n}{r}|/\norm{\ColVecA{n}}{1}$. 
After an index tuple $\V{i}=\coord$ is obtained, 
a score is computed that
\begin{align}
\score{\ell}  = sgn(\anr{1}{r}\cdot\anr{2}{r}\cdots\anr{N}{r})
\end{align}
where $\ell$ denotes the $\ell$-th sample. 
If the indices $\V{i}$ has not been sampled before, 
a container is created with $\predx = \score{\ell}$. 
Otherwise, we increase $\predx$ by $\score{\ell}$. 
The procedure is shown in \Alg{CoreSampling}.

Let $\Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}$ 
denote the set of unique indices that have been sampled. 
$P\leq s$ since some $\V{i}$ may be picked more than once. 
There are two strategies for identifying the $t$ largest entries from the samples. 
In the first one, 
we directly compute the exact entry value for each sample in $\Psi_{p}$ using \Eqn{CPValue}
and then find the $t$ largest ones. 
Inspired by previous works, in the second strategy, 
we utilize the scores $\predx$ obtained during sampling. 
Computing the exact entry value using \Eqn{CPValue} for all sampled indices 
would be time consuming when is $R$ and $N$ are large, 
while the load for computing $\predx$ is much lighter. 
Moreover, as we show through theoretical analysis later, 
the expectation of $\predx$ is proportional to the exact value $x_{\V{i}}$. 
Therefore, we can use these estimated scores to filter out the relatively small ones 
and only compute the exact values for a subset of the sampled indices. 
Specifically, we denote the subset by $\Psi_{t'},t'>t$, 
that contains $t'$ indices corresponding to the $t'$ largest $\predx$, i.e.
$
    \Psi_{t'} = \{ \V{i} | \predx \geq \hat{x}_{\V{i'}},
                           \forall i'\in \Psi_{p} \backslash \Psi_{t'}
                \}
$.
Then the exact values  are computed only for $\V{i}$ in $\Psi_{t'}$. 
And the $t$ indices with the largest $x_{\V{i}}$ are extracted.

The chose of $t'$ is a tradeoff between accuracy and computation. 
The recall of the first direct computing strategy is determined by the probability 
that indices of the largest entries are sampled during each iteration. 
The higher the probabilities for the top entries, the more likely they will be sampled 
and thus the higher the recall. 
The second strategy will save some computation. 
However, since the scores are only estimations of the true entry values, 
some orders between the entries may not be kept 
that some top entries may be missed during filtering, 
which will lead to a slightly lower recall than the first strategy.

\begin{algorithm}[!ht]
    \caption{The basic sampling method}
    \label{alg:CoreSampling}
    Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples.
    \begin{algorithmic}[1]
    \For{$r\in{1,2,\ldots,R}$}
    \State $w_r \leftarrow \NormColA{1}{r}\NormColA{2}{r}\ldots\NormColA{N}{r}$
    \label{line:Weight}
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
    \label{line:CorePartition}
    \For {$n = 1,...,N$}
    \label{line:ItemPartitionFor}
    \State Sample $i_n$ with probability $|\anr{n}{r}|/\norm{\ColVecA{n}}{1}$
    \EndFor
    \label{line:ItemPartitionEnd}
    \State
    \label{line:Scoring}
        $\score{\ell} \leftarrow sgn(\anr{1}{r}\cdots\anr{N}{r})$
    \If {$\V{i}=\coord$ has not been sampled}
    \State  Create $\predx \leftarrow \score{\ell} $
    \Else
    \State $\predx \leftarrow \predx + \score{\ell}$
    \EndIf
    \EndFor
    \label{line:ScoringEnd}
    \end{algorithmic}
\end{algorithm}

\subsection{Theoretical Analysis}
In each iteration, 
we sample one compound node, indexed by $\boldsymbol{r}$, 
from the Core$^k$ partition and $N$ nodes, 
indexed by $\boldsymbol{i}$, from the peripheral partitions. 
Let $\epsilon_{\boldsymbol{i},\boldsymbol{r}}$ 
denote the event of choosing $\boldsymbol{r}$ 
and $\boldsymbol{i}$, $\epsilon_{\boldsymbol{i}}$ denote that of choosing $\boldsymbol{i}$.
We first analyze the probability $p(\epsilon_{\V{i}})$ that an entry is sampled by the sampling algorithms and then the expectation of the scores we computed during sampling. We also prove error bound on our estimate.

\begin{lemma}\label{lem:Probability}
    Suppose that all elements of the factor matrix $\M{A}^{(n)}, n\in[N]$ are nonnegative. We have
    $p(\epsilon_{\V{i}})$ equals to $x_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
The probability $p(\epsilon_{\V{i}})$ is the marginal distribution of $p(\epsilon_{\V{i},r})$,
so we have:
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{r} p(\epsilon_{\V{i},r}) \\
& = \sum_{r} \frac{w_{r}}{\norm{\V{W}}{1}}
    \frac{|\anr{1}{r}|}{\NormColA{1}{r}} \ldots \frac{|\anr{N}{r}|}{\NormColA{N}{r}}\\
& = \sum_{r} \frac{\anr{1}{r}\cdots\anr{N}{r}}{\norm{\V{W}}{1}}
  = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}
Let $c_{\V{i},\ell}$ be a random variable that if $\V{i}$ is the index tuple 
being picked in the l-th iteration, $c_{\V{i},\ell}=\score{\ell}$. 
Otherwise $c_{\V{i},\ell}=0$. 
The final score $\predx$ can be written as
\begin{equation}\label{eq:score}
\predx = \sum_{\ell=1}^{s} c_{\V{i},\ell}
\end{equation}

\begin{lemma}\label{lem:Expectation}
The expectation of $\predx$ equals to $sx_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
    $c_{\V{i},\ell}$ are i.i.d for fixed $\V{i}$ and varing $\ell$. 
    The expectation of $\predx$ is:
\begin{align*}
\mathbb{E}[\predx]
& = \sum_{\ell=1}^{s}\mathbb{E}[c_{\V{i},\ell}] = \sum_{\ell=1}^{s}\sum_{r} p(\epsilon_{\V{i},r})\score{\ell} \\
& = s\sum_{r} \frac{|\anr{1}{r}\cdots\anr{N}{r}|}{\norm{\V{W}}{1}}
                  sgn(\anr{1}{r}\cdots\anr{N}{r})\\
& = s\sum_{r} \frac{\anr{1}{r}\cdots\anr{N}{r}}{\norm{\V{W}}{1}}
  = \frac{sx_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}

\subsubsection{Error Bounds}
We derive two error bounds, which suggest the number of samples required to get reasonable results. 
\begin{theorem}\label{theo:ObservationBound}
Fix error probability $\sigma \in (0,1)$.
Assuming all entries in the factor matrices are nonnegative.
The number of samples required for coordinate $\V{i}$ to be picked as least once is
\[
    s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})
\]
\end{theorem}
\begin{proof}
This is a binomial distribution${\sim}B(s,p(\epsilon_{\V{i}}))$,
in which $p(\epsilon_{\V{i}})$ is small and $s$ large.
So the Poisson distribution with $\lambda = sp(\epsilon_{\V{i}})$
can be used to approximate $Pr(x\geq1)$.
That is $Pr(x\geq1) = 1-Pr(x=0)\approx 1-e^{-sp(\epsilon_{\V{i}})} \geq 1-\sigma$,
which gives $s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})$.
\end{proof}

\begin{theorem}\label{theo:Bound}
Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
Assuming all entries in the factor matrices are nonnegative.
If the number of samples
\[
    s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x_{\V{i}})
\]
then
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
\]
\end{theorem}

\begin{proof}
We follow the proof of Lemma 3 in~\cite{BaPiKoSe15}.
Since  $c_{\V{i},1},\cdots,c_{\V{i},s}$
are independent random variables taking values in $\{0,1\}$.
So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are a special case).
The Chernoff bounds on the sum of Poisson trials shows, for any $0 <\delta <1 $:
\[
    Pr(|\predx - \mu|\geq\delta\mu) \leq 2\exp{(-\mu\delta^2/3)}
\]
where $\mu=\mathbb{E}[\predx]=sx_{\V{i}}/\norm{\V{W}}{1}$.
And by the choice of $s$, we have
$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
Then
\[
    Pr(|\predx-sx_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx_{\V{i}}/\norm{\V{W}}{1}) \leq \sigma
\]
multiplying $\norm{\V{W}}{1}/s$ inside ${\rm Pr}[\cdot]$ gives
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
\]
\end{proof}


\section{Extension of the Basic Sampling Method}
The recall we can obtain heavily depends on 
the probability distribution for sampling the entries. 
To achieve better performance, 
we should improve this distribution by amplifying the odds for the top entries.

Instead of sampling one node $r$ from the core partition, 
we can pick a pair of nodes $(r,r')$ from the core partition, 
where $r'$ is sampled with replacement. 
Then conditioned on $(r,r')$, we sample $N$ nodes, 
indexed by $(i_1,\ldots,i_N)$ from the peripheral partitions. 
More generally, we can pick $k$ nodes $(r_1,r_2,\ldots,r_k)$ from the core partition 
and then draw $N$ peripheral nodes. 
We name this strategy Core$^k$ sampling. 
The method discussed in previous section is a special case with $k=1$.

By sampling $k$ nodes with replacement from the core partition, 
we virtually create $R^k$ compound nodes, 
indexed by $\V{r}=(r_1,\ldots,r_k)$. 
These compound nodes constitute a new partition called Core$^k$ partition. 
And the original asteroid structure is used for sampling.
To assign probabilities for picking the compound nodes and probabilities for picking a peripheral node given a compound nodes, 
we extend each factor matrix $\FacMat{A}{n}$ with size $L_n\times R$ to a new matrix $\M{E}^{(n)}$ with size $L_n\times R^k$. 
The entries of $\M{E}^{(n)}$ are obtained by
\begin{align}
\enr{n}{r} = \anr{n}{r_1}\cdots\anr{n}{r_k}
\end{align}
where we use $\V{r}$ to index the columns of $\M{E}^{(n)}$. The weight we assign to the compound node $\V{r}$ is
\begin{align}
w_{\V{r}} = \NormColE{1}{r}\cdots\NormColE{N}{r}
\end{align}
with $\V{e}_{*\V{r}}^{(n)}$ being the an column of $\M{E}^{(n)}$ indexed by $\V{r}$ and
\begin{align}
\NormColE{n}{r} = \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|
\end{align}
The sampling procedure is similar with \Alg{CoreSampling} 
and we summarize it in \Alg{CoreExtensionSampling}. 
We prove that in Core$^k$ sampling,
the probability that entry $\V{i}$ is sampled is proportional to $x_{\V{i}}^k$.
Due to space limit, we put the proofs in supplementary material.
Therefore, in Core$^k$ extension, more focus will be put on the top entries.

Note that in the diamond sampling method for matrix~\cite{BaPiKoSe15}, 
they also sampled two nodes from the core partition. 
However, the second node $r'$ was sampled after the indices $i_1$ and $i_2$ were picked 
and none extension of the factor matrices were introduced. 
As a result, only the estimation $\predx$ was improved 
but not the probability of picking the top entries.

\begin{algorithm}[!ht]
    \caption{Core$^k$ sampling}
    \label{alg:CoreExtensionSampling}
    Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples, $k$ for the extension order.
    \begin{algorithmic}[1]
    \For{$\V{r}\in{\underbrace{R\times \cdots \times R}_{k}}$}
    \For{$n = 1,...,N$}
    \State $\NormColE{n}{r} \leftarrow \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|$
    \EndFor
    \State $w_{\V{r}} \leftarrow \NormColE{1}{r} \cdots \NormColE{N}{r} $
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$
    \label{line:nodes}
    \For {$n = 1,...,N$}
    \State Sample $i_n$ with probability $|\enr{n}{r}|/\NormColE{n}{r}$
    \EndFor
    \State
        $\score{\ell} \leftarrow sgn(\enr{1}{r}\cdots\enr{N}{r})$
    \If {$\V{i}=\coord$ has not been sampled}
    \State  Create $\predx \leftarrow \score{\ell} $
    \Else
    \State $\predx \leftarrow \predx + \score{\ell}$
    \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{Sampling for Multiple Users}
In this subection, 
we present an algorithm for efficiently identifying top entries in sub-arrays of a tensor,
which is of particular interest for recommender systems. 
Without loss of generality, 
we assume that the first factor matrix $\FacMat{A}{1}$ corresponds to users, 
with each row characterizing a single user. 
For user $\V{u} = \RowVecA{1}$, 
we are interested in top entries in the (N-1)-order tensor $\T{X}_u$,
whose CP decomposition is
\begin{align}
    \T{X}_{\V{u}} \approx \KT{\V{u},\FacMat{A}{2},\cdots,\FacMat{A}{N}} 
\end{align}

To generate recommendations for multiple users, 
an naive approach is to run the above sampling algorithm independently for different users. 
However, this is totally unnecessary. 
We find that for different users, 
only the probabilities for sampling nodes from the core partition are different. 
In the following step, they share the same distribution for sampling the peripheral nodes. 
Based on this observation, for each node in the core partition, 
we build a pool containing indices $\V{i}=(i_2,\ldots,i_N)$ that have been picked give that core node. 
For a new user, when a core node is chosen, 
we can directly use the indices kept in its pool and only sample new $\V{i}$ when necessary. 
By sharing among users, a huge number of samples can be saved. 
This procedure is illustrated in~\Alg{QuerySampling}.

\begin{algorithm}[!ht]
    \caption{Finding top-$t$ entries for a multiple users}
    \label{alg:QuerySampling}
        Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.
        $\FacMat{A}{1}$ is the set of quering users.
        Let $s$ be the number of samples, and $m=R^k$.
    \begin{algorithmic}[1]
    \State Initialize $m$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_m$
    \State Initialize $f_{\V{r}} = 0$ for all $\V{r}$.
    \For {$i_1 =1,2,\ldots,L_1$}
    \State Let query $\V{u}=\RowVecA{1}$
    \ForAll{$\V{r}$}
    \State $w_\V{r} \leftarrow |u_{r_1}\cdots u_{r_k}|$
    \EndFor
    \For {$\ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$.
    \label{line:Indexes}
    \State  Increment $f_{\V{r}}$.
    \EndFor
    \ForAll {$\V{r}$}
    \If {$f_\V{r} > |\V{g}_\V{r}|$ }
    \State Sample $f_{\V{r}} - |\V{g}_{\V{r}}|$ coordinates into $\V{g}_{\V{r}}$.
    \EndIf
    \State Use $f_{\V{r}}$ coordinates in $\V{g}_{\V{r}}$ and compute scores.
    \EndFor
    \State Post-processing for finding top-$t$ tuples of $\V{u}$.
    \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Implementation Details}
In this section, we discuss some implementation details that could improve the performance.

Instead of first picking a node from the Core/Core$^k$ partition 
and then choosing $N$ peripheral nodes, 
in implementation, we first directly get $s$ core nodes in one loop. 
Specifically, we compute the expected number of occurrence of the nodes.
Let $\mu_{\V{r}}=sw_\V{r}/\norm{\V{w}}{1}$, the count for $\V{r}$ is:
\begin{equation}f_\V{r}=
    \left\{
      \begin{array}{ll}
        \lfloor \mu_{\V{r}} \rfloor,
        & \hbox {with probability $p=\lceil \mu_{\V{r}} \rceil - \mu_{\V{r}}$} \\\\
        \lceil \mu_{\V{r}} \rceil,
        & \hbox{with probability $p=\lfloor \mu_{\V{r}} \rfloor - \mu_{\V{r}}$}
      \end{array}
    \right.
\end{equation}
This requires $O(R^k)$ work while sampling $\V{r}$ $s$ times requires $O(sk\log R)$ work.

To conduct Core$^k$ sampling, 
we extend the factor matrices to $\FacMat{E}{n}$,
which requires $O(L_nR^k)$ space for each $n\in[N]$.
This will be costly in practice when k is large. 
Using the above strategy, we have obtained the number of times each core code appears. 
Given a core node, the sampling of the peripheral nodes 
will only depend on one column in $\FacMat{E}{n}$ that is indexed by $\boldsymbol{r}$. 
Therefore, instead of storing the whole $\FacMat{E}{n}$, 
we only keep one column of it at a time. 
This requires $O(L_1+\cdots+L_N)$ space in total.
When k is small, we can directly store $\FacMat{E}{n}$ in exchange for speed.
\section{Experiments}
%
%
%
%
%
We evaluate our algorithms on four real-world data sets:
Delicious Bookmarks\footnote{http://www.delicious.com}(delicious),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens\footnote{http://www.grouplens.org}+IMDb\footnote{http://www.imdb.com}/Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ml-2k),
and a larger MovieLens data(ml-10m). 
The first three are from~\cite{Cantador:RecSys2011} and the forth one is from ~\cite{Harper2015}.
Following previous practice, users, items and tags that occur at least 5 times are used. 
Some statistics of the data sets after preprocessing are shown in \Table{Data}.
\subsection{Data and Preprocessing}
\begin{table}[!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    DataSet     & User & Item    & Tag    & top-1   & top-$10^3$\\
    \hline
    ml-2k       & 456  &  1973   &  1222  & 81.5643  & 43.3850 \\
    ml-10m      & 993  &  3298   &  2555  & 88.7368 & 50.5948 \\
    lastfm      & 1348 &  6927   &  2132  & 234.7302 & 101.7950\\
    delicious   & 1681 &  29540  &  7251  & 95.4723  & 40.8289 \\
    \hline
  \end{tabular}
  \caption{Data Statistics}
  \label{table:Data}
\end{table}
To learn the factor matrices of CP decomposition, 
we optimize the pair-wise 
ranking function Bayesian Personalized Ranking (BPR)~\cite{Rendle_BPR,Rendle_RTF}.
The factor matrices are learned so that the observed entries have ranking scores that are higher than the unobserved ones. $R$ is set to 64. After getting the factor matrices, we run different algorithms to find the top entries of the tensor.

\subsection{Results}
\subsubsection{Accuracy and Time}

\begin{figure}[!]
  \centering
  \includegraphics[width=3in]{./img/fig_recall}\\
  \caption{Recall of Core$^k$ sampling for $k\in\{1,2,3\}$.}
  \label{fig:recall}
\end{figure}
\begin{figure}[!]
    \centering
    \includegraphics[width=3in,viewport = 10 10 600 400]{./img/fig_times}\\
    \caption{Running time for the experiments in~\Fig{recall}.}
    \label{fig:time}
\end{figure}
We present time and accuracy results for the four data sets in 
~\Fig{recall} and~\Fig{time}. 
In~\Fig{recall}, we plot recall, i.e. the percentage of the top-$t$ entries identified, 
versus the number of samples. 
The results for $t\in\{1,10,100,1000\}$ are shown. 
~\Fig{time} plots the computation time versus the number of samples. 
The time for exhaustive computing is also shown. 
Here we set $t'=s$ and focus on the performance of the sampling step. 
For each $s$, we run the sampling algorithms 10 times and the average of the recalls is reported.

The results show that by extending the core partition to Core$^2$ and Core$^3$,
we can get the same level of recalls with much fewer samples.
This is consistent with our theoretical analysis that in Core$^k$ sampling,
the focus on top entries is amplified, 
making them more probable to be sampled. 
All extensions are orders of magnitude faster than exhaustive computing.

\subsubsection{Use Prefiltering}
We then evaluate the performance of using $\predx$ for prefilting with $t'=s/10$. 
Due to space limit, we only show results on the lastfm data set in~\Fig{lastfm_budget}. 
Compared with the results in~\Fig{recall} and~\Fig{time},
we notice that using prefiltering leads to a slightly lower recall but less computation.
\begin{figure}[H]
    \centering   
    \subfigure[]{
        \label{fig:a}    
        \includegraphics[width=3in]{./img/fig_lastfm_recall_budget} 
    }    
    \subfigure[]{
        \label{fig:b}    
        \includegraphics[width=2.7in]{./img/fig_lastfm_times_budget}    
    }    
\caption{Recall and time of using prefiltering. 
        The budget size is set to $t'=s/10$. (a) Recall (b) Time. 
        We show the ratio of time used by $t'=s/10$ to that used by $t'=s$.}
\label{fig:lastfm_budget}    
\end{figure}
\subsubsection{Sampling for Collaborative Filtering}
In~\Fig{Queries}, we show the performance of~\Alg{QuerySampling} 
applied for collaborative filtering, 
where the samples are shared among different users. 
Here Core$^2$ sampling is used to find the top-100 largest entries for the lastfm data set
and $s$ is set to $10^6$. 
We can see that only for the first few users we need some time to get the samples 
and build the sample pools. 
After that, the computation time for a new user is much reduced.
\begin{figure}[!]
  \centering
  \includegraphics[width=2.7in]{./img/fig_lastfm_queries}\\
  \caption{Recall and time results of~\Alg{QuerySampling} for the lastfm data set.
           For better visualization, we show one result for every $10$ users.}
  \label{fig:Queries}
\end{figure}
\subsubsection{Comparison to Diamond Sampling}
\begin{figure}[!ht]
  \centering
  \includegraphics[width=3in,viewport = 00 20 460 265]{./img/fig_lastfm_comparision_recall}\\
  \caption{Comparison with diamond sampling for the lastfm data set.}
  \label{fig:Comparison_recall}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=2.7in,viewport = 0 10 450 180]{./img/fig_lastfm_comparison_times}\\
    \caption{Running time for the experiments in~\Fig{Comparison_recall}.}
\label{fig:Comparison_time}
\end{figure}
We also compare our algorithms with diamond sampling~\cite{BaPiKoSe15} 
for finding the top entries in the product of two matrices. 
For each user, we multiply the vector characterizing it into the item matrix 
to get a user-oriented item matrix. 
The problem is to identify the top entries in the product of the item matrix and the tag matrix. 
Note that the searchings for different users are conducted independently,
i.e. we run~\Alg{CoreExtensionSampling} instead of~\Alg{QuerySampling} for this experiment.
We show the recalls and the time consumed by the sampling algorithms for the lastfm data set
in~\Fig{Comparison_recall} and~\Fig{Comparison_time}.
$t'$ is set to $s/10$. 
The performance of diamond sampling is better than Core$^1$ sampling.
This is reasonable since the score $\hat{x}_{\V{i}}$ it computed is proportional to the square of the entry magnitude. 
However, Core$^2$ and Core$^3$ extensions achieve higher recalls than diamond sampling with the same number of samples. 
This illustrates the advantage of augmenting the sampling probability to higher power of the entry magnitude.



\bibliography{IIP}
\bibliographystyle{aaai}
\end{document}
