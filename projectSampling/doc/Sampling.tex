\documentclass[letterpaper]{article}
% Required Packages
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Core Sampling for Top t Retrial Processing)
/Author (Zhi Lu,Yang Hu,Zeng Bing)
/Keywords ()
}
\title{Core Sampling for Top t Retrieval Processing}
\author{Zhi Lu \and Yang Hu \and Bing Zeng\\
School of Electronic Engineering, University of Electronic Science and Technology of China\\
Email: zhilu@std.uestc.edu.cn\\
Email: \{yanghu,eezeng\}@uestc.edu.cn
}
%
%\usepackage{amsfonts}
%\usepackage{mathrsfs}
\usepackage{amsthm,amsmath,amssymb}
% \usepackage{bm}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfigure}
% for algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
\newcommand{\anr}[2]{\Sca{a}{#1}{#2}}
\newcommand{\enr}[2]{\Sca{e}{#1}{\V{#2}}}
\newcommand{\Scah}[3]{h^{(#1,#2)}_{#3}(\V{x}_{i_#1})}
\newcommand{\score}[1]{\xi_{\V{i},#1}}
%\newcommand{\score}[1]{\M{X}_{\V{i},#1}}
%% -------
%% Tensor
%% -------
%\usepackage[mathscr]{eucal}
\newcommand{\T}[1]{\mathcal{#1}}

\newcommand{\KT}[1]{\llbracket #1 \rrbracket}

%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\boldsymbol{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\VnC}[3]{\V{#1}^{(#2)}_{#3}}
% norm one of r-th cloumn
\newcommand{\Nrocl}[2]{\norm{\VnC{a}{#1}{*#2}}{1}}
\newcommand{\Varow}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\Vacol}[1]{\V{a}^{(#1)}_{*r}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\textbf{{\MakeUppercase{#1}}}}}
%Matrix with superscript
\newcommand{\Mn}[2]{\M{#1}^{(#2)}}

% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}

%% ------------
%% reference
%% ------------

% reference:definition
\newcommand{\Def}[1]   {Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]   {Eq.(\ref{eq:#1})}
% reference:figure
\newcommand{\Fig}[1]   {Figure~\ref{fig:#1}}
\newcommand{\Figs}[2]  {Figure~\ref{fig:#1}$\sim$\ref{fig:#2}}
% reference:table
\newcommand{\Table}[1] {Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]  {Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1] {Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1] {Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1] {Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newcommand{\coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WeightR}{\Nrocl{1}{r}\ldots\Nrocl{N}{r}}
\newcommand{\predx}{\hat{x}_{\V{i}}}
\newcommand{\predxn}{\hat{x}_{\V{i},n}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
\maketitle
\section{Abstract}
The technique of matrix/tensor factorization has been used widely to describe the 
interaction between objects in the application of recovering missing entries.
And finding the most relevant of two or more objects is the central concerns.
Since the target matrix/tensor is not obtain directly in application, instead,
sets of instance vectors are the output for data representation.
It is need to search in all possibilities to find the most related objects.
However, the number of instances is always in vast scale, 
and the naive full search is time-consuming.
In this work, we study the problem of efficiently
identifying the top entries of a tensor via sampling method.


\section{Introduction}
Matrix and tensor completion has received considerable attention in recent years. 
Many problems in application can be formulated as recovering the missing entries of a matrix or tensor. 
In some scenarios, such as image and video in-painting, 
all lost entries are needed to be filled in. 
In some others, however, it is difficult and also unnecessary to recover them all. 
For example, for recommender systems, the size of the matrix or tensor, 
which is determined by the numbers of users and items, is usually rather large. 
It is computational expensive to compute all of the unknown values. 
On the other hand, for recommendation purpose, 
we are only interested in a few entries that are the largest within a sub-array of the matrix or tensor. 
The largest entries of a matrix/tensor are not only the central concerns for personalized recommendation, 
but also meaningful in many other cases. 
In a similarity matrix, the top entries correspond to pairs of items that are most similar, 
which are of interest for applications like link prediction in graph, 
duplicate detection as well as information retrieval. 
And in neuroimage meta-analysis, 
the largest matrix/tensor entries may suggest the most probable associations between brain functions and behaviors.

In this work, 
we study the problem of efficiently identifying the top entries of a tensor 
without exhaustive computing and searching all entries. 
The tensor, as a multi-way generalization of the matrix, 
has been exploited more and more recently. 
Take recommender systems for example. 
While traditional focus is on the user-item matrix\cite{KoYe09}, 
tensor\cite{Rendle_PITF,HuYiLa15} is required for data representation in many emerging settings 
such as context-aware recommendation, 
where contextual information like time and location is considered, and set-based recommendation, 
where the object to be recommended is a set of items that interact with each other. 
Following the most common paradigm, we assume that the tensor can be decomposed into some factors, 
which can be estimated from the observed entries by some learning algorithms. 
Specifically, we focus on the CANDECOMP/PARAFAC decomposition model.

CP decomposition\cite{KoBa09} is a widely used technique for exploring and 
extracting the underlying structure of multi-way data. 
Given a $N$-order tensor $\T{X}\in\mathbb{R}^{L_1\times \cdots\times L_N}$, 
CP decomposition approximates it by $N$ factor matrices $\Mn{A}{1},\Mn{A}{2},\ldots,\Mn{A}{N}$, 
such that
\begin{align}
\label{eq:CPDecomposition}
\T{X}&\approx\KT{\Mn{A}{1},\Mn{A}{2},\cdots,\Mn{A}{N}} \\ \notag
&=\sum_{r=1}^R\VnC{a}{1}{r}\circ\VnC{a}{2}{r}\circ\cdots\circ\VnC{a}{N}{r}
\end{align}
where each factor matrix 
$\Mn{A}{n}=[\VnC{a}{n}{1}\VnC{a}{n}{2}\cdots\VnC{a}{n}{R}], n=1,\ldots,N$
is of size $L_n\times R$ with $\VnC{a}{n}{r}\in\mathbb{R}^{L_n}$ 
being the $r$-th column.
And to distinguish, we ues $\Varow{n}$ to represnt the $i_n$ row of factor matrix.
The symbol ``$\circ$'' represents the vector outer product. 
$R$ is the tensor rank, indicating the number of latent factors. 
Elementwise, \Eqn{CPDecomposition} is written as
\begin{align}
\label{eq:CPValue}
x_\V{i} \approx \sum_{r=1}^{R}\anr{1}{r}\anr{2}{r}\cdots\anr{N}{r}
\end{align}
where $\V{i}$ is short for the index set $(i_1,i_2,\ldots,i_N)$.

\subsection{Related Work}
Given $N$ factor matrices $\textbf{A}^{(1)},\ldots,\textbf{A}^{(N)}$ and a parameter $t$, 
we would like to find $t$ indices $\{\boldsymbol{i}_1,\ldots,\boldsymbol{i}_t\}$ 
which correspond to the $t$ largest $x_{\boldsymbol{i}}$.
This problem subsumes many existing problems in the literature.
When $N=2$, it is exactly the MAD (Maximum All-pairs Dot-product Search)\cite{BaPiKoSe15} problem 
that finds the largest entries in the product of two matrices. 
And MAD contains the MIPS (Maximum Inner Product Search)\cite{Cohen97} problem 
as the special case with one matrix being a single column.
The most obvious approach is to compute the entries exhaustively. 
However, this becomes prohibitive as the sizes of the factor matrices grow. 
There is some literature in approximate matrix multiplication. 
But these methods are not suited even for MAD, 
since only a few entries among the millions are of interest. 
The more efficient solution is to directly searching the top ones. 
This has been extensively studied for the MIPS problem. 
Popular approaches include LSH (Locality Sensing Hashing), 
space partition techniques like k-d tree, etc. 
Recently, Ballard et al. proposed a randomized approach called diamond sampling to the MAD problem. 
They selected diamonds, i.e. four-cycles, from a weighted tripartite representation of the two factor matrices, 
with the probability of selecting a diamond corresponding to index pair $(i_1,i_2)$ being proportional to $(\Varow{1}\cdot\Varow{2})^2$.
For tensor, there hasn't been any study conducted yet.

Inspired by \cite{BaPiKoSe15}, 
we apply index sampling methods to the case of tensor, 
whose entries are computed by the CP decomposition model. 
We design a strategy to sample the index $\V{i}$ proportional to the magnitude of the corresponding entries. 
We further extend it to make the sampling proportional to the $k$-th power of the entries,
amplifying the focus on the largest ones. 
For the application of recommender systems, 
an algorithm that reuse the samples for different users is presented. 
We provide theoretical analysis for the sampling algorithms, 
and provide concentration bounds on the behavior. 
We evaluate the sampling methods on several real-world datasets. 
The results show that they are orders of magnitude faster than exact computing. 
When compared to previous approach for matrix sampling, our methods require much fewer samples.

\subsection{Notations}

We use the norm $\norm{\cdot}{1}$ operation refereed in diamond sampling.
For a vector $\V{v}\in \mathbb{R}^n$ or a matrix $\M{M}\in \mathbb{R}^{m\times n}$.
It is defined as:
\begin{align}
    \label{eq:norm}
    \norm{\V{v}}{1} = \sum_{i=1}^{n}|v_i|
    \ \  {\rm and} \ \
    \norm{\M{M}}{1} = \sum_{i=1}^{m}\sum_{j=1}^{n}|m_{ij}|
\end{align}


\section{Core Sampling}
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[]{./img/fig_graph_factor_matrices}\\
  \caption{Graph Presentation of Factor Matrices}
  \label{fig:GraphMatrices}
\end{figure}
\subsection{Sampling basic}
The idea of the randomized approach is 
to sample the entries of the tensor according to some probability distribution 
so that the larger the entry, the more probable it is picked. 
To achieve this, we use a (N+1)-partite graph to represent the factor matrices
$\Mn{A}{1},\ldots,\Mn{A}{N}$ in \Eqn{CPDecomposition}.
Consider one of the factor matrix $\Mn{A}{n}\in \mathbb{R}^{L_n \times R}$,
it is represented by a weighted bipartite graph shown within the dash-dot box on ~\Fig{GraphMatrices}.
We first use $R$ nodes indexed by $r\in[R]$ to denote the $R$ columns in the matrices. 
We use the notion $[R]$ to denote $\{1,\ldots,R\}$. 
These nodes constitute the core partition of the graph. 
Then for each $\textbf{A}^{(n)}\in\mathbb{R}^{L_n\times R}$, $L_n$ nodes, 
indexed by $i_n\in[L_n]$, are used to represent the rows. 
They constitute a peripheral partition. 
Edge $(i_n,r)$ exists if the entry $a_{i_nr}^{(n)}$ is nonzero.
To sample an entry, we sequentially pick each of its indices. 
We start the sampling from the core partition. 
We first assign some weights to the nodes in the core partition. 
For $r\in[R]$, let
\[
    w_r = \WeightR
\]
where $\norm{\cdot}{1}$is defined in \Eqn{norm}. 
We sample the nodes in core partition with probability $w_r/\norm{\V{W}}{1}$. 
After one core node is chosen, 
$N$ peripheral nodes, one from each peripheral partition, are picked, 
where the index $i_n$ is drawn with probability $|\anr{n}{r}|/\norm{\Vacol{n}}{1}$. 
After an index set $\V{i}=(i_1,\ldots,i_N)$ is obtained, 
a score is computed that
\[
\score{\ell}  = sgn(\anr{1}{r}\cdot\anr{2}{r}\cdots\anr{N}{r})
\]
where $\ell$ denotes the $\ell$-th sample. 
If the index $\boldsymbol{i}$ has not been sampled before, 
a container is created with $\predx = \score{\ell}$. 
Otherwise, we increase $\predx$ by $\score{\ell}$. 
The procedure is shown in \Alg{CoreSampling}

Let $\Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}$ 
denote the set of unique indices that have been sampled. 
$P\leq s$ since some $\boldsymbol{i}$ may be picked more than once. 
There are two strategies for identifying the $t$ largest entries from the samples. 
In the first one, 
we directly compute the exact entry value for each sample in $\Psi_{p}$ using \Eqn{CPValue}
and then find the $t$ largest ones. 
Inspired by previous works, in the second strategy, 
we utilize the scores $\predx$ obtained during sampling. 
Computing the exact entry value using \Eqn{CPValue} for all sampled indices 
would be time consuming when is $R$ and $N$ are large, 
while the load for computing $\predx$ is much lighter. 
Moreover, as we show through theoretical analysis later, 
the expectation of $\predx$ is proportional to the exact value $x_{\V{i}}$. 
Therefore, we can use these estimated scores to filter out the relatively small ones 
and only compute the exact values for a subset of the sampled indices. 
Specifically, we denote the subset by $\Psi_{t'},t'>t$, 
that contains $t'$ indices corresponding to the $t'$ largest $\predx$, i.e.
\[
    \Psi_{t'} = \{ \V{i} | \predx \geq \hat{x}_{\V{i'}},
                           \forall i'\in \Psi_{p} \backslash \Psi_{t'}
                \}
\] 
Then the exact values  are computed only for $\boldsymbol{i}$ in $\Psi_{t'}$. 
And the $t$ indices with the largest $x_{\V{i}}$ are extracted.

The chose of $t'$ is a tradeoff between accuracy and computation. 
The recall of the first direct computing strategy is determined by the probability 
that indices of the largest entries are sampled during each iteration. 
The higher the probabilities for the top entries, the more likely they will be sampled 
and thus the higher the recall. 
The second strategy will save some computation. 
However, since the scores are only estimations of the true entry values, 
some orders between the entries may not be kept 
that some top entries may be missed during filtering, 
which will lead to a slightly lower recall than the first strategy.

\begin{algorithm}[ht]
    \caption{Core Sampling with factor matrixes}
    \label{alg:CoreSampling}
    Given factor matrix $\M{A}^{(n)}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples.
    \begin{algorithmic}[1]
    \For{$r\in{1,2,\ldots,R}$}
    \State $w_r \leftarrow \Nrocl{1}{r}\norm{\Vacol{2}}{1}\ldots\Nrocl{N}{r}$
    \label{line:Weight}
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
    \label{line:CorePartition}
    \For {$n = 1,...,N$}
    \label{line:ItemPartitionFor}
    \State Sample $i_n$ with probability $|\anr{n}{r}|/\norm{\Vacol{n}}{1}$
    \EndFor
    \label{line:ItemPartitionEnd}
    \State
    \label{line:Scoring}
        $\score{\ell} \leftarrow sgn(\anr{1}{r}\cdots\anr{N}{r})$
    \If {$\V{i}=\coord$ has not been sampled}
    \State  Create $\predx \leftarrow \score{\ell} $
    \Else
    \State $\predx \leftarrow \predx + \score{\ell}$
    \EndIf
    \EndFor
    \label{line:ScoringEnd}
    \end{algorithmic}
\end{algorithm}

\subsection{Theoretical Analysis}

In this subsection, we provides a theoretical analysis of our sampling algorithms. 
In one iteration, we use $\epsilon_{\V{i},r}$ to represent the event that
the coordinate $\V{i}$ and $r$ have been sampled,
and event $\epsilon_{\V{i}}$ for coordinate $\V{i}$ occurred.
We first analyze the probability $p(\epsilon_{\V{i}})$ that an entry is sampled by the sampling algorithms and then the expectation of the scores we computed during sampling. We also prove error bound on our estimate.

\begin{lemma}\label{lem:Probability}
    Suppose all factor matrices are nonnegative,
    $p(\epsilon_{\V{i}})$ equals to $x_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
The probability $\epsilon_{\V{i}}$ is the marginal distribution of $p(\epsilon_{\V{i},r})$,
so we have:
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{r} p(\epsilon_{\V{i},r}) \\
%& = \sum_{r} p({\rm pick\ }r)p({\rm pick\ }i_1|r)\cdots p({\rm pick\ }i_N|r)\\
& = \sum_{r} \frac{w_{r}}{\norm{\V{W}}{1}}
    \frac{|\Sca{a}{1}{r}|}{\norm{\VnC{a}{n}{*r}}{1}}\ldots\frac{|\Sca{a}{N}{r}|}{\norm{\VnC{a}{N}{*r}}{1}}\\
& = \sum_{r} \frac{\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}}{\norm{\V{W}}{1}}
  = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}

\subsubsection{Expectation of Scores}
For $\V{i}$ that not be sampled in $\ell$-th turn,
we can assume that $\score{\ell}=0$,
so that the final approximation value of each coordinate is
\begin{equation}\label{eq:score}
\predx = \sum_{\ell=1}^{s} \score{\ell}
\end{equation}

\begin{lemma}\label{lem:Expectation}
The expectation of $\predx$ equals to $sx_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
    The approximation value is shown in ~\Eqn{score},
    with the independence of $\score{\ell}$ in different iterations $\ell$,
    the expectation of $\hat{x}_{\V{i}}$ is:
\begin{align*}
\mathbb{E}[\predx]
& = \sum_{\ell=1}^{s}\mathbb{E}[\score{\ell}] = \sum_{\ell=1}^{s}\sum_{r} p(\epsilon_{\V{i},r})\score{\ell} \\
& = s\sum_{r} \frac{|\anr{1}{r}\cdots\anr{N}{r}|}{\norm{\V{W}}{1}}
                  sgn(\anr{1}{r}\cdots\anr{N}{r})\\
& = s\sum_{r} \frac{\anr{1}{r}\cdots\anr{N}{r}}{\norm{\V{W}}{1}}
  = \frac{sx_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}

\subsubsection{Error Bounds}
There are two error bounds showing how many samples we demand to get a reasonable result. 
\begin{theorem}\label{theo:ObservationBound}
Fix error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
The demand number of samples for coordinate $\V{i}$ to be observed as least once is
\[
    s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})
\]
\end{theorem}
\begin{proof}
The finding task is a binomial distribution$\sim B(s,p(\epsilon_{\V{i}}))$,
in which $p(\epsilon_{\V{i}})$ is small and $s$ large.
So the Poisson distribution with $\lambda = sp(\epsilon_{\V{i}})$
can used to approximate ${\rm Prob}(x\geq1)$.
That is ${\rm Prob}(x\geq1) = 1-{\rm Prob}(x=0)\approx 1-e^{-sp(\epsilon_{\V{i}})} \geq 1-\sigma$,
which gives $s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})$.
\end{proof}
%The $p(\epsilon_{\V{i}})$ of core$^k$ sampling is shown in \Lem{Probability}.
%So with the high order extension $k$, the recall will increase when we fix the number of sample.
\begin{theorem}\label{theo:Bound}
Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
If the number of samples
\[
    s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x_{\V{i}})
\]
then
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
\]
\end{theorem}

\begin{proof}
This result is closely follows the proof of Lemma 3 from \cite{BaPiKoSe15}.
Since  $ \score{1},\cdots,\score{s} $
are independent random variables taking values in $\{0,1\}$.
So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are a special case).
The Chernoff bounds for the sum of Poisson trials shows for any $0 <\delta <1 $:
\[
    Pr[|\predx - \mu|\geq\delta\mu)] \leq 2\exp{(-\mu\delta^2/3)}
\]
where $\mu=\mathbb{E}[\predx]=(sx_{\V{i}})/\norm{\V{W}}{1}$.
And by the choice of $s$, we have
$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
Then
\[
    Pr[|\predx-sx_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx_{\V{i}}/\norm{\V{W}}{1}] \leq \sigma
\]
multiplying by $\norm{\V{W}}{1}/s$ inside the ${\rm Pr}[\cdot]$ gives
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
\]
\end{proof}
\Theo{ObservationBound} and \Theo{Bound} show that
to reach a reasonable result of finding or estimate the value $x_{\V{i}}$,
the demand samples for core sampling is $s\approx \norm{\V{w}}{1}/x_{\V{i}}$.
\section{Extension of the basic sampling}

The upper bound of the recall we can obtain 
depends on the probability distribution for sampling the entries. 
To achieve better performance, 
we should improve this distribution by amplifying the odds for the top entries.

Instead of sampling one node $r$ from the core partition, 
we can pick a pair of nodes $(r,r')$ from the core partition, 
where $r'$ is sampled with replacement. 
Then conditioned on $(r,r')$, we sample $N$ nodes, 
indexed by $(i_1,\ldots,i_N)$ from the peripheral partitions. 
More generally, we can pick $k$ nodes $(r_1,r_2,\ldots,r_k)$ from the core partition 
and then draw $N$ peripheral nodes. 
We name this strategy star$^k$ sampling. 
The method discussed in previous section is a special case with $k=1$.

By sampling $k$ nodes with replacement from the core partition, we virtually create $R^k$ compound nodes, indexed by $\boldsymbol{r}=(r_1,\ldots,r_k)$. These compound nodes constitute a new partition called core$^k$ partition. To assign probabilities for picking the compound nodes and probabilities for picking a peripheral node given a compound nodes, we extend each factor matrix $\textbf{A}^{(n)}$ with size $L_n\times R$ to a new matrix $\textbf{E}^{(n)}$ with size $L_n\times R^k$. The entries of $\textbf{E}^{(n)}$ are obtained by
\begin{align}
\Sca{e}{n}{\V{r}} = \anr{n}{r_1}\cdots\anr{n}{r_k}
\end{align}
where we use $\boldsymbol{r}$ to index the columns of $\textbf{E}^{(n)}$. The weight we assign to the compound node $\boldsymbol{r}$ is
\begin{align}
w_{\V{r}} = \norm{\VnC{e}{1}{*\V{r}}}{1}\cdots\norm{\VnC{e}{N}{*\V{r}}}{1}
\end{align}
with $\mathbf{e}_{\boldsymbol{r}}^{(n)}$ being the an column of $\textbf{E}^{(n)}$ indexed by $\boldsymbol{r}$ and
\begin{align}
\norm{\VnC{e}{n}{*\V{r}}}{1} = \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|
\end{align}
The sampling procedure is similar with \Alg{CoreSampling} and we summarize it in \Alg{CoreExtensionSampling}. 
In star$^k$ sampling, the probability that entry $\boldsymbol{i}$ is sampled is proportional to $x_{\boldsymbol{i}}^k$. Therefore, in star$^k$ sampling, more focus will be put on the top entries.

Note that in the diamond sampling method for matrix, they also sampled two nodes from the core partition. However, the second node $r'$ was sampled after the indices $i_1$ and $i_2$ were picked and none extension of the factor matrices were introduced. As a result, only the estimation $\predx$ was improved but not the probability of picking the top entries.

\begin{algorithm}[ht]
    \caption{Core$^k$ Sampling with factor matrixes}
    \label{alg:CoreExtensionSampling}
    Given factor matrix $\M{A}^{(n)}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples, $k$ for the extension order.
    \begin{algorithmic}[1]
    \For{$\V{r}\in{\underbrace{R\times \cdots \times R}_{k}}$}
    \For{$n = 1,...,N$}
    \State $\norm{\VnC{e}{n}{*\V{r}}}{1} \leftarrow \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|$
    \EndFor
    \State $w_{\V{r}} \leftarrow \norm{\VnC{e}{1}{*\V{r}}}{1}\cdots\norm{\VnC{e}{N}{*\V{r}}}{1}$
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$
    \label{line:nodes}
    \For {$n = 1,...,N$}
    \State Sample $i_n$ with probability $|\Sca{e}{n}{\V{r}}|/\norm{\VnC{e}{n}{*\V{r}}}{1}$
    \EndFor
    \State
        $\score{\ell} \leftarrow sgn(\Sca{e}{1}{\V{r}}\cdots\Sca{e}{N}{\V{r}})$
    \If {$\V{i}=\coord$ has not been sampled}
    \State  Create $\predx \leftarrow \score{\ell} $
    \Else
    \State $\predx \leftarrow \predx + \score{\ell}$
    \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Core Sampling for Queries}

In this section, we present an algorithm for efficiently identifying top entries in sub-arrays of a tensor, which is of particular interest for recommender systems. Without loss of generality, we assume that the first factor matrix $\textbf{A}^{(1)}$ corresponds to users, with each row characterizing a single user. For user $\V{u}=\VnC{a}{1}{i_1*}$, we are interested in top entries in the (N-1)-order tensor $\T{X}_u$, whose CP decomposition is
\begin{align}
    \T{X}_{\V{u}} \approx \KT{\V{u},\Mn{A}{2},\cdots,\Mn{A}{N}} 
\end{align}

To generate recommendations for multiple users, an naive approach is to run the above sampling algorithm independently for different users. However, this is totally unnecessary. We find that for different users, only the probabilities for sampling nodes from the core partition are different. In the following step, they share the same distribution for sampling the peripheral nodes. Based on this observation, for each node in the core partition, we build a pool containing indices $\boldsymbol{i}=(i_2,\ldots,i_N)$ that have been picked give that core node. For a new user, when a core node is chosen, we can directly use the indices kept in its pool and only sample new $\boldsymbol{i}$ when necessary. By sharing indices among users, a huge number of samples can be saved. This procedure is illustrated in~\Alg{QuerySampling}.

\begin{algorithm}[ht]
    \caption{Finding top-$t$ tuples for a query}
    \label{alg:QuerySampling}
        Given factor matrix $\M{A}^{(n)}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.
        And $\M{A}^{(1)}$ is the set of quering users.
        Let $s$ be the number of samples, $m=R^k$.
    \begin{algorithmic}[1]
    \State Initialize $m$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_m$
    \State Initialize $c_{\V{r}} = 0$ for all $\V{r}$.
    \For {$i_1 =1,2,\ldots,L_1$}
    \State Let query $\V{u}=\VnC{a}{1}{i_1*}$
    \ForAll{$\V{r}$}
    \State $w_\V{r} \leftarrow |u_{r_1}\cdots u_{r_k}|$
    \EndFor
    \For {$\ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$.
    \label{line:Indexes}
    \State  Increment $c_{\V{r}}$.
    \EndFor
    \ForAll {$\V{r}$}
    \If {$c_\V{r} > |\V{g}_\V{r}|$ }
    \State Sample $c_{\V{r}} - |\V{g}_{\V{r}}|$ coordinates into $\V{g}_{\V{r}}$.
    \EndIf
    \State Use $c_{\V{r}}$ coordinates in $\V{g}_{\V{r}}$ and compute scores.
    \EndFor
    \State Post-processing for finding top-$t$ tuples of $\V{u}$.
    \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Fast Implementation}
In this section, we introduce some implementing details to get improvements for sampling.
\subsection{Core Partiton Sampling}
During sampling we samples only one compound nodes $\V{r}$ in each iteration,
like \AlgLine{CoreExtensionSampling}{nodes} and \AlgLine{QuerySampling}{Indexes}.
In practice, instead of sampling specific compound nodes $\V{r}$, 
we sample the frequency $c_\V{r}$ based on the work of Cohen et al,
so that the expectation $c_\V{r}$ is $\mu_{c_\V{r}}=sw_\V{r}/\norm{\V{w}}{1}$.
The distribution of $c_\V{r}$ is:
\begin{equation*}c_\V{r}=
    \left\{
      \begin{array}{ll}
        \lfloor \mu_{c_\V{r}} \rfloor,
        & \hbox{$p=\lceil \mu_{c_\V{r}} \rceil - \mu_{c_\V{r}}$} \\\\
        \lceil \mu_{c_\V{r}} \rceil,
        & \hbox{$p=\lfloor \mu_{c_\V{r}} \rfloor - \mu_{c_\V{r}}$}
      \end{array}
    \right.
\end{equation*}
It requires $O(R^k)$ complexity comparing $O(sk\log{R})$
for sampling $s$ vertices in core$^k$ partition.
\subsection{Extension of Core Partition}
Using the extension matrices has the same result of core$^k$ sampling,
and when $k$ is small it will be computation saving.
Each extension matrix takes extra $O(L_nR^k)$ storage,
the same storage costing for core$^k$ sampling without optimizing.
Utilizing the frequency sampling, we do save the storage in memory.
Firstly, $O(R^k)$ in storage is needed for sampling the $c_\V{r}$.
And for each vertex $v^c_{\V{r}}$,
we sample $c_{\V{r}}$ coordinates,
in which we only need to compute the probability vectors
$|\Sca{e}{n}{\V{r}}|/\norm{\VnC{e}{n}{*\V{r}}}{1}$ and uesd once.
The extra storage for the probability vectors takes totally
$O(R^k + L_1 + L_2 + \cdots + L_N)$
that makes high order extension possible.
\section{Experiments Results}
In this section, we will analysis the recall
and time consuming of our algorithms on real data sets.

\subsection{Data and Preprocessing}
We evaluate our algorithms on some real date sets:
Delicious Bookmarks
\footnote{http://www.delicious.com}(delicious for short),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens\footnote{http://www.grouplens.org}+IMDb\footnote{http://www.imdb.com }/
Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ml-2k).
Those data sets are released in the framework of the 2nd International Workshop (HetRec2011).
And one larger data sets of MovieLens\cite{Harper2015}:ml-10m.

We use 5-core of each data set in which all user, item and tag occurred at least 5 times.
We use the algorithm in \cite{Rendle_RTF} to train each data set under CP model.
The factor size $R$ is set to $64$.
After training, we get three factor matrices for user, item and tag.
We didn't limit the factor matrices to be nonnegative.
The statistics of data is shown in ~\Table{Data}.
\begin{table}[]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    DataSet     & User & Item    & Tag    & top-1   & top-1000\\
    \hline
    ml-2k       & 456  &  1973   &  1222  & 6.6813  & 2.5663 \\
    ml-10m      & 993  &  3298   &  2555  & 67.1037 & 26.747 \\
    lastfm      & 1348 &  6927   &  2132  & 78.1688 & 30.8263\\
    delicious   & 1681 &  29540  &  7251  & 3.9153  & 2.7896 \\
    \hline
  \end{tabular}
  \caption{Data Statistics}
  \label{table:Data}
\end{table}

\subsection{Recall and Time Consuming}
We use the maximum of $t'$
to show the performance of our core$^k$ sampling algorithms on finding top-$t$ values.
For each number of samples, we do $10$ times experiments independently,
and the average result is used.
The recall of each data set is show in ~\Fig{recall}
and the time consuming shown in ~\Fig{times}.
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{./img/fig_recall}\\
  \caption{Recall for different methods.}
  \label{fig:recall}
\end{figure}
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.25in]{./img/fig_times}\\
  \caption{Time for all data sets.}
  \label{fig:times}
\end{figure}
\subsubsection{Sampling with scores}
The score we introduced in \Eqn{score} in used for the pre-sorting strategy.
We use a budget $t' = s/10$ to do sampling on data lastfm.
The recall shown in \Fig{budget}, and the time saved is drawn in \Fig{times}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.25in,viewport = 0 10 650 200]{./img/fig_lastfm_recall_budget}\\
  \caption{Recall for different methods in delicious data sets with budget $t'=s/10$.}
  \label{fig:budget}
\end{figure}
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{./img/fig_lastfm_times_budget}\\
  \caption{Recall for different methods in delicious data sets with budget $t'=s/10$.}
  \label{fig:time_budget}
\end{figure}
Compare with the result in \Fig{recall}, we can notice that the scores of coordinate
do save the orders.
\subsubsection{Sampling for queries}
We use the coordinates pools to save the computation.
And core$^2$ extension is uesd 
to find the top-$100$ largerst values for queries on date set lastfm.
The recall and time of each queries on shown in ~\Fig{Queries}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{./img/fig_queries}\\
  \caption{Recall and time for queries in date set lastfm with $s=10^6$.
           For better visualization,
           we show every $10$ queries in processing.}
  \label{fig:Queries}
\end{figure}
\subsubsection{Comparison with diamond sampling}
To compare our algorithm with diamond sampling, we use item and tag matrices.
For each user, we multiply the user vector into item matrix
to get user-oriented factor matrices.
We evalute the average recall and time-consuming for those users.
And the budget of $t'=s/10$ is used since the diamond sampling only consider the result with budgets.
The result of comparison is shown in \Fig{Comparison_recall} and \Fig{Comparison_time}.

\subsection{Estimation of Values}
By \Theo{Bound}, we give the error bound of estimating the actual values.
We show the performance on data set lastfm with core$^3$ sampling and $s=10^8$.
Since the score $\hat{x}_{\V{i}}$ is an estimation of $sx^3_{\V{i}}/\norm{\V{w}}{1}$,
we draw the result of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$.
We show the top-$10^4$ pairs with largest scores in ~\Fig{Est}.
We can notice that there are several actual values that have the same approximation values.
This is because the sampling algorothm will only distinguish the value with certain gap.
With high order of extension, this gap will be small. 
\section{Conclusion}
In this paper propose a method for finding top-$t$ values within CP model.
After which we point out that by the extension of core partiton, 
we can get the $n$-the order estimation and reach the high probability of finding one coordinate.
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{./img/fig_lastfm_est}\\
  \caption{Plot of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$.
          The deshed line is the reference for equality.}
  \label{fig:Est}
\end{figure}

\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{./img/fig_comparision_recall}\\
  \caption{The comparison of recall in different algorithm for data set lasftm.}
  \label{fig:Comparison_recall}
\end{figure}

\begin{figure}[H]
    \centering
% Requires \usepackage{graphicx}
    \includegraphics[width=3in]{./img/fig_comparison_times}\\
    \caption{The comparison of time in different algorithm for data set lasftm.}
\label{fig:Comparison_time}
\end{figure}


\bibliography{IIP}
\bibliographystyle{aaai}
\end{document}
