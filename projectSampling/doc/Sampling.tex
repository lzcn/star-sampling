\documentclass[letterpaper]{article}
% Required Packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Core Sampling for Top t Retrial Processing)
/Author (Zhi Lu)
/Keywords ()
}
\title{Core Sampling for Top t Retrial Processing}
\author{}
%
%\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfigure}
% for algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
\newcommand{\anr}[2]{\Sca{a}{#1}{#2}}
\newcommand{\enr}[2]{\Sca{e}{#1}{\V{#2}}}
\newcommand{\Scah}[3]{h^{(#1,#2)}_{#3}(\V{x}_{i_#1})}
\newcommand{\score}[1]{\xi_{\V{i},#1}}
%\newcommand{\score}[1]{\M{X}_{\V{i},#1}}
%% -------
%% Tensor
%% -------
\usepackage[mathscr]{eucal}
\newcommand{\T}[1]{\boldsymbol{\mathscr{{#1}}}}

\newcommand{\KT}[1]{\left\llbracket #1 \right\rrbracket}

%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\bm{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\VnC}[3]{\V{#1}^{(#2)}_{#3}}
% norm one of r-th cloumn
\newcommand{\Nrocl}[2]{\norm{\VnC{a}{#1}{*#2}}{1}}
\newcommand{\Varow}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\Vacol}[1]{\V{a}^{(#1)}_{*r}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\bm{{\MakeUppercase{#1}}}}}
%Matrix with superscript
\newcommand{\Mn}[2]{\M{#1}^{(#2)}}

% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}

%% ------------
%% reference
%% ------------

% reference:definition
\newcommand{\Def}[1]   {Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]   {Equation~\ref{eq:#1}}
% reference:figure
\newcommand{\Fig}[1]   {Figure~\ref{fig:#1}}
\newcommand{\Figs}[2]  {Figure~\ref{fig:#1}$\sim$\ref{fig:#2}}
% reference:table
\newcommand{\Table}[1] {Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]  {Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1] {Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1] {Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1] {Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newcommand{\coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WeightR}{\Nrocl{1}{r}\ldots\Nrocl{N}{r}}
\newcommand{\predx}{\widehat{x}_{\V{i}}}
\newcommand{\predxn}{\widehat{x}_{\V{i},n}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
\maketitle

\section{Introduction}

In recommendation system,
all users and items(movies, books, etc) are trained to a joint latent space with the same dimensionality.
That is, all items and users are represented as a set of vectors.
Suppose $I,U$ are sets for items and users that $\V{x}_i,\V{q}_u$ are the corresponding instances.
The interaction between user $\V{q}_u$ and item $\V{x}_i$ is modeling by their inner products $\V{q}_u^T\V{x}_i$.
It is an essential task to find the most relevant items when a specific user is given.
That is given $\V{q}_u$, find the top $t$ largest value of $\V{q}_u^T\V{x}_i$.
The user and item vectors are obtained by learning from training set,
of which the goal is to make $\V{q}_u^T\V{x}_i$ larger for $\V{q}_u$
with the positive(related) items $\V{x}_{i_+}$ than the negative items $\V{x}_{i_-}$,
like the work of matrix factorization\cite{KoYe09}.
However, it is most often required to recommends a suit of items instead of one.
Consistently, we use $S_1,S_2,\ldots,S_N$ to be the classes
of items or users.
For one possibility, $S_1$ can be the set for users' vector,
$S_2$ is for shoes, $S_3$ for tops and $S_4$ for bottoms.
And finding the most favourite suit for users is the final task.
The prediction value of one tuple (user,items) is computed according to the
interaction model that used.

In personalized tag recommendation,
different factorization models have been used to tackle the interaction
between the tag and post(the user-item pair),
like Higher-Order-Singular-Value-Decomposition model\cite{SyNa08},
Tucker Decomposition model\cite{Rendle_RTF} and
Pairwise-Interaction-Tensor-Factorization model\cite{Rendle_PITF}.
And for fashion outfit recommendation\cite{HuYiLa15},
it also utilizes the factorization model to
map user and $N$ categories of items into latent space.

We consider on how to find the the top-$t$ mostly related tuples when the factor matrices are given.
The factor matrix is the set of instances $S_n$ that arranged into matrix format.
Since the number of items is always in vast scale,
finding the top-$t$ mostly related tuples exhaustively is time-consuming.
We proposed a sampling based algorithm for finding the top-$t$ most relevant tuples.
The interaction model we considered is the CANDECOMP/PARAFAC decomposition\cite{KoBa09},
since the PITF is a special case of the CP decomposition
and TD factorization takes so much computations that used lesser.
Suppose $\Mn{A}{1},\Mn{A}{2}\ldots\Mn{A}{N}$ are the learned factor matrices,
and prediction value of tuples are given by:
\begin{equation}\label{eq:CPDecomposition}
    \T{X} := \Mn{A}{1}\times_{L_2}\Mn{A}{2}\cdots\times_{L_N}\Mn{A}{N}
\end{equation}
where $\T{X}$ the $N$-order prediction tensor with size
$L_1\times L_2\times\cdots\times L_N$ and
\[
\M{A}^{(n)} =
\begin{bmatrix}
    \VnC{a}{n}{1},\VnC{a}{n}{2},\cdots,\VnC{a}{n}{r}
\end{bmatrix}  \in R^{L_n\times R}
\]
the factor matrix. The column size $R$ represents the dimensionality of latent space.
The prediction $x_\V{i}$ equals to:
\begin{equation}\label{eq:ValueInTensor}
    x_\V{i} = \sum_{r=1}^{R}\anr{1}{r}\cdot\anr{2}{r}\cdots\anr{N}{r}
\end{equation}
where $\V{i}$ is a shorthand for multi-index $\coord$.

Our algorithm takes the users as a special class of items and search in all the tuples,
in which given a particular user is a degeneration.
We aim to find the top-$t$ tuples by sampling according to their actual prediction values.
Our algorithm works inefficiently when the value of all tuples are indiscriminate.
However it is feasible because the goal of recommendation system is to train a relative
high scores for related tuples.
And it is also reasonable since the actual prediction value and
top-$t$ tuples are not critical in recommendation.

%\begin{table}[t]
%  \centering
%  \begin{tabular}{|c|c|}
%    \hline
%    Notations & Explanation \\
%    \hline
%    $\T{A}$ & tensor \\
%    $\M{A}$ & matrix \\
%    $\Mn{A}{n}$ & $n$-th factor matrix of tensor\\
%    $\V{A}_{*r},\V{A}_r$ & $k$-th column of matrix \\
%    $\V{A}_{i*}$ & $i$-th row of matrix \\
%    $\V{A}$ & vector \\
%    $a_{ir}$ & element of matrix, a scalar\\
%    \hline
%  \end{tabular}
%  \caption{Notation}
%  \label{table:Notation}
%\end{table}

\subsection{Notations}

We use the norm $\norm{*}{1}$ operation refereed in \cite{BaPiKoSe15}.
For a vector $\V{v}\in R^n$ or a matrix $\M{M}\in R^{m\times n}$.
It is defined as:
\[
    \norm{\V{v}}{1} = \sum_{i=1}^{n}|v_i|
    \ \  {\rm and} \ \
    \norm{\M{M}}{1} = \sum_{i=1}^{m}\sum_{j=1}^{n}|m_{ij}|
\]
%Some frequently used notations are listed in ~\Table{Notation}.


\section{Related work}
Our work is inspired by the state of art called diamond sampling
in Maximum All-pairs Dot-product(MAD) search\cite{BaPiKoSe15}
which estimate the actual value of matrix multiplication
$\M{X} = \M{A}\M{B}^T$ via sampling approach.
Matrices $\M{A}\in R^{M\times R}$ and $\M{B}\in R^{N\times R}$ 
are two class of feature vectors with dimensionality $R$.
It firstly compute a weight matrix $\M{W}$ that has the same size of $\M{A}$.
Then in each round, sample the index-pair $(m,r)$,
which is a guide for sampling others indexes $r'$ with pobability
$p(r'|m)$ and $n$ with $p(n|r)$.
Finally assign a score to sampled indexes ${(m,n)}$ after this iteration.
By the design of probabilities and scores,
it makes the final result $\widehat{x}_{mn}$ be an good estimation of $x^2_{mn}$ in scale.
The matrix multiplication is the special case
when we restrict the number of factor matrices to $2$ in the previous discussion.
Another work Maximum Inner Product Search(MIPS)\cite{Cohen97}
is a special case when $\M{A}\in R^{1\times R}$ is a vector in MAD search.
It samples the result $\widehat{x}_{1n}$ to be an estimation of $x_{1n}$.
The diamond sampling outperform the the latter thanks to the sampling structure,
and shows a remarkable performance in the multiplication of sparse matrices.
However it is pinned by the dense matrices when the top-$t$'s values are relative closer.
It is because that the probability of the coordinate that has the largest value became much lower
and no more dominating.
Consequently, the recall of top-$t$ decreases.
So, simply extending diamond sampling algorithm is not enough 
for finding the top-$t$ tuples in which the factor matrices are dense.
In next section we carry out a method for sampling the coordinate $\V{i}$,
mentioned in \Eqn{ValueInTensor},
proportionating to value of $x^n_{\V{i}}$.

\section{Core Sampling}
All factor matrices are adjoined to the latent space with dimensionality $R$.
Especially we call it the core partition and represent by $R$ vertices.
The more detailed representation is shown in this subsection via the graph.
And the concept of core we used is borrowed from the core tensor in TD decomposition,
since the CP decomposition is a special case with a all-one diagonal core tensor.

\subsection{Graph Presentation of Factor Matrices}
Consider one of the factor matrix $\Mn{A}{n}$ in \Eqn{CPDecomposition}
with size $L_n \times R$,
it is presented by a weighted bipartite graph shown by the dash-dot box in ~\Fig{GraphMatrices}.
The lower partition has $L_n$ vertices referred to $v^n_{i_n}$
and the $R$ vertices in core partition as $v^c_r$.
The weight of edge $e^n_{i_nr} = \{v^{n}_{i_n},v^c_r\}$ is set to $a^{(n)}_{i_nr}$.
Further, the $N$ factor matrices share the core partition.
So those $N$ factor matrices are represented by a weighted (N+1)-partite graph.
In the lower part of ~\Fig{GraphMatrices}, we have $N$ partitions of vertices.
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[]{fig_graph_factor_matrices}\\
  \caption{Graph Presentation of Factor Matrices}
  \label{fig:GraphMatrices}
\end{figure}
\subsection{Sampling Mechanism}

We start our sampling algorithm from the core partition
by assigning a particular probability to each vertices
then picking the index $r$ with that distribution.
In each iteration after the vertex $v^c_r$ has been sampled,
we continuely sample $N$ vertices $v^{n}_{i_n}$ from other partitions
basing on the weight of edge.
Then we assign a score to the coordinate $\coord$ occurred,
so that the expectation of estimation value $\widehat{x}_\V{i}$
after sampling will equal to the actual prediction $x_\V{i}$ in scale.
The sampling method is shown in \Alg{CoreSampling} and will be explained in details
in this subsections.
\begin{algorithm}[!ht]
    \caption{Core Sampling with factor matrixes}
    \label{alg:CoreSampling}
    Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples.
    \begin{algorithmic}[1]
    \For{$r\in{1,2,\ldots,R}$}
    \State $w_r \leftarrow \Nrocl{1}{r}\norm{\Vacol{2}}{1}\ldots\Nrocl{N}{r}$
    \label{line:Weight}
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
    \label{line:CorePartition}
    \For {$n = 1,...,N$}
    \label{line:ItemPartitionFor}
    \State Sample $i_n$ with probability $|\anr{n}{r}|/\norm{\Vacol{n}}{1}$
    \EndFor
    \label{line:ItemPartitionEnd}
    \State
    \label{line:Scoring}
        $\score{\ell} \leftarrow sgn(\anr{1}{r}\cdots\anr{N}{r})$
    \If {$\V{i}=\coord$ has not been sampled}
    \State  Create $\predx \leftarrow \score{\ell} $
    \Else
    \State $\predx \leftarrow \predx + \score{\ell}$
    \EndIf
    \EndFor
    \label{line:ScoringEnd}
    \end{algorithmic}
\end{algorithm}

\subsubsection{Probabilities}
The weights we designed for vertices in core partition are:
\[
    w_r = \WeightR
\]
which is shown in ~\AlgLine{CoreSampling}{Weight}.

So in each iteration, we sample the index $r$
with probability $w_r/\norm{\V{w}}{1}$
(~\AlgLine{CoreSampling}{CorePartition}).
Then repeat $N$ times,
walk form the vertex $v^c_r$ to other partitions with probability
$|\anr{n}{r}|/\norm{\Vacol{n}}{1}$
(~\AlgLines{CoreSampling}{ItemPartitionFor}{ItemPartitionEnd}),
after which vertices $i_1,\ldots,i_N$ are sampled.
In one iteration, we use $\epsilon_{\V{i},r}$ to represents the event that
the coordinate $\V{i}:\coord$ and $r$ have been sampled.
And use the event $\epsilon_{\V{i}}$ for the coordinate $\V{i}$ occurred.
\subsubsection{Scores}
We highlight a point that the scores we assigned to sampled indexes are used for estimation.
However, for finding task, we can just record the coordinate that has been sampled.

We use scores for the completeness of our algorithms.
Each time a coordinate $\coord $ occurred, then a score is assigned with:
\[
\score{\ell}  = sgn(\anr{1}{r}\cdot\anr{2}{r}\cdots\anr{N}{r})
\]
where $\ell$ is the $\ell$-th iteration.
If this coordinate has not been sampled previously,
create a container $\predx = \score{\ell}$.
Otherwise, increase $\predx$ by $\score{\ell}$
(~\AlgLines{CoreSampling}{Scoring}{ScoringEnd}).
For $\V{i}$ that not be sampled in $\ell$-th turn,
we can assume that $\score{\ell}=0$,
so that the final estimation value of each coordinate is
\begin{equation}\label{eq:score}
\predx = \sum_{\ell=1}^{s} \score{\ell}
\end{equation}

\subsection{Extract Top-$t$ Largest Values}
After sampling, we get a set of coordinates with estimation value $\predx$
or just a set of coordinates which have been sampled.
Denotes it by
\[
    \Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}
\]
The size of $\Psi_p$ is $P$.
Since $\V{i}$ may occurred more than once, $P$ is always less or equal to $s$.

There are two strategies for extracting top-$t$ largest values.
The naive strategy is computing all actual values of coordinates in $\Psi_p$,
and extract the top-$t$ largest actual values via sorting.
We call it the full-computing strategy.
The other one is called pre-sorting strategy,
which cooperates with the estimation value $\predx$.
It sort the coordinates in $\Psi_p$ according to scores $\widehat{x}_{\V{i}_p}$
and extract the top-$t'$ coordinates which gives
\[
    \Psi_{t'} = \{ \V{i} | \predx \geq \widehat{x}_{\V{i'}},
                           \forall i'\in \Psi_{p} \backslash \Psi_{t'}
                \}
\]  with size $|\Psi_{t'}| = t'>t$.
Then we compute the actual value $x_{\V{i}}$ of $\V{i}$ in set $\Psi_{t'}$.
And the final top-$t$ largest value's coordinates will be
\[
    \Psi_{t} =
                \{ \V{i} | x_{\V{i}}\geq x_{\V{i'}},
                           \forall i' \in \Psi_{t'} \backslash \Psi_{t}
                \}
\]
The use of budget $t'$ is a tradeoff between accuracy and computation.

The recall of full-computing strategy is determined by
the probability $p(\epsilon_{\V{i}})$ of coordinate $\V{i}$ in each iteration of sampling.
For the task of finding one coordinate, it is a binomial distribution$\sim B(s,p(\epsilon_{\V{i}}))$.
So the higher probability $p(\epsilon_{\V{i}})$ of largest elements is,
the more likely it will be sampled finally,
and thus the higher recall will be.
The pre-sorting strategy do save computation,
however, the recall will less than the full-computing in statistical.
That ideal case is that the final score keeps the same order of actual values.

\subsection{Extension of Core Partition}

Since the occurrence probability $p(\epsilon_{\V{i}})$ of coordinates 
determine the upper bound of the recall.
The effort for significantly improving the performance
should focus on the factor of $p(\epsilon_{\V{i}})$.
To reach the improvements,
we introduce the core partition extension to sampled the index pair $(r,r')$,
instead of one index $r$.

We expend the core partition into $2$ dimension with size $R\times R$.
And the vertices in expended core partition are represented by $v^c_{r,r'}$.
The same sampling framework is used to find the coordinates $\V{i}$.
That is we start from the expended core partition by sampling the vertex $v^c_{r,r'}$
and then the $\coord$ under $(r,r')$.
Similarly, for the hign dimension extension,
we extend the core partition to $k$ dimension.
For convience, we call it ${\rm core}^k$ extension,
thus the original sampling method is the simplest one named as ${\rm core}^1$.
The vertices in ${\rm core}^k$ partition is represented by $v^c_{\V{r}}$,
where $\V{r}$ is short for $(r_1,r_2,\ldots,r_k)$.
The details for realizing ${\rm core}^k$ will be introuced in following.

Fistly, we assign a weight to each vertex $v^c_\V{r}$ in the ${\rm core}^k$ partition:
\[
w_{\V{r}} = \norm{\VnC{e}{1}{*\V{r}}}{1}\cdots\norm{\VnC{e}{N}{*\V{r}}}{1}
\]\
where
\[
\norm{\VnC{e}{n}{*\V{r}}}{1} = \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|
\]
The algorithm for ${\rm core}^k$ sampling is shown in \Alg{CoreExtensionSampling}.
We can see that the extension in core partition equals to expand
the factor matrix $\Mn{A}{n}$ of size $L_n\times R$
to a new extension matrix $\Mn{E}{n}$ with size $L_n\times R^n$,
in which the element of $\Mn{E}{n}$ satisfies
\[
\Sca{e}{n}{\V{r}} = \anr{n}{r_1}\cdots\anr{n}{r_n}
\]
And using the original core sampling to deal with the extension matrices obtains same results.
The ${\rm core}^k$ sampling makes the probability $p(\epsilon_{\V{i}})$ of coordinate proportioning to $x^n_{\V{i}}$.
\begin{algorithm}[ht]
    \caption{${\rm Core}^k$ Sampling with factor matrixes}
    \label{alg:CoreExtensionSampling}
    Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples, $k$ for the extension order.
    \begin{algorithmic}[1]
    \For{$\V{r}\in{\underbrace{R\times \cdots \times R}_{k}}$}
    \For{$n = 1,...,N$}
    \State $\norm{\VnC{e}{n}{*\V{r}}}{1} = \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|$
    \EndFor
    \State $w_{\V{r}} = \norm{\VnC{e}{1}{*\V{r}}}{1}\cdots\norm{\VnC{e}{N}{*\V{r}}}{1}$
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$
    \For {$n = 1,...,N$}
    \State Sample $i_n$ with probability $|\Sca{e}{n}{\V{r}}|/\norm{\VnC{e}{n}{*\V{r}}}{1}$
    \EndFor
    \State
        $\score{\ell} \leftarrow sgn(\Sca{e}{1}{\V{r}}\cdots\Sca{e}{N}{\V{r}})$
    \If {$\V{i}=\coord$ has not been sampled}
    \State  Create $\predx \leftarrow \score{\ell} $
    \Else
    \State $\predx \leftarrow \predx + \score{\ell}$
    \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Core Sampling for Queries}
In recommendation system,
it is also required to find every the top-$t$ relevant items for a set of users.
For consistency,
say $\M{A}^{(1)}$ stands for the querying users,
and the rest $N-1$ matrices $\M{A}^{(n)},n=2,\ldots,N$ are for items.
Each row in matrix $\M{A}^{(1)}$ represents a particular user vector $\V{u}$.
And the $N-1$ order tenor in ~\Eqn{RankTensorCP}
represents the predicted ranking score of this user for all item combinations.
\begin{equation}\label{eq:RankTensorCP}
\T{X}_{\V{u}} := \V{u}\times_{L_2}\Mn{A}{2}\cdots\times_{L_N}\Mn{A}{N}
\end{equation}
It is an exception to replace one factor matrix by a single vector in \Eqn{CPDecomposition}.
However, when we have a lots of queries to process,
the previous users will have useful informations for the next user to do sampling.
The most noted one is that
the probabilitied on walking form core partition to other partitions is same for all users.
And the biggest difference between two users is the weight of vertices in core partition,
in other word,
the frequency number $c_{\V{r}}$ of each $v^c_{\V{r}}$ is expected to be sampled.
We define $m$ coordinates pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_{m}$ to save the occurred $\V{i}$,
used for the next user to save computations.
For ${\rm core}^k$ extension $m = R^k$.
And the algorithm for query sampling is show in ~\Alg{QuerySampling}.

\begin{algorithm}[ht]
    \caption{Finding top-$t$ tuples for a query}
    \label{alg:QuerySampling}
        Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.
        And $\M{A}^{(1)}$ is the set of quering users.\\
        Let $s$ be the number of samples,
        The number of vertices in ${\rm core}^k$ partition is $m=R^k$.
    \begin{algorithmic}[1]
    \State Initialize $m$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_m$
    \State Initialize $c_{\V{r}} = 0$ for all $\V{r}$.
    \For {$i_1 =1,2,\ldots,L_1$}
    \State Let query $\V{u}=\VnC{a}{1}{i_1*}$
    \ForAll{$\V{r}$}
    \State $w_\V{r} = |u_{r_1}\cdots u_{r_k}|$
    \EndFor
    \For {$\ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$.
    \label{line:Indexes}
    \State  Increment $c_{\V{r}}$.
    \EndFor
    \ForAll {$\V{r}$}
    \If {$c_\V{r} > |\V{g}_\V{r}|$ }
    \State Sample $c_{\V{r}} - |\V{g}_{\V{r}}|$ coordinates into $\V{g}_{\V{r}}$.
    \EndIf
    \State Use $c_{\V{r}}$ coordinates in $\V{g}_{\V{r}}$ and compute scores.
    \EndFor
    \State Post-processing for finding top-$t$ tuples of $\V{u}$.
    \EndFor
    \end{algorithmic}
\end{algorithm}
\section{Theoretical Analysis}

Until now, we just give the result without any proofs.
In this section,
we will do analysis on the probabilities of coordinates $p(\epsilon_{\V{i}})$
and the expectation of scores in our algorithms.
Then we will give some useful error bounds.

\subsection{Probability of Coordinates}
In each iteration of sampling, one vertex $v^c_{\V{r}}$ from ${\rm core}^k$ partition,
and $N$ vertices from other partitions will be sampled.
And we use the notation $\epsilon_{\V{i},\V{r}}$ to represent the random event
that $v^c_{\V{r}},\V{i}$ occurred,
and $\epsilon_{\V{i}}$ for that $\V{i}$ occurs.

\begin{lemma}\label{lem:Probability}
    Suppose all factor matrices are nonnegative.
    In $core^k$ sampling, $p(\epsilon_{\V{i}})$ equals $x^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
The probability $\epsilon_{\V{i}}$ is the marginal distribution of $p(\epsilon_{\V{i},r})$,
so we have:
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{\V{r}} p(\epsilon_{\V{i},\V{r}}) \\
%& = \sum_{\V{r}} p({\rm pick\ }\V{r})p({\rm pick\ }i_1|\V{r})\cdots p({\rm pick\ }i_N|\V{r})\\
& = \sum_{\V{r}} \frac{w_{\V{r}}}{\norm{\V{W}}{1}}
    \frac{|\Sca{e}{1}{\V{r}}|}{\norm{\VnC{e}{n}{*\V{r}}}{1}}\ldots\frac{|\Sca{e}{N}{\V{r}}|}{\norm{\VnC{e}{N}{*\V{r}}}{1}}\\
& = \sum_{\V{r}} \frac{\Sca{e}{1}{\V{r}}\cdots\Sca{e}{N}{\V{r}}}{\norm{\V{W}}{1}}\\
& = \sum_{r}\frac{(\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
  = \frac{x^k_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}
The probability vector $\V{w}$ is choosed according to the particular algorithm used.
Notice that this lemma is on condition that all factor matrices are nonnegative.
\subsection{Expectation of Scores}
In this part, we show the expectation of scores is an estimation of actual value in scale.
\begin{lemma}\label{lem:Expectation}
The expectation of score $\widehat{x}_{\V{i}}$ in $core^k$ sampling equals to $sx^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
    The final score shown in ~\Eqn{score},
    with the independence of $\score{\ell}$ in different iterations $\ell$,
    the expectation of final score in  $core^k$ sampling is:
\begin{align*}
\mathbb{E}[\predx]
& = \sum_{\ell=1}^{s}\mathbb{E}[\score{\ell}] = \sum_{\ell=1}^{s}\sum_{\V{r}} p(\epsilon_{\V{i},\V{r}})\score{\ell} \\
& = s\sum_{\V{r}} \frac{|\enr{1}{r}\cdots\enr{N}{r}|}{\norm{\V{W}}{1}}
                  sgn(\enr{1}{r}\cdots\enr{N}{r})\\
& = s\sum_{\V{r}} \frac{\enr{1}{r}\cdots\enr{N}{r}}{\norm{\V{W}}{1}}\\
& = s\sum_{r}\frac{(\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
= \frac{sx^k_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}
By \Lem{Expectation}, we notice that the final score, via sampling,
is an estimation of the actual prediction value.
So we can use the final score to approximate the actual values.
We will give two usefull error bounds for finding and approximating the top-$t$ values and
\subsection{Error Bounds}
\begin{theorem}\label{theo:ObservationBound}
Fix error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
The demand number of samples for coordinate $\V{i}$ to be observed as least once is
\[
    s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})
\]
\end{theorem}
\begin{proof}
The finding task is a binomial distribution$\sim B(s,p(\epsilon_{\V{i}}))$,
in which $p(\epsilon_{\V{i}})$ is small and $s$ large.
So the Poisson distribution with $\lambda = sp(\epsilon_{\V{i}})$
can used to approximate $Prob(x\geq1)$.
That is ${\rm Prob}(x\geq1) = 1-{\rm Prob}(x=0)\approx 1-e^{-sp(\epsilon_{\V{i}})} \geq 1-\sigma$.
That gives $s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})$.
\end{proof}
The $p(\epsilon_{\V{i}})$ of ${\rm core}^k$ sampling is shown in \Lem{Probability}.
So with the high order extension $k$, the recall will increase when we fix the number of sample.
\begin{theorem}\label{theo:Bound}
Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
In core sampling, if the number of samples
\[
    s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x^k_{\V{i}})
\]
then
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
\]
\end{theorem}

\begin{proof}
This result is closely follows the proof of Lemma 3 from \cite{BaPiKoSe15}.
Since  $ \score{\ell},\cdots,\score{s} $
are independent random variables taking values in $\{0,1\}$.
So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are a special case).
The Chernoff bounds for the sum of Poisson trials shows for any $0 <\delta <1 $:
\[
    Pr[|\predx - \mu|\geq\delta\mu)] \leq 2\exp{(-\mu\delta^2/3)}
\]
where $\mu=\mathbb{E}[\predx]=(sx^k_{\V{i}})/\norm{\V{W}}{1}$.
And by the choice of $s$, we have
$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
Then
\[
    Pr[|\predx-sx^k_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx^k_{\V{i}}/\norm{\V{W}}{1}] \leq \sigma
\]
multiplying by $\norm{\V{W}}{1}/s$ inside the ${\rm Pr}[\cdot]$ gives
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
\]
\end{proof}
The ~\Theo{ObservationBound} and ~\Theo{Bound} show that shows that
to reach a reasonable result of finding or estimate the value $x^k_{\V{i}}$,
the demand samples for ${\rm core}^k$ sampling is $s\approx \norm{\V{w}}{1}/x_{\V{i}}^k$.
\section{Fast Implementation}
In this section, we introduce some implementing details to get improvements.
\subsection{Core Partiton Sampling}
We samples only one vertex $v^c_\V{r}$ in one iteration like
~\AlgLine{CoreSampling}{CorePartition} and ~\AlgLine{QuerySampling}{Indexes}.
In practice, instead of sampling $v^c_\V{r}$, we sample the frequency $c_\V{r}$,
so that the expectation $c_\V{r}$ is $\mu_{c_\V{r}}=sw_\V{r}/\norm{\V{w}}{1}$.
The distribution of $c_\V{r}$ is:
\begin{equation*}c_\V{r}=
    \left\{
      \begin{array}{ll}
        \lfloor \mu_{c_\V{r}} \rfloor,
        & \hbox{$p=\lceil \mu_{c_\V{r}} \rceil - \mu_{c_\V{r}}$} \\\\
        \lceil \mu_{c_\V{r}} \rceil,
        & \hbox{$p=\lfloor \mu_{c_\V{r}} \rfloor - \mu_{c_\V{r}}$}
      \end{array}
    \right.
\end{equation*}
It requires $O(R^k)$ complexity comparing $O(sk\log{R})$
for sampling $s$ vertices in ${\rm core}^k$ partition.
\subsection{Extension of Core Partition}
Using the extension matrices has the same result of ${\rm core}^k$ sampling,
and when $k$ is small it will be computation saving but storage-costing.
Each extension matrix takes extra $O(L_nR^k)$ storage,
the same storage costing for ${\rm core}^k$ sampling without optimizing.
Utilizing the frequency sampling, we do save the storage in memory.
Firstly, we compute the probability vector in core partition, which takes $O(R^k)$ in storage,
And for each time we sample $c_{\V{r}}$ times coordinates with given $\V{r}$,
in which we only need to compute the probability vectors
$|\Sca{e}{n}{\V{r}}|/\norm{\VnC{e}{n}{*\V{r}}}{1}$ and uesd once.
The extra storage for the probability vectors takes totally
$O(R^k + L_1 + L_2 + \cdots + L_N)$ in storage,
that makes high order extension possible.
\section{Experiments Results}
In this section, we will analysis the recall
and time consuming of our algorithms on real data sets.

\subsection{Data and Preprocessing}
We evaluate our algorithms on some real date sets:
DeliciousBookmarks
\footnote{http://www.delicious.com}(delicious for short),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens+IMDb\footnote{http://www.imdb.com }/
Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ml-2k).
Those data sets are released in the framework of the 2nd International Workshop (HetRec2011).
And one larger data sets of MovieLens\cite{Harper2015}:ml-10m.

We use 5-core of each data set in which all user, item and tag occurred at least 5 times.
The statistics of preprocessed data is shown in ~\Table{Data}.
\begin{table}[t]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    DataSet     & User & Item    & Tag    & top-1   & top-1000\\
    \hline
    ml-2k       & 456  &  1973   &  1222  & 6.6813  & 2.5663 \\
    ml-10m      & 993  &  3298   &  2555  & 67.1037 & 26.747 \\
    lastfm      & 1348 &  6927   &  2132  & 78.1688 & 30.8263\\
    delicious   & 1681 &  29540  &  7251  & 3.9153  & 2.7896 \\
    \hline
  \end{tabular}
  \caption{Data Statistics}
  \label{table:Data}
\end{table}
We use the algorithm in \cite{Rendle_RTF} to train each data set under CP decomposition model.
The factor size $R$ is $64$.
After training, we get three factor matrices for user, item and tag of each data set.

\subsection{Recall and Time Consuming}
We evaluate those different methods' performance on time and recall of top-$t$ values.
We both use first order score for central sampling and the extension version.
We use the maximum budget which is the full-strategy to show the performance of our algorithms.
For each number of samples, we do $10$ times experiments independently, and the average recall is used.
The recall of each data set is show in ~\Figs{ml_2k_recall}{delicious_recall} .
And the time consuming for each algorithms in shown in ~\Fig{times}.
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_ml_2k_recall}\\
  \caption{Recall for different methods in ml-2k data sets.}
  \label{fig:ml_2k_recall}
\end{figure}
\begin{figure}[]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_ml_10m_recall}\\
  \caption{Recall for different methods in ml-10m data sets.}
  \label{fig:ml_10m_recall}
\end{figure}


\begin{figure}[]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_lastfm_recall}\\
  \caption{Recall for different methods in lastfm data sets.}
  \label{fig:lastfm_recall}
\end{figure}
\begin{figure}[]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_delicious_recall}\\
  \caption{Recall for different methods in delicious data sets.}
  \label{fig:delicious_recall}
\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_ml_2k_times}\\
%  \caption{Time for ml-2k data set.}
%  \label{fig:ml_2k_times}
%\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_ml_10m_times}\\
%  \caption{Time for ml-10m data set.}
%  \label{fig:ml_10m_times}
%\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_lastfm_times}\\
%  \caption{Time for lastfm data set.}
%  \label{fig:lastfm_times}
%\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_delicious_times}\\
%  \caption{Time for delicious data sets.}
%  \label{fig:delicious_times}
%\end{figure}
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_times}\\
  \caption{Time for all data sets.}
  \label{fig:times}
\end{figure}
\subsection{Sampling with Budgets}
The score we introduced in \Eqn{score} in used for the pre-sorting strategy.
We use a budget $t' = s/10$ to do sampiling for data lastfm.
The recall shown in \Fig{budget}.
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{fig_lastfm_recall_budget}\\
  \caption{Recall for different methods in delicious data sets with budget $t'=s/10$.}
  \label{fig:budget}
\end{figure}
Compare with the result in \Fig{lastfm_recall}, we can notice that the scores of coordinate
do save the orders good.
\subsection{Sampling for Queries}
We use the coordinates pools to save the computation.
And ${\rm core}^2$ extension is uesd 
to find the top-$100$ largerst values for queries in date set lastfm.
The recall and time of each queries is shown in ~\Fig{Queries}.
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_queries}\\
  \caption{Recall and time for queries in date set lastfm with $s=10^5$.
           For better visualization,
           we show every $10$ queries in processing.}
  \label{fig:Queries}
\end{figure}
\subsection{Estimation of Values}
In ~\Theo{Bound}, we gives the error bound of estimate the actual prediction values.
We the performance on data set lastfm with ${\rm core}^3$ sampling and $s=10^8$.
The score $\widehat{x}_{\V{i}}$ is an estimation of $sx^3_{\V{i}}/\norm{\V{w}}{1}$.
We draw the result of pairs $(\norm{\V{w}}{1}\widehat{x}_{\V{i}}/s,x^3_{\V{i}})$.
We show the top-$10^4$ pairs with largest scores in ~\Fig{Est}.
We can see that there are a several actual value have the same estimation values.
This is because the sampling algorothm will distinguish the value with certain gaps.
With high order of extension, this gap will be small. 
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_lastfm_est}\\
  \caption{Plot of pairs $(\norm{\V{w}}{1}\widehat{x}_{\V{i}}/s,x^3_{\V{i}})$.
          The deshed line is the reference for equality.}
  \label{fig:Est}
\end{figure}
\subsection{Comparison with Diamond Sampling}
The diamond sampling is for MAD which is the matrix multiplication task.
To compare our algorithm with diamond sampling, we use item and tag matrices.
For each user, we multiply the user vector into item matrix
to get user oriented matrix pair.
We evalute the average recall and time-consuming of those user oriented factor matrices.
And the budget of $t'=s/10$ is used since the diamond sampling only consider the result with budgets.
The result of comparison is shown in \Fig{Comparison_recall} and \Fig{Comparison_time}.

\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_comparision_recall}\\
  \caption{The comparison of recall in different algorithm for data set lasftm.}
  \label{fig:Comparison_recall}
\end{figure}

\begin{figure}[H]
    \centering
% Requires \usepackage{graphicx}
    \includegraphics[width=3in]{fig_comparison_times}\\
    \caption{The comparison of time in different algorithm for data set lasftm.}
\label{fig:Comparison_time}
\end{figure}

\section{Conclusion}
In this paper propose a method for finding top-$t$ values for given CP-models.
After which we point out that we can adjust this method to get the $n$-the order estimation
and reach the high probability of finding one coordinate.
\bibliography{IIP}
\bibliographystyle{aaai}
\end{document}
