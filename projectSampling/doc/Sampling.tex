\documentclass[letterpaper]{article}
% Required Packages
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Core Sampling for Top t Retrial Processing)
/Author (Zhi Lu,Yang Hu,Zeng Bing)
/Keywords ()
}
\title{Core Sampling for Top t Retrieval Processing}
\author{Zhi Lu \and Yang Hu \and Bing Zeng\\
School of Electronic Engineering, University of Electronic Science and Technology of China\\
Email: zhilu@std.uestc.edu.cn\\
Email: \{yanghu,eezeng\}@uestc.edu.cn
}
%
%\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm,amsmath,amssymb}
% \usepackage{bm}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfigure}
% for algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
\newcommand{\anr}[2]{\Sca{a}{#1}{#2}}
\newcommand{\enr}[2]{\Sca{e}{#1}{\V{#2}}}
\newcommand{\Scah}[3]{h^{(#1,#2)}_{#3}(\V{x}_{i_#1})}
\newcommand{\score}[1]{\xi_{\V{i},#1}}
%\newcommand{\score}[1]{\M{X}_{\V{i},#1}}
%% -------
%% Tensor
%% -------
%\usepackage[mathscr]{eucal}
\newcommand{\T}[1]{\mathcal{#1}}

\newcommand{\KT}[1]{\left\llbracket #1 \right\rrbracket}

%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\boldsymbol{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\VnC}[3]{\V{#1}^{(#2)}_{#3}}
% norm one of r-th cloumn
\newcommand{\Nrocl}[2]{\norm{\VnC{a}{#1}{*#2}}{1}}
\newcommand{\Varow}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\Vacol}[1]{\V{a}^{(#1)}_{*r}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\textbf{{\MakeUppercase{#1}}}}}
%Matrix with superscript
\newcommand{\Mn}[2]{\M{#1}^{(#2)}}

% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}

%% ------------
%% reference
%% ------------

% reference:definition
\newcommand{\Def}[1]   {Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]   {Eq.(\ref{eq:#1})}
% reference:figure
\newcommand{\Fig}[1]   {Figure~\ref{fig:#1}}
\newcommand{\Figs}[2]  {Figure~\ref{fig:#1}$\sim$\ref{fig:#2}}
% reference:table
\newcommand{\Table}[1] {Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]  {Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1] {Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1] {Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1] {Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newcommand{\coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WeightR}{\Nrocl{1}{r}\ldots\Nrocl{N}{r}}
\newcommand{\predx}{\widehat{x}_{\V{i}}}
\newcommand{\predxn}{\widehat{x}_{\V{i},n}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
\maketitle
\section{Abstract}
The technique of matrix/tensor factorization has been used widely to describe the 
interaction between objects in the application of recovering missing entries.
And the finding the most relevant of two or more objects is the central concerns.
Since the target matrix/tensor is not obtain directly in application, instead,
sets of instance vectors are the output for data representation.
It is need to search in all possibilities to find the related objects.
However, the number of instances is always in vast scale, 
the naive full search is time-consuming.
In this work, we study the problem of efficiently
identifying the top entries of a tensor via sampling method.


\section{Introduction}
Matrix and tensor completion has received considerable attention in recent years. 
Many problems in application can be formulated as recovering the missing entries of a matrix or tensor. 
In some scenarios, such as image and video in-painting, 
all lost entries are needed to be filled in. 
In some others, however, it is difficult and also unnecessary to recover them all. 
For example, for recommender systems, the size of the matrix or tensor, 
which is determined by the numbers of users and items, is usually rather large. 
It is computational expensive to compute all of the unknown values. 
On the other hand, for recommendation purpose, 
we are only interested in a few entries that are the largest within a sub-array of the matrix or tensor. 
The largest entries of a matrix/tensor are not only the central concerns for personalized recommendation, 
but also meaningful in many other cases. 
In a similarity matrix, the top entries correspond to pairs of items that are most similar, 
which are of interest for applications like link prediction in graph, 
duplicate detection as well as information retrieval. 
And in neuroimage meta-analysis, 
the largest matrix/tensor entries may suggest the most probable associations between brain functions and behaviors.

In this work, 
we study the problem of efficiently identifying the top entries of a tensor 
without exhaustive computing and searching all entries. 
The tensor, as a multi-way generalization of the matrix, 
has been exploited more and more recently. 
Take recommender systems for example. 
While traditional focus is on the user-item matrix\cite{KoYe09}, 
tensor\cite{Rendle_PITF,HuYiLa15} is required for data representation in many emerging settings 
such as context-aware recommendation, 
where contextual information like time and location is considered, and set-based recommendation, 
where the object to be recommended is a set of items that interact with each other. 
Following the most common paradigm, we assume that the tensor can be decomposed into some factors, 
which can be estimated from the observed entries by some learning algorithms. 
Specifically, we focus on the CANDECOMP/PARAFAC decomposition model.

CP decomposition\cite{KoBa09} is a widely used technique for exploring and 
extracting the underlying structure of multi-way data. 
Given a $N$-order tensor $\T{X}\in\mathbb{R}^{L_1\times \cdots\times L_N}$, 
CP decomposition approximates it by $N$ factor matrices $\Mn{A}{1},\Mn{A}{2},\ldots,\Mn{A}{N}$, 
such that
\begin{align}
\label{eq:CPDecomposition}
\T{X}&\approx\KT{\Mn{A}{1},\Mn{A}{2},\cdots,\Mn{A}{N}} \\ \notag
&=\sum_{r=1}^R\VnC{a}{1}{r}\circ\VnC{a}{2}{r}\circ\cdots\circ\VnC{a}{N}{r}
\end{align}
where each factor matrix 
$\Mn{A}{n}=[\VnC{a}{n}{1}\VnC{a}{n}{2}\cdots\VnC{a}{n}{R}], n=1,\ldots,N$
is of size $L_n\times R$ with $\VnC{a}{n}{r}\in\mathbb{R}^{L_n}$ 
being the $r$-th column.
And to distinguish, we ues $\Varow{n}$ to represnt the $i_n$ row of factor matrix.
The symbol ``$\circ$'' represents the vector outer product. 
$R$ is the tensor rank, indicating the number of latent factors. 
Elementwise, \Eqn{CPDecomposition} is written as
\begin{align}
\label{eq:CPValue}
x_\V{i} \approx \sum_{r=1}^{R}\anr{1}{r}\anr{2}{r}\cdots\anr{N}{r}
\end{align}
where $\V{i}$ is short for the index set $(i_1,i_2,\ldots,i_N)$.

\subsection{Related Work}
Given $N$ factor matrices $\textbf{A}^{(1)},\ldots,\textbf{A}^{(N)}$ and a parameter $t$, 
we would like to find $t$ indices $\{\boldsymbol{i}_1,\ldots,\boldsymbol{i}_t\}$ 
which correspond to the $t$ largest $x_{\boldsymbol{i}}$.
This problem subsumes many existing problems in the literature.
When $N=2$, it is exactly the MAD (Maximum All-pairs Dot-product Search)\cite{BaPiKoSe15} problem 
that finds the largest entries in the product of two matrices. 
And MAD contains the MIPS (Maximum Inner Product Search)\cite{Cohen97} problem 
as the special case with one matrix being a single column.
The most obvious approach is to compute the entries exhaustively. 
However, this becomes prohibitive as the sizes of the factor matrices grow. 
There is some literature in approximate matrix multiplication. 
But these methods are not suited even for MAD, 
since only a few entries among the millions are of interest. 
The more efficient solution is to directly searching the top ones. 
This has been extensively studied for the MIPS problem. 
Popular approaches include LSH (Locality Sensing Hashing), 
space partition techniques like k-d tree, etc. 
Recently, Ballard et al. proposed a randomized approach called diamond sampling to the MAD problem. 
They selected diamonds, i.e. four-cycles, from a weighted tripartite representation of the two factor matrices, 
with the probability of selecting a diamond corresponding to index pair $(i_1,i_2)$ being proportional to $(\Varow{1}\cdot\Varow{2})^2$.
For tensor, there hasn't been any study conducted yet.

Inspired by \cite{BaPiKoSe15}, 
we apply index sampling methods to the case of tensor, 
whose entries are computed by the CP decomposition model. 
We design a strategy to sample the index $\V{i}$ proportional to the magnitude of the corresponding entries. 
We further extend it to make the sampling proportional to the $k$-th power of the entries,
amplifying the focus on the largest ones. 
For the application of recommender systems, 
an algorithm that reuse the samples for different users is presented. 
We provide theoretical analysis for the sampling algorithms, 
and provide concentration bounds on the behavior. 
We evaluate the sampling methods on several real-world datasets. 
The results show that they are orders of magnitude faster than exact computing. 
When compared to previous approach for matrix sampling, our methods require much fewer samples.

\subsection{Notations}

We use the norm $\norm{*}{1}$ operation refereed in diamond sampling.
For a vector $\V{v}\in \mathbb{R}^n$ or a matrix $\M{M}\in \mathbb{R}^{m\times n}$.
It is defined as:
\[
    \norm{\V{v}}{1} = \sum_{i=1}^{n}|v_i|
    \ \  {\rm and} \ \
    \norm{\M{M}}{1} = \sum_{i=1}^{m}\sum_{j=1}^{n}|m_{ij}|
\]


\section{Core Sampling}
The CP decomposition is a special case of Tucker decomposition
when the core tensor is a diagonal tensor and the entries along the superdiagonal are ones.
The core tensor indicates the interaction between factor matrices.
For CP model, we represent the core tensor by $R$ vertices and call it core partition,
and all factor matrices are adjoined to that partition.
More details are shown in this subsection via graph representation.

\subsection{Graph Presentation for Factor Matrices}
Consider one of the factor matrix $\Mn{A}{n}$ in \Eqn{CPDecomposition}
with size $L_n \times R$,
it is represented by a weighted bipartite graph shown within the dash-dot box on ~\Fig{GraphMatrices}.
The lower partition, simply named $n$-th factor partition, has $L_n$ vertices referred to $v^n_{i_n}$
and the $R$ vertices in core partition to $v^c_r$.
The weight of edge $e^n_{i_nr} = \{v^{n}_{i_n},v^c_r\}$ is set to $a^{(n)}_{i_nr}$.
Further, the $N$ factor matrices share the core partition.
So those $N$ factor matrices are represented by a weighted (N+1)-partite graph.
In the lower part of ~\Fig{GraphMatrices}, we have $N$ partitions of vertices.
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[]{fig_graph_factor_matrices}\\
  \caption{Graph Presentation of Factor Matrices}
  \label{fig:GraphMatrices}
\end{figure}
\subsection{Sampling Mechanism}

We start our sampling algorithm from the core partition
by assigning a particular probability to each vertices,
then we sample the index $r$ with designed distribution.
After the vertex $v^c_r$ has been sampled,
we continuely sample $N$ vertices $v^{n}_{i_n}$ from each factor partitions
basing on the weight of edge.
In addition, we assign a score to the coordinate $\coord$ that has been sampled.
We do $s$ iterations to get a set of coordinate-value pairs.
The probabilities are designed to make the expectation of estimation value $\widehat{x}_\V{i}$ 
after sampling be the actual prediction $x_\V{i}$ in scale.
%The sampling method is shown in \Alg{CoreSampling} and will be explained in details
%in this subsections.
%\begin{algorithm}[ht]
%    \caption{Core Sampling with factor matrixes}
%    \label{alg:CoreSampling}
%    Given factor matrix $\M{A}^{(n)}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
%    Let $s$ be the number of samples.
%    \begin{algorithmic}[1]
%    \For{$r\in{1,2,\ldots,R}$}
%    \State $w_r \leftarrow \Nrocl{1}{r}\norm{\Vacol{2}}{1}\ldots\Nrocl{N}{r}$
%    \label{line:Weight}
%    \EndFor
%    \For{$ \ell = 1,\ldots,s$}
%    \State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
%    \label{line:CorePartition}
%    \For {$n = 1,...,N$}
%    \label{line:ItemPartitionFor}
%    \State Sample $i_n$ with probability $|\anr{n}{r}|/\norm{\Vacol{n}}{1}$
%    \EndFor
%    \label{line:ItemPartitionEnd}
%    \State
%    \label{line:Scoring}
%        $\score{\ell} \leftarrow sgn(\anr{1}{r}\cdots\anr{N}{r})$
%    \If {$\V{i}=\coord$ has not been sampled}
%    \State  Create $\predx \leftarrow \score{\ell} $
%    \Else
%    \State $\predx \leftarrow \predx + \score{\ell}$
%    \EndIf
%    \EndFor
%    \label{line:ScoringEnd}
%    \end{algorithmic}
%\end{algorithm}

\subsubsection{Probabilities}
The weights we designed for vertices in core partition are:
\[
    w_r = \WeightR
\]
%which is shown in ~\AlgLine{CoreSampling}{Weight}.

So in each iteration, we sample the index $r$
with probability $w_r/\norm{\V{w}}{1}$.
%(~\AlgLine{CoreSampling}{CorePartition}).
Then repeat $N$ times,
walk form the vertex $v^c_r$ to $N$-th factor partitions with probability
$|\anr{n}{r}|/\norm{\Vacol{n}}{1}$,
%(~\AlgLines{CoreSampling}{ItemPartitionFor}{ItemPartitionEnd}),
after which vertices $i_1,\ldots,i_N$ are sampled.
In one iteration, we use $\epsilon_{\V{i},r}$ to represent the event that
the coordinate $\V{i}$ and $r$ have been sampled,
and event $\epsilon_{\V{i}}$ for coordinate $\V{i}$ occurred.
\subsubsection{Scores}
We highlight a point that the scores we assigned to sampled indexes are used for approximation.
For finding task, we can just record the coordinate that has been sampled.
We use scores for the completeness of our algorithms.
The score for occurred coordinate $\V{i} $ is:
\[
\score{\ell}  = sgn(\anr{1}{r}\cdot\anr{2}{r}\cdots\anr{N}{r})
\]
where $\ell$ is the $\ell$-th iteration.
If this coordinate has not been sampled previously,
create a container $\predx = \score{\ell}$.
Otherwise, increase $\predx$ by $\score{\ell}$.
%(~\AlgLines{CoreSampling}{Scoring}{ScoringEnd}).
For $\V{i}$ that not be sampled in $\ell$-th turn,
we can assume that $\score{\ell}=0$,
so that the final estimation value of each coordinate is
\begin{equation}\label{eq:score}
\predx = \sum_{\ell=1}^{s} \score{\ell}
\end{equation}

\subsection{Extract Top-$t$ Largest Values}
After sampling, we get a set of coordinates with estimation value $\predx$,
denoted by
\[
    \Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}
\]
The size of $\Psi_p$ is $P$.
Since $\V{i}$ may occurred more than once, $P$ is always less or equal to $s$.

There are two strategies for extracting top-$t$ largest values.
The naive strategy is computing all actual values of coordinates in $\Psi_p$,
and extract the top-$t$ largest actual values via sorting.
We call it the full-computing strategy.
The other one is called pre-sorting strategy,
which cooperates with the estimation value $\predx$.
It sort the coordinates in $\Psi_p$ according to scores $\widehat{x}_{\V{i}_p}$
and extract the top-$t'$ coordinates which gives
\[
    \Psi_{t'} = \{ \V{i} | \predx \geq \widehat{x}_{\V{i'}},
                           \forall i'\in \Psi_{p} \backslash \Psi_{t'}
                \}
\]  with size $|\Psi_{t'}| = t'\geq t$.
Then the full-computing strategy is carried out among $\Psi_{t'}$.
Then we compute the actual value $x_{\V{i}}$ of $\V{i}$ in set $\Psi_{t'}$.

%And the final top-$t$ largest value's coordinates will be
%\[
%    \Psi_{t} =
%                \{ \V{i} | x_{\V{i}}\geq x_{\V{i'}},
%                           \forall i' \in \Psi_{t'} \backslash \Psi_{t}
%                \}
%\]
The use of budget $t'$ is a tradeoff between accuracy and computation.
The recall of full-computing strategy is determined by
the probability $p(\epsilon_{\V{i}})$ of coordinate $\V{i}$ in each iteration.
For the task of finding one coordinate, it is a binomial distribution$\sim B(s,p(\epsilon_{\V{i}}))$.
So the higher probability $p(\epsilon_{\V{i}})$ of largest elements is,
the more likely it will be sampled finally,
and thus the higher recall will be.
The pre-sorting strategy do save computation,
however, the recall will less than the full-computing in statistical.
That ideal case is that the final score keeps the same order of actual values.

\subsection{Extension of Core Partition}
The occurrence probability $p(\epsilon_{\V{i}})$ determine the upper bound of recall.
So we focus on the factor $p(\epsilon_{\V{i}})$ for significantly improving the performance.
To do so, we introduce the core partition extension to sampled the index pair $(r,r')$,
instead of one index $r$.

We expend the core partition into $2$ dimension with size $R\times R$.
The vertices after extension are represented by $v^c_{r,r'}$.
And the same sampling framework is used to find the coordinates $\V{i}$.
That is start from the expended core partition by sampling the vertex $v^c_{r,r'}$
and then the $\coord$ under $(r,r')$.
Similarly, for the hign dimension extension,
we extend the core partition into $k$ dimension.
For convience, we call it ${\rm core}^k$ extension,
thus the original sampling method is the simplest one named as ${\rm core}^1$.
The vertices in ${\rm core}^k$ partition is represented by $v^c_{\V{r}}$,
where $\V{r}$ is short for $(r_1,r_2,\ldots,r_k)$.
The details for realizing ${\rm core}^k$ will be introuced in following.

Fistly, we assign a weight to each vertex $v^c_\V{r}$ in the ${\rm core}^k$ partition:
\[
w_{\V{r}} = \norm{\VnC{e}{1}{*\V{r}}}{1}\cdots\norm{\VnC{e}{N}{*\V{r}}}{1}
\]\
where
\[
\norm{\VnC{e}{n}{*\V{r}}}{1} = \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|
\]
The algorithm for ${\rm core}^k$ sampling is shown in \Alg{CoreExtensionSampling}.
We can see that the extension in core partition equals to expanding
the factor matrix $\Mn{A}{n}\in\mathbb{R}^{L_n\times R}$
to a new extension matrix $\Mn{E}{n}\in\mathbb{R}^{L_n\times R^k}$,
whose elements satisfy
\[
\Sca{e}{n}{\V{r}} = \anr{n}{r_1}\cdots\anr{n}{r_n}
\]
Using the original core sampling to deal with the extension matrices obtains same results.
The ${\rm core}^k$ sampling makes the probability $p(\epsilon_{\V{i}})$ of coordinate proportioning to $x^k_{\V{i}}$.
\begin{algorithm}[ht]
    \caption{${\rm Core}^k$ Sampling with factor matrixes}
    \label{alg:CoreExtensionSampling}
    Given factor matrix $\M{A}^{(n)}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples, $k$ for the extension order.
    \begin{algorithmic}[1]
    \For{$\V{r}\in{\underbrace{R\times \cdots \times R}_{k}}$}
    \For{$n = 1,...,N$}
    \State $\norm{\VnC{e}{n}{*\V{r}}}{1} = \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|$
    \EndFor
    \State $w_{\V{r}} = \norm{\VnC{e}{1}{*\V{r}}}{1}\cdots\norm{\VnC{e}{N}{*\V{r}}}{1}$
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$
    \For {$n = 1,...,N$}
    \State Sample $i_n$ with probability $|\Sca{e}{n}{\V{r}}|/\norm{\VnC{e}{n}{*\V{r}}}{1}$
    \EndFor
    \State
        $\score{\ell} \leftarrow sgn(\Sca{e}{1}{\V{r}}\cdots\Sca{e}{N}{\V{r}})$
    \If {$\V{i}=\coord$ has not been sampled}
    \State  Create $\predx \leftarrow \score{\ell} $
    \Else
    \State $\predx \leftarrow \predx + \score{\ell}$
    \EndIf
    \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Core Sampling for Queries}
In recommendation system,
it is also required to find every the top-$t$ relevant items for a set of users.
For consistency,
say $\M{A}^{(1)}$ stands for the querying users,
and the rest $N-1$ matrices $\M{A}^{(n)},n=2,\ldots,N$ are for items.
Each row in matrix $\M{A}^{(1)}$ represents a particular user vector $\V{u}$.
And the $N-1$ order tenor in ~\Eqn{RankTensorCP}
represents the predicted ranking score of this user for all item combinations.
\begin{equation}\label{eq:RankTensorCP}
\T{X}_{\V{u}} \approx \KT{\V{u},\Mn{A}{2},\cdots,\Mn{A}{N}} 
\end{equation}
It is an exception to replace one factor matrix by a single vector in \Eqn{CPDecomposition}.
However, when we have a lots of queries to process,
the previous users will have useful informations for the next user to do sampling.
The most noted one is that
the probabilitied on walking form core partition to factor partitions are same for all users.
And the biggest difference between two users is the weight of vertices in core partition,
in other word,
the frequency number $c_{\V{r}}$ of each $v^c_{\V{r}}$ is expected to be sampled.
We define $m$ coordinates pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_{m}$ to save the occurred $\V{i}$ for the next user.
For ${\rm core}^k$ extension $m = R^k$,
and the algorithm for query sampling is show in ~\Alg{QuerySampling}.

\begin{algorithm}[ht]
    \caption{Finding top-$t$ tuples for a query}
    \label{alg:QuerySampling}
        Given factor matrix $\M{A}^{(n)}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.
        And $\M{A}^{(1)}$ is the set of quering users.
        Let $s$ be the number of samples, $m=R^k$.
    \begin{algorithmic}[1]
    \State Initialize $m$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_m$
    \State Initialize $c_{\V{r}} = 0$ for all $\V{r}$.
    \For {$i_1 =1,2,\ldots,L_1$}
    \State Let query $\V{u}=\VnC{a}{1}{i_1*}$
    \ForAll{$\V{r}$}
    \State $w_\V{r} = |u_{r_1}\cdots u_{r_k}|$
    \EndFor
    \For {$\ell = 1,\ldots,s$}
    \State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$.
    \label{line:Indexes}
    \State  Increment $c_{\V{r}}$.
    \EndFor
    \ForAll {$\V{r}$}
    \If {$c_\V{r} > |\V{g}_\V{r}|$ }
    \State Sample $c_{\V{r}} - |\V{g}_{\V{r}}|$ coordinates into $\V{g}_{\V{r}}$.
    \EndIf
    \State Use $c_{\V{r}}$ coordinates in $\V{g}_{\V{r}}$ and compute scores.
    \EndFor
    \State Post-processing for finding top-$t$ tuples of $\V{u}$.
    \EndFor
    \end{algorithmic}
\end{algorithm}
\section{Theoretical Analysis}

Until now, we just give the result without any proofs.
In this section,
we give the probabilities of coordinates $p(\epsilon_{\V{i}})$, 
the expectation of scores in our algorithms and two useful error bounds.

\subsection{Probability of Coordinates}
In each iteration, one vertex $v^c_{\V{r}}$ from ${\rm core}^k$ partition,
and $N$ vertices from other partitions will be sampled.
And notations $\epsilon_{\V{i},\V{r}}$ and $\epsilon_{\V{i}}$ are the corresponding random events.

\begin{lemma}\label{lem:Probability}
    Suppose all factor matrices are nonnegative.
    In $core^k$ sampling, $p(\epsilon_{\V{i}})$ equals to $x^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
The probability $\epsilon_{\V{i}}$ is the marginal distribution of $p(\epsilon_{\V{i},r})$,
so we have:
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{\V{r}} p(\epsilon_{\V{i},\V{r}}) \\
%& = \sum_{\V{r}} p({\rm pick\ }\V{r})p({\rm pick\ }i_1|\V{r})\cdots p({\rm pick\ }i_N|\V{r})\\
& = \sum_{\V{r}} \frac{w_{\V{r}}}{\norm{\V{W}}{1}}
    \frac{|\Sca{e}{1}{\V{r}}|}{\norm{\VnC{e}{n}{*\V{r}}}{1}}\ldots\frac{|\Sca{e}{N}{\V{r}}|}{\norm{\VnC{e}{N}{*\V{r}}}{1}}\\
& = \sum_{\V{r}} \frac{\Sca{e}{1}{\V{r}}\cdots\Sca{e}{N}{\V{r}}}{\norm{\V{W}}{1}}\\
& = \sum_{r}\frac{(\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
  = \frac{x^k_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}
The probability vector $\V{w}$ is different when different $k$ is used.

\subsection{Expectation of Scores}
In this part, we show the expectation of scores is an approximation of actual value in scale.
\begin{lemma}\label{lem:Expectation}
The expectation of approximation $\widehat{x}_{\V{i}}$ in $core^k$ sampling equals to $sx^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
    The approximation value is shown in ~\Eqn{score},
    with the independence of $\score{\ell}$ in different iterations $\ell$,
    the expectation of $\widehat{x}_{\V{i}}$ is:
\begin{align*}
\mathbb{E}[\predx]
& = \sum_{\ell=1}^{s}\mathbb{E}[\score{\ell}] = \sum_{\ell=1}^{s}\sum_{\V{r}} p(\epsilon_{\V{i},\V{r}})\score{\ell} \\
& = s\sum_{\V{r}} \frac{|\enr{1}{r}\cdots\enr{N}{r}|}{\norm{\V{W}}{1}}
                  sgn(\enr{1}{r}\cdots\enr{N}{r})\\
& = s\sum_{\V{r}} \frac{\enr{1}{r}\cdots\enr{N}{r}}{\norm{\V{W}}{1}}\\
& = s\sum_{r}\frac{(\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
= \frac{sx^k_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}
By \Lem{Expectation}, we notice that the approximation after sampling
is an estimation of the actual prediction value.
So we can use the $\widehat{x}_{\V{i}}$ to approximate the actual values $\V{x}_{\V{i}}$.

\subsection{Error Bounds}
\begin{theorem}\label{theo:ObservationBound}
Fix error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
The demand number of samples for coordinate $\V{i}$ to be observed as least once is
\[
    s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})
\]
\end{theorem}
\begin{proof}
The finding task is a binomial distribution$\sim B(s,p(\epsilon_{\V{i}}))$,
in which $p(\epsilon_{\V{i}})$ is small and $s$ large.
So the Poisson distribution with $\lambda = sp(\epsilon_{\V{i}})$
can used to approximate ${\rm Prob}(x\geq1)$.
That is ${\rm Prob}(x\geq1) = 1-{\rm Prob}(x=0)\approx 1-e^{-sp(\epsilon_{\V{i}})} \geq 1-\sigma$,
which gives $s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})$.
\end{proof}
%The $p(\epsilon_{\V{i}})$ of ${\rm core}^k$ sampling is shown in \Lem{Probability}.
%So with the high order extension $k$, the recall will increase when we fix the number of sample.
\begin{theorem}\label{theo:Bound}
Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
In $core^k$ sampling, if the number of samples
\[
    s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x^k_{\V{i}})
\]
then
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
\]
\end{theorem}

\begin{proof}
This result is closely follows the proof of Lemma 3 from \cite{BaPiKoSe15}.
Since  $ \score{1},\cdots,\score{s} $
are independent random variables taking values in $\{0,1\}$.
So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are a special case).
The Chernoff bounds for the sum of Poisson trials shows for any $0 <\delta <1 $:
\[
    Pr[|\predx - \mu|\geq\delta\mu)] \leq 2\exp{(-\mu\delta^2/3)}
\]
where $\mu=\mathbb{E}[\predx]=(sx^k_{\V{i}})/\norm{\V{W}}{1}$.
And by the choice of $s$, we have
$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
Then
\[
    Pr[|\predx-sx^k_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx^k_{\V{i}}/\norm{\V{W}}{1}] \leq \sigma
\]
multiplying by $\norm{\V{W}}{1}/s$ inside the ${\rm Pr}[\cdot]$ gives
\[
    Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
\]
\end{proof}
The ~\Theo{ObservationBound} and ~\Theo{Bound} show that shows that
to reach a reasonable result of finding or estimate the value $x^k_{\V{i}}$,
the demand samples for ${\rm core}^k$ sampling is $s\approx \norm{\V{w}}{1}/x_{\V{i}}^k$.
\section{Fast Implementation}
In this section, we introduce some implementing details to get improvements.
\subsection{Core Partiton Sampling}
During sampling we samples only one vertex $v^c_\V{r}$ in each iteration.
%like ~\AlgLine{CoreSampling}{CorePartition} and ~\AlgLine{QuerySampling}{Indexes}.
In practice, instead of sampling $v^c_\V{r}$, we sample the frequency $c_\V{r}$,
so that the expectation $c_\V{r}$ is $\mu_{c_\V{r}}=sw_\V{r}/\norm{\V{w}}{1}$.
The distribution of $c_\V{r}$ is:
\begin{equation*}c_\V{r}=
    \left\{
      \begin{array}{ll}
        \lfloor \mu_{c_\V{r}} \rfloor,
        & \hbox{$p=\lceil \mu_{c_\V{r}} \rceil - \mu_{c_\V{r}}$} \\\\
        \lceil \mu_{c_\V{r}} \rceil,
        & \hbox{$p=\lfloor \mu_{c_\V{r}} \rfloor - \mu_{c_\V{r}}$}
      \end{array}
    \right.
\end{equation*}
It requires $O(R^k)$ complexity comparing $O(sk\log{R})$
for sampling $s$ vertices in ${\rm core}^k$ partition.
\subsection{Extension of Core Partition}
Using the extension matrices has the same result of ${\rm core}^k$ sampling,
and when $k$ is small it will be computation saving but storage-costing.
Each extension matrix takes extra $O(L_nR^k)$ storage,
the same storage costing for ${\rm core}^k$ sampling without optimizing.
Utilizing the frequency sampling, we do save the storage in memory.
Firstly, $O(R^k)$ in storage is needed for sampling the $c_\V{r}$.
And for each vertex $v^c_{\V{r}}$,
we sample $c_{\V{r}}$ coordinates,
in which we only need to compute the probability vectors
$|\Sca{e}{n}{\V{r}}|/\norm{\VnC{e}{n}{*\V{r}}}{1}$ and uesd once.
The extra storage for the probability vectors takes totally
$O(R^k + L_1 + L_2 + \cdots + L_N)$
that makes high order extension possible.
\section{Experiments Results}
In this section, we will analysis the recall
and time consuming of our algorithms on real data sets.

\subsection{Data and Preprocessing}
We evaluate our algorithms on some real date sets:
DeliciousBookmarks
\footnote{http://www.delicious.com}(delicious for short),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens+IMDb\footnote{http://www.imdb.com }/
Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ml-2k).
Those data sets are released in the framework of the 2nd International Workshop (HetRec2011).
And one larger data sets of MovieLens\cite{Harper2015}:ml-10m.

We use 5-core of each data set in which all user, item and tag occurred at least 5 times.
We use the algorithm in \cite{Rendle_RTF} to train each data set under CP model.
The factor size $R$ is set to $64$.
After training, we get three factor matrices for user, item and tag.
The statistics of data is shown in ~\Table{Data}.
\begin{table}[t]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    DataSet     & User & Item    & Tag    & top-1   & top-1000\\
    \hline
    ml-2k       & 456  &  1973   &  1222  & 6.6813  & 2.5663 \\
    ml-10m      & 993  &  3298   &  2555  & 67.1037 & 26.747 \\
    lastfm      & 1348 &  6927   &  2132  & 78.1688 & 30.8263\\
    delicious   & 1681 &  29540  &  7251  & 3.9153  & 2.7896 \\
    \hline
  \end{tabular}
  \caption{Data Statistics}
  \label{table:Data}
\end{table}

\subsection{Recall and Time Consuming}
We use the maximum budget which is the full-strategy
to show the performance of our ${\rm core}^k$ sampling algorithms on finding top-$t$ values.
For each number of samples, we do $10$ times experiments independently,
and the average result is used.
The recall of each data set is show in ~\Figs{ml_2k_recall}{delicious_recall}
and the time consuming shown in ~\Fig{times}.
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_ml_2k_recall}\\
  \caption{Recall for different methods in ml-2k data sets.}
  \label{fig:ml_2k_recall}
\end{figure}
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_ml_10m_recall}\\
  \caption{Recall for different methods in ml-10m data sets.}
  \label{fig:ml_10m_recall}
\end{figure}
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_lastfm_recall}\\
  \caption{Recall for different methods in lastfm data sets.}
  \label{fig:lastfm_recall}
\end{figure}
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in,viewport = 0 0 650 200]{fig_delicious_recall}\\
  \caption{Recall for different methods in delicious data sets.}
  \label{fig:delicious_recall}
\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_ml_2k_times}\\
%  \caption{Time for ml-2k data set.}
%  \label{fig:ml_2k_times}
%\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_ml_10m_times}\\
%  \caption{Time for ml-10m data set.}
%  \label{fig:ml_10m_times}
%\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_lastfm_times}\\
%  \caption{Time for lastfm data set.}
%  \label{fig:lastfm_times}
%\end{figure}
%\begin{figure}[H]
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=2.5in]{fig_delicious_times}\\
%  \caption{Time for delicious data sets.}
%  \label{fig:delicious_times}
%\end{figure}
\begin{figure}[H]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_times}\\
  \caption{Time for all data sets.}
  \label{fig:times}
\end{figure}
\subsection{Sampling with Budgets}
The score we introduced in \Eqn{score} in used for the pre-sorting strategy.
We use a budget $t' = s/10$ to do sampiling on data lastfm.
The recall shown in \Fig{budget}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3.3in]{fig_lastfm_recall_budget}\\
  \caption{Recall for different methods in delicious data sets with budget $t'=s/10$.}
  \label{fig:budget}
\end{figure}
Compare with the result in \Fig{lastfm_recall}, we can notice that the scores of coordinate
do save the orders.
\subsection{Sampling for Queries}
We use the coordinates pools to save the computation.
And ${\rm core}^2$ extension is uesd 
to find the top-$100$ largerst values for queries on date set lastfm.
The recall and time of each queries on shown in ~\Fig{Queries}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_queries}\\
  \caption{Recall and time for queries in date set lastfm with $s=10^5$.
           For better visualization,
           we show every $10$ queries in processing.}
  \label{fig:Queries}
\end{figure}
\subsection{Estimation of Values}
By \Theo{Bound}, we gives the error bound of estimate the actual prediction values.
We show the performance on data set lastfm with ${\rm core}^3$ sampling and $s=10^8$.
Since the score $\widehat{x}_{\V{i}}$ is an estimation of $sx^3_{\V{i}}/\norm{\V{w}}{1}$,
we draw the result of pairs $(\norm{\V{w}}{1}\widehat{x}_{\V{i}}/s,x^3_{\V{i}})$.
We show the top-$10^4$ pairs with largest scores in ~\Fig{Est}.
We can notice that there are several actual values that have the same approximation values.
This is because the sampling algorothm will only distinguish the value with certain gap.
With high order of extension, this gap will be small. 
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_lastfm_est}\\
  \caption{Plot of pairs $(\norm{\V{w}}{1}\widehat{x}_{\V{i}}/s,x^3_{\V{i}})$.
          The deshed line is the reference for equality.}
  \label{fig:Est}
\end{figure}
\subsection{Comparison with Diamond Sampling}
To compare our algorithm with diamond sampling, we use item and tag matrices.
For each user, we multiply the user vector into item matrix
to get user-oriented factor matrices.
We evalute the average recall and time-consuming for those users.
And the budget of $t'=s/10$ is used since the diamond sampling only consider the result with budgets.
The result of comparison is shown in \Fig{Comparison_recall} and \Fig{Comparison_time}.

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=3in]{fig_comparision_recall}\\
  \caption{The comparison of recall in different algorithm for data set lasftm.}
  \label{fig:Comparison_recall}
\end{figure}

\begin{figure}[ht]
    \centering
% Requires \usepackage{graphicx}
    \includegraphics[width=3in]{fig_comparison_times}\\
    \caption{The comparison of time in different algorithm for data set lasftm.}
\label{fig:Comparison_time}
\end{figure}

\section{Conclusion}
In this paper propose a method for finding top-$t$ values within CP model.
After which we point out that by the extension of core partiton, 
we can get the $n$-the order estimation and reach the high probability of finding one coordinate.
\bibliography{IIP}
\bibliographystyle{aaai}
\end{document}
