\documentclass[letterpaper]{article}
% Required Packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
%%%%%%%%%%
% PDFINFO for PDFLATEX
% Uncomment and complete the following for metadata
\pdfinfo{
/Title (Core Sampling for Top t Retrial Processing)
/Author (Zhi Lu)
/Keywords ()
}
\title{Core Sampling for Top t Retrial Processing}
\author{}
%
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsthm,amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage[mathscr]{eucal}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfigure}
% for algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
\newcommand{\Scah}[3]{h^{(#1,#2)}_{#3}(\V{x}_{i_#1})}
%% -------
%% Tensor
%% -------
\newcommand{\T}[1]{\boldsymbol{\mathscr{\MakeUppercase{#1}}}}

\newcommand{\KT}[1]{\left\llbracket #1 \right\rrbracket}

%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\bm{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\VnC}[3]{\V{#1}^{(#2)}_{#3}}
% norm one of r-th cloumn
\newcommand{\Nrocl}[2]{\norm{\VnC{a}{#1}{*#2}}{1}}

\newcommand{\Varow}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\Vacol}[1]{\V{a}^{(#1)}_{*r}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\bm{\mathbf{\MakeUppercase{#1}}}}}
%Matrix with superscript
\newcommand{\Mn}[2]{\M{#1}^{(#2)}}

% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}

%% ------------
%% reference
%% ------------

% reference:definition
\newcommand{\Def}[1]   {Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]   {Equation~\ref{eq:#1}}
% reference:figure
\newcommand{\Fig}[1]   {Figure~\ref{fig:#1}}
% reference:table
\newcommand{\Table}[1] {Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]  {Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1] {Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1] {Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1] {Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newcommand{\Coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WreightR}{\Nrocl{1}{r}\ldots\Nrocl{N}{r}}
\newcommand{\predx}{\widehat{x}_{\V{i}}}
\newcommand{\predxn}{\widehat{x}_{\V{i},n}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
\maketitle

\section{Introduction}

In recommendation system,
it is an essential task to find the most relevant items when a specific user is given.
Since the item size is always in vast scale, it demands a fast retrial algorithm.
Generally, all users and items are trained to a joint latent spacea with the same dimensionality.
The most widely used technique is based on the factorization framework,
like the matrix factorization framework\cite{KoYe09}.
Matrix factorization is a way to map both users and items to the latent space
that modeling the user-item interaction with the inner products.
For multi-items recommendation system, the factorization model is then tensoer based.
Take the personalized tag recommendation system for example,
many facotrization models have been used to tackle the interaction
between the tag and post(the user-item pair),
like Higher-Order-Singular-Value-Decomposition(HOSVD) model\cite{SyNa08},
Tucker Decomposition(TD) model\cite{Rendle_RTF} and
Pairwise-Interaction-Tensor-Factorization(PITF) model\cite{Rendle_PITF}.
Meantime, fashion outfit recommendation\cite{HuYiLa15} also utilizes the factorization model to
map user and $N$ categories of items into latent sapce.

In those works, after factor matrices(a set of user or item vectors in latent space) have been trained,
finding the top-$t$ mostly related tuples is still a time-consuming task.
We study those tensor factorization models and
proposed a sampling-based origthm to estimate the top-$t$ most relevant targerts.
The tensor factorization we considered is the CANDECOMP/PARAFAC decomposition\cite{KoBa09},
since the PITF is a special case of the CP decomposition
and TD factorization take so much computations that used less at present.
We generalzie our algorithm to search in all-tuples ,
in which given a particular user or post is a degeneration.
The problem we summarized is called the top-t retrial problem:

\begin{definition}\label{def:DefinitionTopt}
(top-t Retrial.) Suppose $S_1,S_2,..S_N$ are $N$ different categories.
Vectors $\V{t}_{i_1},\V{t}_{i_2},...,\V{t}_{i_N}$ are instances
in each category with same dimensionality.
Under the interaction function $f(\V{t}_{i_1},\V{t}_{i_2},...,\V{t}_{i_N})$,
find $t$ tuples $(i_1,i_2,...,i_N)$ that have the largest value over all.
\end{definition}

The prediction $f$ is made by multiplying over all feature matrices.
\begin{equation}\label{eq:CPDecomposition}
    \T{X} := \Mn{A}{1}\times_{L_2}\Mn{A}{N}\cdots\times_{L_N}\Mn{A}{N}
\end{equation}
where $\T{X}$ the $N$-order prediction tensor with size $L_1\times L_2\times\cdots\times L_N$ and
\[
\M{A}^{(n)} =
\begin{bmatrix}
    \VnC{a}{n}{1},\VnC{a}{n}{2},\cdots,\VnC{a}{n}{r}
\end{bmatrix}  \in R^{L_n\times R}
\]
the factor matrix. The column size $R$ represents the dimensionality of latent space.
When factor matrices are trained, the prediction $x_\V{i}$ is:
\begin{equation}\label{eq:ValueInTensor}
x_\V{i} = \sum_{r=1}^{R}\Sca{a}{1}{r}\cdot\Sca{a}{2}{r}\cdots\Sca{a}{N}{r}
\end{equation}
where $\V{i}$ is a shorthand for multi-index $\Coord$.
Our work is inspired by the Maximum All-pairs Dot-product(MAD) search~\cite{BaPiKoSe15}.

\begin{table}[t]
  \label{table:Notation}
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Notations & Explanation \\
    \hline
    $\T{A}$ & tensor \\
    $\M{A}$ & matrix \\
    $\Mn{A}{n}$ & $n$-th factor matrix of tensor\\
    $\V{A}_{*r},\V{A}_r$ & $k$-th column of matrix \\
    $\V{A}_{i*}$ & $i$-th row of matrix \\
    $\V{A}$ & vector \\
    $a_{ir}$ & element of matrix, a scalar\\
    \hline
  \end{tabular}
  \caption{Notation}
\end{table}

\subsection{Notations}

We use the norm $\norm{*}{1}$ operation refereed in \cite{BaPiKoSe15}.
For a vector $\V{v}\in R^n$ or a matrix $\M{M}\in R^{m\times n}$.
It is defined as:
\[
    \norm{\V{v}}{1} = \sum_{i=1}^{n}|v_i|
    \ \  {\rm and} \ \
    \norm{\M{M}}{1} = \sum_{i=1}^{m}\sum_{j=1}^{n}|m_{ij}|
\]
Some frequently used notations are listed in ~\Table{Notation}.


\section{Related work}
If we restrict the number of factor matrices to $2$,
it becomes the matrix multiplication task.
The state of art\cite{BaPiKoSe15} is in MAD search.
And Maximum Inner Product Search(MIPS)\cite{Cohen97,Ram12}
is a special case of MAD search.
Howere finding the top-$t$ tuples based on CP factorization model is not well studied yet.
We study those similar works with probabilistic method that
to sample the coordinate $\V{i}$  proportionating to value of $x_{\V{i}}$.
CP factorization is the special case of TD factorization with a all-one diagonal core tensor.
Borrowing this concept, we proposed the core sampling to handle the CP model.

\section{Core Sampling}
All factor matrices are adjoin to the latent space.
So we represent the latent space by $R$ vertexes, and call it the core partition.
It is derived by the graph presentation of matrices.

\subsection{Graph Presentation of Factor Matrices}
Firstly, consider a matrix $\M{A}$ in size $M \times N$,
it is presented by a weighted bipartite graph shown in ~\Fig{GraphMatrix}.
The left partition has $M$ vertexes called $v^l_m$ and the right $N$ called $v^r_n$.
The weight of edge $e_{mn} = v^l_mv^r_n$ equals to $a_{mn}$.
Further, the $N$ factor matrices in ~\Eqn{CPDecomposition}
have the same latent space dimensionality,
for which we use only one partition called core partition to represent.
So those $N$ factor matrices can be represented by a weighted (N+1)-partite graph
that shown in ~\Fig{GraphMatrices}.
The vertexes in core partition are called $v^c_r$,
and the other partitions $v^{n}_{i_n}$ respectively.
Similarly, the weight of edge $e_{i_nr} = v^{n}_{i_n}v^c_r$ equals to $a^{(n)}_{i_nr}$.
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.4]{fig_graph_matrix}\\
  \caption{Graph Presentation of Matrix $\M{A} \in R^{M \times N}$}
  \label{fig:GraphMatrix}
\end{figure}
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale = 0.4]{fig_graph_factor_matrices}\\
  \caption{Graph Presentation of Factor Matrices}
  \label{fig:GraphMatrices}
\end{figure}
\subsection{Sampling Mechanism}

We start our sampling algorithm from core partition
by assigning a particular value to each vertexes.
In each one round, we sample the vertexes $v^c_r$ according to the value,
after which $N$ vertexes $v^{n}_{i_n}$ from other partitions are sampled
based on the weight of edge.
At the end of each round,
we assign a particular score to the coordinate $\Coord$ occurred.

\begin{algorithm}[t]
    \caption{Core Sampling with factor matrixes}
    \label{alg:CoreSampling}
    Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples.
    \begin{algorithmic}[1]
    \For{$r\in{1,2,\ldots,R}$}
    \State $w_r \leftarrow \Nrocl{1}{r}\norm{\Vacol{2}}{1}\ldots\Nrocl{N}{r}$
    \label{line:Weight}
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
    \label{line:CorePartition}
    \For {$n = 1,...,N$}
    \label{line:ItemPartitionFor}
    \State Sample $i_n$ with probability $|\Sca{a}{n}{r}|/\norm{\Vacol{n}}{1}$
    \EndFor
    \label{line:ItemPartitionEnd}
    \State
    \label{line:Scoring}
        $\M{X}_{\V{i},\ell} \leftarrow sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})$
    \If {$\V{i}=\Coord$ has not been sampled}
    \State  Creat $\predx \leftarrow \M{X}_{\V{i},\ell} $
    \Else
    \State $\predx \leftarrow \predx + \M{X}_{\V{i},\ell}$
    \EndIf
    \EndFor
    \label{line:ScoringEnd}
    \end{algorithmic}
\end{algorithm}

\subsubsection{Probabilities}
The weights we designed for vertexes in core partition are:
\[
    w_r = \WreightR
\]
shown in ~\AlgLine{CoreSampling}{Weight}.

In each round, we sample the index $r$
with probability $w_r/\norm{\V{w}}{1}$
(~\AlgLine{CoreSampling}{CorePartition}).
Then repeat $N$ times,
walk form the vertex $v^c_r$ to other partitions with probability
$|\Sca{a}{n}{r}|/\norm{\Vacol{n}}{1}$
(~\AlgLines{CoreSampling}{ItemPartitionFor}{ItemPartitionEnd}).

\subsubsection{Scores}
Each time a coordinate $\Coord $ occurred, a score of is assigneg to it.
\[
\M{X}_{\V{i},\ell}  = sgn(\Sca{a}{1}{r}\cdot\Sca{a}{2}{r}\cdots\Sca{a}{N}{r})
\]
where $\ell$ is the iteration.
If this coordinate has not been sampled previously,
create a container $\predx = \M{X}_{\V{i},\ell}$.
Otherwise, increase $\predx$ by $\M{X}_{\V{i},\ell}$
(~\AlgLines{CoreSampling}{Scoring}{ScoringEnd}).
For $\V{i}$ that not be sampled in $\ell$-th turn,
we also assume that $\M{X}_{\V{i},\ell}=0$,
so that the final sacore
\begin{equation}\label{eq:score}
\predx = \sum_{\ell} \M{X}_{\V{i},\ell}
\end{equation}

\subsection{Extract Top-$t$ Largest Values}
After sampling, we get a set of coordinates with scores $\predx$.
Let
\[
    \Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}
\]
be all the coordinates have occurred. The size of $\Psi_p$ is $P$.
To reduce the computation, a pre-sort is carried out.
It sort the coordinates in $\Psi_p$ according to scores $\widehat{x}_{\V{i}_p}$
and extract the top-$t'$ coordinates
\[
    \Psi_{t'} = \{ \V{i} | \predx \geq \widehat{x}_{\V{i'}},
                           \forall i'\in \Omega_{s} \backslash \Omega_{t'}
                \}
\]  where $t'>t$.
Then we compute the actual value $x_{\V{i}}$ of $\V{i}$ in set $\Psi_{t'}$.
And the final top-$t$ largest value's coordinates will be
\[
    \Psi_{t} =
                \{ \V{i} | x_{\V{i}}\geq x_{\V{i'}},
                           \forall i' \in \Omega_{t'} \backslash \Omega_{t}
                \}
\]

The use of budget $t'$ is a tradeoff between accuracy and computation.

\subsection{Core Sampling with Different Scores}
There are two strategies for extracting top-$t$ largest values of what one is the pre-sorting.
The other strategy is computing the actual values of all occurred coordinate,
and extract the top-$t$ largest values.
This is special case to use maximum budget in pre-sorting strategy.

The accuracy of full-computing strategy is determined by the probability of coordinate $\V{i}$.
For one coordinate, it is Bernoulli trials $\sim B(s,p(\epsilon_{\V{i}}))$.
So the higher probability $p(\epsilon_{\V{i}})$ of largest elements is,
the more likely it will be sampled finally,
and thus the higher the accuracy will be.
The pre-sorting strategy do save computation,
however, the accuracy will less than the full-computing in statistical.
That ideal case is that the final score keeps the same order of actual values.

The occurrence probability of coordinate and isotonicity of score are two main factors
that influence our algorithm.
Next, we introduce how to do extra sampling in core partition
to get the better isotonicity of score.

For example, after index $r$ has been sampled in one round,
we can do an extra sample to get anther index $r'$
with the sample probability $w_{r'}/\norm{\V{w}}{1}$.
And we adjust the score to be:
\[
\M{X}_{\V{i},\ell}  = \frac{sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})\Sca{a}{1}{r'}\cdots\Sca{a}{N}{r'}}{\WreightR}
\]

It will make the $\predx$ be an estimation of $sx_{\V{i}}^2/\norm{\V{w}}{1}^2$.
The same we can sample more additional indexes in core partition with adjusted score
to make the final score be an estimation of $sx_{\V{i}}^n/\norm{\V{w}}{1}^n$.
We call it $n$-order estimation.
However, the demand number of samples will increase exponentially.
In practice, we use the first or second order estimation to reach a good performance.

\subsection{Extension of Core Sampling}

We now consider the first factor the occurrence of coordinate
which determine the upper bound of the final accuracy.
The effort for significantly improving the final accuracy significantly
should to focus on $p(\epsilon_{\V{i}})$.
So we introduce the extension version of core sampling to reach a higher accuracy bound.

Instead of sampling one index $r$, we sampled the indexes pair $(r,r')$ at a time.
To implement this mechanism,
we introduce the extension matrix which expend the feature dimension from $R$ to $R^2$.
Suppose $\V{v}$ is an vector with dimension $R$ and the extension vector $\V{v}^e$,
the rule given below:
\[
    \V{v}^e(r+r'R) = \V{v}(r)\V{v}(r')\ {\rm for\ }r,r' = {1,2,...,R}
\]
After such expansion, we get $N$ expanded factor matrices
\begin{equation}\label{eq:ExtensionMatrices}
    \M{E}^{(n)}=[\VnC{e}{n}{1},\ldots,\VnC{e}{n}{R^2}]
\end{equation}
of each $\M{A}^{(n)}$.
Apply core sampling to process these expanded factor matrices
will make the probability of coordinate proportioning to $x_{\V{i}}^2$.
We can make the probability of coordinate proportioning to $n$-the power the its value by extending the feature vector to $R^N$. However, the cost the computing cost and storage for the extension matrix will increase exponentially.

\section{Theoretical Analysis}

Until now, we just give the result without any proofs.
In this section,
we will show the probabilities of coordinates and the expectation of scores in our algorithms,
and give some useful error bounds.
We suppose all factor matrices are nonnegative.
This premise of the following compact conclusions.

\subsection{Probability of Coordinates}

\begin{lemma}\label{lem:Probability}
    In core sampling, $p(\epsilon_{\V{i}})$ equals $x_{\V{i}}/\norm{\V{w}}{1}$,
    even with extra sampling in core partition.
\end{lemma}
\begin{proof}
For the primary core sampling, then probability of event $\epsilon_{\V{i}}$ is:
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{r=1}^{R} p(\epsilon_{\V{i},r}) \\
& = \sum_{r=1}^{R} \frac{\WreightR}{\norm{\V{W}}{1}}
    \frac{|\Sca{a}{1}{r}|}{\Nrocl{1}{r}}\ldots\frac{|\Sca{a}{N}{r}|}{\Nrocl{N}{r}}\\
& = \sum_{r=1}^{R} \frac{\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}}{\norm{\V{W}}{1}}
  = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
Because each extra sampling in core partition is independent, we have
\[
    p(\epsilon_{\V{i},r_1,\ldots,r_n}) =
    p(\epsilon_{\V{i},r_1,\ldots,r_{n-1}})w_{r_n}/\norm{\V{w}}{1}
\]
So doing $n$ extra sampling, then probability of event $\epsilon_{\V{i}}$ is:
\begin{align*}
p(\epsilon_{\V{i}})
&= \sum_{r_1,\ldots,r_n} p(\epsilon_{\V{i},r_1,\ldots,r_n})\\
&= \sum_{r_1,\ldots,r_n} p(\epsilon_{\V{i},r_1,\ldots,r_{n-1}})
                         w_{r_n}/\norm{\V{w}}{1} \\
&= \sum_{r_1,\ldots,r_{n-1}} p(\epsilon_{\V{i},r_1,\ldots,r_{n-1}}) \\
& \cdots\\
&= \sum_{r_1} Pr(\epsilon_{\V{i},r_1})
 = x_{\V{i}}/\norm{\V{w}}{1}
\end{align*}
\end{proof}
\begin{lemma}\label{lem:Probability}
    In extension core sampling, $p(\epsilon_{\V{i}})$ equals $x^2_{\V{i}}/\norm{\V{w^e}}{1}$,
    even with extra sampling in core partition.
\end{lemma}

\begin{proof}
It is easy to notice that
for each column of the extension matrices in~\Eqn{ExtensionMatrices} satisfies:
\[
    \norm{\VnC{e}{n}{*l}}{1} = \sum\nolimits_{i_n}|\Sca{a}{n}{r}\Sca{a}{n}{r'}|,l=r+r'R
\]
According to central sampling,
we first sample the indexes pair $(r,r'),l = r+r'R$
with probability $w^e_l/\norm{\V{w^e}}{1}$.
Then sample the other indexes $v^{n}_{i_n}$
with probability $e^{(n)}_{i_nl}/\norm{\VnC{e}{n}{*l}}{1}$.
By definition, we know
\[
w^e_l = \norm{\VnC{e}{1}{*l}}{1}\cdots \norm{\VnC{e}{N}{l}}{1}, l = 1,\ldots,R^2
\]
and
\[
e^{(n)}_{i_nl} = \Sca{a}{n}{r}\Sca{a}{n}{r'}, l = r+r'R
\]
So, let $l = r+r'R$
\begin{align*}
p(\epsilon_{\V{i}})
& = \sum_{l}    p(\epsilon_{\V{i},l})
  = \sum_{r,r'} p(\epsilon_{\V{i},r,r'})\\
& = \sum_{l} \frac{w^e_l}{\V{w^e}}
             \frac{|e^{(1)}_{i_1l}|}{\norm{\VnC{e}{1}{*l}}{1}}\cdots
             \frac{|e^{(N)}_{i_Nl}|}{\norm{\VnC{e}{N}{*l}}{1}} \\
& = \frac{1}{\norm{\V{W^e}}{1}}
    \sum_{r,r'} |\Sca{a}{1}{r} \cdots\Sca{a}{N}{r}\Sca{a}{1}{r'}\cdots\Sca{a}{N}{r'}| \\
& = \frac{1}{\norm{\V{W^e}}{1}}
    (\sum_{r}|\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}|)^2\\
& = \frac{x_{\V{i}}^2}{\norm{\V{w^e}}{1}}
\end{align*}
\end{proof}

\subsection{Expectation of Scores}
\begin{lemma}\label{lem:Expectation}
The expectation of $n$-order score $\widehat{x}_{\V{i},n}$ equals to $sx_{\V{i}}^n/\norm{\V{w}}{1}^n$.
\end{lemma}
\begin{proof} Use~\Eqn{score} and the independence of scores in different iterations,
     the expectation of score for primary core sampling is:
\begin{align*}
\mathbb{E}[\predx]
& =\sum_{\ell=1}^{s}\sum_{r=1}^{R} p(\epsilon_{\V{i},r})\M{X}_{\V{i},\ell}\\
& = s\sum_{r=1}^{R} \frac{|\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}|}{\norm{\V{W}}{1}}sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})\\
& = s\sum_{r=1}^{R} \frac{\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}}{\norm{\V{W}}{1}}\\
& = \frac{sx_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
The score for $n$-order estimation in $\ell$-th turn satisfies
\[
\M{X}_{\V{i},\ell,n}  = \frac{\Sca{a}{1}{r_n}\cdots\Sca{a}{N}{r_n}}{\Nrocl{1}{r_n}\ldots\Nrocl{N}{r_n}}
\M{X}_{\V{i},\ell,n-1}
\]
Then the expectation score for $n$-order estimation is:
\begin{align*}
\mathbb{E}[\widehat{x}_{\V{i},n}/s]
& = \sum_{r_1,\ldots,r_n} p(\epsilon_{\V{i},r_1,\cdots,r_n})\M{X}_{\V{i},\ell,n}\\
& = \sum_{r_1,\ldots,r_n}
\frac{p(\epsilon_{\V{i},r_1,\cdots,r_{n-1}})w_{r_n}}{\norm{\V{w}}{1}}\M{X}_{\V{i},\ell,n}\\
& = \mathbb{E}[\widehat{x}_{\V{i},n-1}/s]\sum_{r_n}\frac{|\Sca{a}{1}{r_n}\cdots\Sca{a}{N}{r_n}|}{\norm{\V{w}}{1}}\\
& = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}\mathbb{E}[\widehat{x}_{\V{i},n-1}/s]\\
& \cdots \\
& = \frac{x^{n-1}_{\V{i}}\mathbb{E}[\predx/s]}{\norm{\V{w}}{1}^{n-1}}
= \frac{x^n_{\V{i}}}{s\norm{\V{w}}{1}^n}
\end{align*}
Then we get $\mathbb{E}[\widehat{x}_{\V{i},n}] = sx^n_{\V{i}}/\norm{\V{w}}{1}^n$
\end{proof}

\subsection{Error Bounds}
\begin{lemma}\label{lem:Bound}
Fix $\varepsilon > 0$ and error probability $\sigma \in (0,1)$.
Assuming all entries in factor matrices are nonnegative.
In core sampling, if the number of samples
\[
    s \geq 3\norm{\V{w}}{1}^n\log{(2/\sigma)}/(\epsilon^2 x^n_{\V{i}})
\]
then
\[
    Pr(|\predx\norm{\V{W}}{1}^n/s - x^n_{\V{i}}| > \epsilon x^n_{\V{i}}) \leq \sigma
\]
\end{lemma}

\begin{proof}
Since $\M{X}_{\V{i},\ell}$ is in $[0,1]$
and $\predx=\sum_{\ell}\M{X}_{\V{i},\ell}$ is a sum of random variables in $[0,1]$.
Applying the Chernoff bound,
\[
    Pr[\predx \geq (1+\epsilon)\mathbb{E}[\predx]] < \exp{(-\epsilon^2\mathbb{E}[\predx]/3)}
\]
Substitute bound of $s$ to ~\Lem{Expectation}, we have
$\mathbb{E}[\predx]=(sx^n_{\V{i}})/\norm{\V{W}}{1}^n \leq 3\log{(2/\sigma)/\epsilon^2}$.
Then
\[
    Pr[\predx\norm{\V{W}}{1}^n/s \geq (1+\epsilon) x^n_{\V{i}}] < \sigma/2
\]
and
\[
    Pr[\predx\norm{\V{W}}{1}^n/s \leq (1-\epsilon) x^n_{\V{i}}] \leq \sigma/2
\]
which combined, we get
\[
    Pr(|\predx\norm{\V{W}}{1}^n/s - x^n_{\V{i}}| > \epsilon x^n_{\V{i}}) \leq \sigma
\]
\end{proof}

\begin{theorem}\label{theo:Order}
Given the threshold $\tau$ and error probability $\sigma$ in core sampling,
and let
\[
    s \geq 12\norm{\V{w}}{1}^n\log{(2/\sigma)}/\tau^n
\]
Then with probability at least $1-\sigma$,
the following holds for all coordinates $\V{i}$
and $\V{i}'$:
if $x_{\V{i}} > \tau$ and $x_{\V{i}'} < \tau/\sqrt[n]{4e}$,
then $\widehat{x_{\V{i}}} > \widehat{x}_{\V{i}'}$.
\end{theorem}

\begin{proof}

\end{proof}
We call $d=\sqrt[n]{4e}$ the distinguishability.
And to get high distinguishability,
we can use the $n$-order score of core sampling.
And to reach the same error boundneed,
the $n$-order estimation demands more sample number.

\section{Core Sampling for Queries}
In recommendation system,
it is also demanded to find the most $n$ relevant items for a set of users.
For consistency,
say $\M{A}^{(1)}$ stands for the querying users,
and the rest $N-1$ matrices $\M{A}^{(n)},n=2,\ldots,N$ are items matrices.
Each row in matrix $\M{A}^{(1)}$ represents a particular user $\V{u}$.
And the $N-1$ order tenor in ~\Eqn{RankTensorCP}
represents the predicted ranking score of this user for all item combinations.

\begin{equation}\label{eq:RankTensorCP}
\T{X}_{\V{u}} := \KT{ \V{u},\Mn{A}{2}\dots,\Mn{A}{N}}
\end{equation}
It is a exception to replace one factor matrix by a vector in previous algorithm.
When we have a lots of queries to deal,
the previous user will have many useful information for the next user to process.
The most noted one in sampling processing is walking form core partition to other partitions.
And the biggest difference between two users in the weight of vertexes in core partition,
in other word,
the frequency number $(c_1,c_2,\ldots,c_R)$ of each $v^c_r$ is expected to be sampled.
We define $R$ pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_R$ to save occurred coordinates.
The algorithm for query sampling is show in ~\Alg{QuerySampling}.
In ~\AlgLine{QuerySampling}{Indexes},
the user-oriented indexes $r$ is depending on the specific choice of sampling algorithm.
For core sampling, and we notice that for one user,
the probability of nodes $\overline{v}_r$ is the same.

\begin{algorithm}[t]
    \caption{Finding top k-tuples for a query}\label{alg:QuerySampling}
        Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.
        $\M{A}^{(1)}$ is the query matrix.\\
        Let $s$ be the number of samples, $k'$ be the budget.
    \begin{algorithmic}[1]
    \State Initialize $R$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_R$
    \State Initialize $c_r = 0,r= 1,\ldots,R$
    \For {$i_1 =1,2,\ldots,L_1$}
    \State Let query $\V{u}=\VnC{a}{1}{i_1*}$
    \For {$\ell = 1,\ldots,s$}
    \State Sample the user-oriented index $r$.
    \label{line:Indexes}
    \State  Increment $c_r$.
    \EndFor
    \For {$r= 1,\ldots,R$}
    \If {$c_r > |\V{g}_r|$ }
    \State Sample $c_r - |\V{g}_r|$ coordinates into $\V{g}_r$.
    \EndIf
    \State Use $c_r$ coordinates in $\V{g}_r$ and compute scores.
    \EndFor
    \State Pre-sorting for finding top-$k$ tuples for query $\V{u}$.
    \EndFor
    \end{algorithmic}
\end{algorithm}

\section{Fast Implementation}
In section, we introduce some implementing details get improvements.

The core sampling sample one $r$ in one round like
~\AlgLine{CoreSampling}{CorePartition} and ~\AlgLine{QuerySampling}{Indexes}.
In practice, instead of sample $r$ once a time,
we sample the frequency $c_r$,
so that the expectation $c_r$ is $sw_r/\norm{\V{w}}{1}$.
\begin{equation*}c_r=
    \left\{
      \begin{array}{ll}
        \lfloor sw_r/\norm{w}{1} \rfloor,
        & \hbox{$p=\lceil sw_r/\norm{w}{1} \rceil - sw_r/\norm{w}{1}$} \\\\
        \lceil sw_r/\norm{w}{1} \rceil,
        & \hbox{$p=\lfloor sw_r/\norm{w}{1} \rfloor - sw_r/\norm{w}{1}$}
      \end{array}
    \right.
\end{equation*}
It requires $O(R)$ complexity comparing $O(s\log{R})$ in core partition sampling phase.

The other benefit of sampling the frequency is when sample number $s$ is large,
$c_r$ will be corresponding large.
So we use the alias method\cite{Vose91}
for efficiently sampling from a discrete probability distribution.
It requires $O(n)$ for initialization, where $n$ is the distribution size, and
const time for each generation.
In our algorithm $c_r$ is the number of generations,
and the probability distribution ${\Sca{a}{n}{r}}/\Nrocl{n}{r}$.

\section{Experiments Results}
We will analysis the recall and time consuming of our algorithms on real data sets.
\subsection{Data and Preprocessing}
We evaluate our algorithms on some real datesets:
DeliciousBookmarks
\footnote{http://www.delicious.com}(delicious for short),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens+IMDb\footnote{http://www.imdb.com }/
Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ml-2k).
Those data sets are released in the framework of the 2nd International Workshop (HetRec2011).
And two more larger data sets of MovieLens\cite{Harper2015}:ml-10m and ml-20m.

We use 5-core of each data set in which all user, item and tag occurred at least 5 times.
The statistics of preprocessed data is shown in ~\Table{Data}.
\begin{table}[ht]
  \label{table:Data}
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    DataSet & User & Item & Tag \\
    \hline
    ml-2k       & 456  &  1973   &  1222  \\
    ml-10m      & 993  &  3298   &  2555  \\
    lastfm      & 1348 &  6927   &  2132  \\
    ml-20m      & 3432 &  8971   &  7979  \\
    delicious   & 1681 &  29540  &  7251  \\
    \hline
  \end{tabular}
  \caption{Data Statistics}
\end{table}
We use the algorithm in \cite{Rendle_RTF} to train each data set under CP decomposition model.
The factor size $R$ is $64$.
After training, we get three factor matrices for user, item and tag of each data set.

\subsection{Recall and Time Consuming}
We evaluate those different methods' performance on time and accuracy. We both use first order score for central sampling and the extension version.
We use the maximum budget which is the full-strategy to show the performance of our algorithms.
For each number of samples, we do $10$ times experiments independently, and the average recall is used.
The recall of each data set is show in ~\Fig{lastfm_recall}. 
And the recall of using budget $t'=10t$ is drawn in ~\Fig{RecallBudget}. And the time consuming for each algorithms in shown in ~\Fig{Time}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.4]{fig_lastfm_recall}\\
  \caption{Recall for different methods in lastfm data sets. "c" is short for the core sampling method, and "e" for the using extension matrices.}
  \label{fig:lastfm_recall}
\end{figure}

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.4]{fig_ml_10m_recall}\\
  \caption{Recall for different methods in ml-10m data sets. "c" is short for the core sampling method, and "e" for the using extension matrices}
  \label{fig:ml_10m_recall}
\end{figure}

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.4]{fig_ml_2k_recall}\\
  \caption{Recall for different methods in ml-2k data sets. "c" is short for the core sampling method, and "e" for the using extension matrices.}
  \label{fig:ml_2k_recall}
\end{figure}

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Recall for different methods in ml-10m data sets. "c" is short for the core sampling method, and "e" for the using extension matrices, we use the budget $t' =10t$}
  \label{fig:RecallBudget}
\end{figure}

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Time for different sampling approaches in different data sets. All methods use the maximum budget that equals to the number of samples}
  \label{fig:Time}
\end{figure}
\subsection{Isotonicity of Score}
The isotonicity of scores means the ability of score to keep the original order in actual value. And as we mentioned previously, we may sample some extra indexes and adjust the score to get a high distinguishability. Since the central sampling has the same probability of coordinate in different scores, they are comparable. ~\Fig{Isotonicity} shows the isotonicity of different we used under the same samples.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Accuracy for different scores. All use the same samples $10^6$ and the budget varies form $10^3$ to $10^6$.}
  \label{fig:Isotonicity}
\end{figure}

\subsection{Sampling for Queries}
We use the list of sub-path to save the computation of sampling for queries, and we will show the recall when we use the sub-indexes pool and not.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Accuracy for each queries. We use $t=100$ and $s=10^4$.}
  \label{fig:Queries}
\end{figure}

\section{Conclusion}

In this work, we extend the diamond sampling to handle $N$ factor matrices, which is the multi-items recommendation in practical. Then we proposed a new method called central sampling, and point out that, we can adjust this method to get the $n$-the order estimation and reach high distinguishability. However, for the exponential increase of demand sample number, we suggest the first and second order score. We also proposed the extension version of central sampling, it increase the probability of coordinate with the expending on pre-computing and storage. At the end, we modify the score to handle the other ranking functions.

\bibliography{IIP}
\bibliographystyle{aaai}
\end{document}
