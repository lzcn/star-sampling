\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai17}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Insert Your Title Here)
/Author (Put All Your Authors Here, Separated by Commas)}
\setcounter{secnumdepth}{0}
%
\usepackage{amsfonts}
%
\usepackage{mathrsfs}
\usepackage{amsthm,amsmath,amsfonts,amssymb}
\usepackage{bm}
\usepackage[mathscr]{eucal}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfigure}
% for algorithm
\usepackage{algorithm}
\usepackage{algpseudocode}
% hyperref
\usepackage[colorlinks,linkcolor=blue, filecolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage{url}
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
\newcommand{\Scah}[3]{h^{(#1,#2)}_{#3}(\V{x}_{i_#1})}
%% -------
%% Tensor
%% -------
\newcommand{\T}[1]{\boldsymbol{\mathscr{\MakeUppercase{#1}}}}

\newcommand{\KT}[1]{\left\llbracket #1 \right\rrbracket}

%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\bm{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\VnC}[3]{\V{#1}^{(#2)}_{#3}}
% norm one of r-th cloumn
\newcommand{\Nrocl}[2]{\norm{\VnC{a}{#1}{*#2}}{1}}

\newcommand{\Varow}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\Vacol}[1]{\V{a}^{(#1)}_{*r}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\bm{\mathbf{\MakeUppercase{#1}}}}}
%Matrix with superscript
\newcommand{\Mn}[2]{\M{#1}^{(#2)}}

% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}

%% ------------
%% reference
%% ------------

% reference:definition
\newcommand{\Def}[1] {\hyperref[def:#1] {Definition~\ref*{def:#1}}}
% reference:equation
\newcommand{\Eqn}[1] {\hyperref[eq:#1]  {Equation~\ref*{eq:#1}}}
% reference:figure
\newcommand{\Fig}[1] {\hyperref[fig:#1] {Figure~\ref*{fig:#1}}} %
% reference:table
\newcommand{\Table}[1] {\hyperref[table:#1] {Table~\ref*{table:#1}}} %
% reference:lemma
\newcommand{\Lem}[1] {\hyperref[lem:#1] {Lemma~\ref*{lem:#1}}} %
% reference:theorem
\newcommand{\Theo}[1]{\hyperref[lem:#1] {Theorem~\ref*{theo:#1}}} %
% reference:property
\newcommand{\Prop}[1]{\hyperref[prop:#1]{Property~\ref*{prop:#1}}} %
% reference:algorithm
\newcommand{\Alg}[1] {\hyperref[alg:#1] {Algorithm~\ref*{alg:#1}}}
\newcommand{\AlgLine}[2]{\hyperref[alg:#1]{line~\ref*{line:#2} of Algorithm~\ref*{alg:#1}}}
\newcommand{\AlgLines}[3]{\hyperref[alg:#1]{lines~\ref*{line:#2}--\ref*{line:#3} of Algorithm~\ref*{alg:#1}}}

\newcommand{\Coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WreightR}{\Nrocl{1}{r}\ldots\Nrocl{N}{r}}


\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\begin{document}
\title{Central Sampling for Top t Retrial Processing}
\date{}
\author{}
\maketitle

\section{Introduction}

In recommendation system,
it is an essential task to find the most relevant items when a specific user is given.
Since the item size is always in vast scale, it demands a fast retrial algorithm.
Generally, all users as trained to a set of feature vectors,
and items are also in to the joint latent space of same dimensionality.
The most widely used technique in the modern is the factorization framework,
like the matrix factorization framework in recommender system\cite{KoYe09}.
Matrix factorization is a way to map both users and items to the latent space
that modeling the user-item interaction with the inner products.
For multi-items recommendation system, the factorization model is tensoer based.
Take the personalized tag recommendation system for example,
many facotrization models have been used to tackle the interaction
between the tag and post (the user-item pair),
like Higher-Order-Singular-Value-Decomposition(HOSVD) model\cite{SyNa08},
Tucker Decomposition (TD) model\cite{Rendle_RTF} and
Pairwise-Interaction-Tensor-Factorization(PITF) model\cite{Rendle_PITF}.
Meantime, fashion outfit recommendation\cite{HuYiLa15} also utilizes the factorization model to
map user and $N$ categories of items into the latent sapce.

In those works, after factor matrices have been trained,
finding the top-$t$ mostly related tuples is still a time-consuming task.
We study those tensor factorization models and
proposed a sampling-based origthm to estimate the top-$t$ reated targerts.
The tensor factorization is CP decomposition,
since the PITF is a special case of the CP decomposition 
and TD factorization take much higer computations that used less in recommendation tasks.
We generalzie our algorithm to all-tuples application 
in which given a particular user or post is the degeneration.
The problem we summarized is called the top-t retrial problem:

\begin{definition}\label{def:DefinitionTopt}
(top-t Retrial.) Suppose $S_1,S_2,..S_N$ are $N$ different categories. $\V{t}_{i_1},\V{t}_{i_2},...,\V{t}_{i_N}$ are instances in each category that are all represented in the same dimensionality. With the interaction function $f(\V{t}_{i_1},\V{t}_{i_2},...,\V{t}_{i_N})$, find $t$ tuples $(i_1,i_2,...,i_N)$ that have the largest value over all.
\end{definition}


\subsection{CP decomposition}

Consider a $N$-order tensor, $\T{X}$ with size $L_1\times L_2\times\ldots\times L_N$,
the CP decomposition\cite{KoBa09} of this tensor is

\begin{equation}\label{eq:CPDecomposition}
\T{X}= \KT{ \Mn{A}{1},\dots,\Mn{A}{N}} =
\sum_{r=1}^{R}\VnC{A}{1}{r} \circ \cdots \circ \VnC{A}{N}{r}
\end{equation}
where
\[
    \M{A}^{(n)} =
    \begin{bmatrix}
        \VnC{a}{n}{1},\VnC{a}{n}{2},\cdots,\VnC{a}{n}{r}
    \end{bmatrix}  \in R^{L_n\times R}
\]
is called factor matrix, and the column size $R$ represents the rank of this tenors,
which is also the dimensionality of latent space.
The vector $\VnC{a}{n}{r}$ represents the $r$-th column of matrix $\Mn{A}{n}$.

When factor matrices are trained, 
the element in $\T{X}$ indicates the relation of a combination of instances, which is:

\begin{equation}\label{eq:ValueInTensor}
x_\V{i} = \sum_{r=1}^{R}\Sca{a}{1}{r}\cdot\Sca{a}{2}{r}\cdots\Sca{a}{N}{r}
\end{equation}
$\V{i}$ is a shorthand for multi-index $\Coord$.

Our work is inspired by \cite{BaPiKoSe15} for MAD task,
which is an improvement of wedge sampling\cite{Cohen97}.

\begin{table}[t]
  \label{table:Notation}
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Notations & Explanation \\
    \hline
    $\T{A}$ & tensor \\
    $\M{A}$ & matrix \\
    $\Mn{A}{n}$ & $n$-th factor matrix of tensor\\
    $\V{A}_{*r},\V{A}_r$ & $k$-th column of matrix \\
    $\V{A}_{i*}$ & $i$-th row of matrix \\
    $\V{A}$ & vector \\
    $a_{ir}$ & element of matrix, a scalar\\
    \hline
  \end{tabular}
  \caption{Notation}
\end{table}




\subsection{Notations}

We use the some definition of vector and matrix norm refereed in \cite{BaPiKoSe15}.
Suppose $\V{v}\in R^n$ is a vector and $\M{M}\in R^{m\times n}$ is a matrix.
The norm $\norm{*}{1}$ operation is defined as:
\[
    \norm{\V{v}}{1} = \sum_{i=1}^{n}|v_i|
    \ \  {\rm and} \ \
    \norm{\M{M}}{1} = \sum_{i=1}^{m}\sum_{j=1}^{n}|m_{ij}|
\]
And some frequently used notations are listed in ~\Table{Notation}.


\section{Related work}
There is work on finding the max all-pairs of dot-product(MAD)\cite{BaPiKoSe15} and
MIPS(Maximum Inner Product Search)\cite{Cohen97,Ram12}.
Howere finding the top-$t$ tuples based on CP model is not well studied.
We study those similar work based on the probabilistic method called sampling. 
The main idea of those methods is to sample the mutil-indexes or coordinate $\V{i}$ that is proportional to the value of this coordinate $x_{\V{i}}$.
The most representative work are diamond sampling and wedge sampling. In following section we will exten the diamond sampling method to deal with $N$ factor matrices.

\section{Graph Presentation of Factor Matrices}

Before the analysis of our algorithm, we introduce the graph presentation of factor matrices.
Firstly, consider a matrix $\M{A}$ with size $M \times N$, 
it is presentated as a weighted bipartite graph shown in ~\Fig{GraphMatrix}. 
The left partition has $M$ nodes and the right has $N$ nodes. 
The weight of path $(m,n)$ is $a_{mn}$.
Since the $N$ factor matrices $\Mn{A}{1},\dots,\Mn{A}{N}$ 
in ~\Eqn{CPDecomposition} have the same factor dimensionality. 
We use only one partition called core partition to represent the latent space of each matrix.
So those $N$ factor matrices can be represented by a weighted $(N+1)$-partite graph $G_{T}$
which shown in ~\Fig{GraphMatrices}.

\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale = 0.4]{fig_graph_matrix}\\
  \caption{Graph Presentation of a Matrix}
  \label{fig:GraphMatrix}
\end{figure}
\begin{figure}[t]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale = 0.4]{fig_graph_factor_matrices}\\
  \caption{Graph Presentation of Factor Matrices}
  \label{fig:GraphMatrices}
\end{figure}

\section{Core Sampling}
We represent the latent space by $R$ nodes, 
and call it the core partition which comes form the concept of core tensor in TD decomposition.
And we call our sampling method as core sampling, which means it starts from the core partition.

\subsection{Scoring Mechanism}

We start our sampling algorithm in core partition.
The size of nodes in core partition are $R$.
We sample the index $r$ with designed probabilities.
After an index $r$ was sampled, we repeat $N$ times to sample orther indexes $\V{i}:\Coord$.
In each round, we will get $N+1$ indexes, and we call this event $\varepsilon_{\V{i},r}$.
After event $\varepsilon_{\V{i},r}$ happened, we give the coordinate $\Coord$ a score.
This is shown in ~\Alg{CoreSampling}

\begin{algorithm}[t]
    \caption{Core Sampling with factor matrixes}
    \label{alg:CoreSampling}
    Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples.
    \begin{algorithmic}[1]
    \For{$r\in{1,2,\ldots,R}$}
    \State $w_r \leftarrow \Nrocl{1}{r}\norm{\Vacol{2}}{1}\ldots\Nrocl{N}{r}$
    \label{line:Weight}
    \EndFor
    \For{$ \ell = 1,\ldots,s$}
    \State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
    \label{line:CorePartition}
    \For {$n = 1,...,N$}
    \label{line:ItemPartitionFor}
    \State Sample $i_n$ with probability $|\Sca{a}{n}{r}|/\norm{\Vacol{n}}{1}$
    \EndFor
    \label{line:ItemPartitionEnd}
    \State 
    \label{line:Scoring}
        $\M{X}_{\V{i},\ell} \leftarrow sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})$
    \If {$\V{i}=\Coord$ has not been sampled}
    \State  Creat $\widehat{x}_{\V{i}} \leftarrow \M{X}_{\V{i},\ell} $
    \Else
    \State $\widehat{x}_{\V{i}} \leftarrow \widehat{x}_{\V{i}} + \M{X}_{\V{i},\ell}$
    \EndIf
    \EndFor
    \label{line:ScoringEnd}
    \end{algorithmic}
\end{algorithm}

\subsubsection{Probabilities}
The weight we designed for nodes in core partition are:
\[
    w_r = \WreightR
\]
which is shown in ~\AlgLine{CoreSampling}{Weight}.

In each round, we sample the index $r$ with probability $w_r/\norm{\V{w}}{1}$ (~\AlgLine{CoreSampling}{CorePartition}). Then repeat $N$ times, walk form the node $\overline{v}_r$ in partition $\overline{V}$ to other partition $V_i$ with probability $|\Sca{a}{n}{r}|/\norm{\Vacol{n}}{1}$ (~\AlgLines{CoreSampling}{ItemPartitionFor}{ItemPartitionEnd}).

\subsubsection{Scores}
Each time we sampled an coordinate $\V{i} = \Coord $. The score of this coordinate in the $\ell $-th turn is
\[
\M{X}_{\V{i},\ell}  = sgn(\Sca{a}{1}{r}\cdot\Sca{a}{2}{r}\cdots\Sca{a}{N}{r})
\]
If this coordinate has not been sampled previously, create a container $\widehat{x}_{\V{i}} = \M{X}_{\V{i},\ell}$. Otherwise, increase $\widehat{x}_{\V{i}}$ by $\M{X}_{\V{i},\ell}$ (~\AlgLines{CoreSampling}{Scoring}{ScoringEnd}). For $\V{i}$ that not be sampled in $\ell$-th turn, we also assume that $\M{X}_{\V{i},\ell}=0$, so that

\[
\widehat{x}_{\V{i}} = \sum_{\ell} \M{X}_{\V{i},\ell}
\]

We will show that $\widehat{x}_{\V{i}}$ is an estimation of $x_{\V{i}}/\norm{\V{w}}{1}$.
\subsection{Extracte Top-$t$ Largest Values}
After sampling, we will get some coordinates with scores.
And we use the sampling score $\widehat{x}_{\V{i}}$ and coordinate set $\V{i}$ to do postprocessing.
Let $\Omega_s = \{\V{i}_j|j = 1,2,\ldots,s\}$ be the coordinates have been sampled.

To reduce the computation, a pre-sort is carried out. It sort the scores $\widehat{x}_{\V{i}}$ and extract the top-$t'$ elements.
Obviously, $t'$ called budget, is always much larger than $t$.

Then we compute the actual value $x_{\V{i}}$ of the coordinates in the less small set $\Omega_{t'} = \{\V{i}|\widehat{x}_{\V{i}}\geq \widehat{x}_{\V{i'}},\forall i'\in \Omega_{s} \backslash \Omega_{t'}\}$. And the top-$t$ largest value's coordinates will be $\Omega_{t}=\{\V{i}|x_{\V{i}}\geq x_{\V{i'}},\forall i' \in \Omega_{t'} \backslash \Omega_{t} \}$.

The reason to do so is that although the score $\widehat{x}_{\V{i}}$ is a good estimation, the variance is much higher in practical. So we the use of actual value is demand for high accuracy,
and the budget $t'$ is a tradeoff between accuracy and computation. The algorithm for finding the top-$t$ largest value is shown in ~\Alg{Topt}.
\begin{algorithm}[t]
    \caption{Finding top-$t$ largest value}
    \label{alg:Topt}
    Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.\\
    Let $s$ be the number of samples, $t'$ be the budget.
    \begin{algorithmic}[1]
    \State Sample the score $\widehat{x}_\V{i}$ using ~\Alg{DiamondSampling} and record the coordinates set $\Omega_s$ have been sampled.
    \State Sort the score to extract
    \[
        \Omega_{t'} = \{\V{i}|\widehat{x}_{\V{i}}\geq \widehat{x}_{\V{i'}},\forall i'\in \Omega_{s} \backslash \Omega_{t'}\}
    \]
    \State Compute the actual value $x_{\V{i}}$ of each coordinate in $\Omega_{t'}$
    \State Sort the actual value to extract
    \[
        \Omega_{t}=\{\V{i}|x_{\V{i}}\geq x_{\V{i'}},\forall i' \in \Omega_{t'} \backslash \Omega_{t} \}
    \]
    \end{algorithmic}
\end{algorithm}

\subsection{Core Sampling with Different Scores}

Actually, there are two primary factors that affect the accuracy of sampling.
The first one is the probability of coordinate $\V{i}$ called $p(\epsilon_{\V{i}})$.
When we use the maximum budget $t'=s$, 
which means we will compute all the actual value of indexes that we sampled. 
And it is just a Bernoulli trials for petical coordinate. 
So the higher probability $p(\epsilon_{\V{i}})$ of largest elements is,
the more likely it will be sampled finally,
and thus the higher the accuracy will be.
The other factor is the final score $\widehat{x}_{\V{i}}$ that we assigned.
Since we compute the top-$t'$ elements' actual value after pre-sorting,
the order of original score is mostly concerned.
That ideal case is that the final score keeps the order of actual values 
in which case the increasing the budget do not influence the accuracy.
We summarize these two factors as occurrence probability of coordinate and isotonicity of score.
In the next, we introduce how to do extra sampling in core partition that get better isotonicity of score.

For example, after sampled $r$ in one round,
we sample another $r'$ with the sample probability $w_{r'}/\norm{\V{w}}{1}$.
And the adjust the score we use:
\[
\M{X}_{\V{i},\ell}  = \frac{sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})\Sca{a}{1}{r'}\cdots\Sca{a}{N}{r'}}{\WreightR}
\]

It will make the $\widehat{x}_{\V{i}}$ be an estimation of $x_{\V{i}}^2/\norm{\V{w}}{1}^2$.
And we can sample more additional nodes in core partition with adjusted score 
to make the final score be an estimation of $x_{\V{i}}^n/\norm{\V{w}}{1}^n$.
However, the demand number of samples will increase exponentially.
In practice, we use the primary or second order estimation to reach a good performance.

\subsection{Extension of Core Sampling}

We now consider the first factor the occurrence of coordinate 
which determine the upper bound of the final accuracy.
And the effort for significantly improving the final accuracy significantly
need to focus on $Pr(\epsilon_{\V{i}})$.
So we introduce the extension version of core sampling to reach a higher accuracy bound.

Instead of sampling one index $r$, we sampled the indexes pair $(r,r')$ firstly.
To implement this mechanism,
we introduce the extension matrix which expend the feature dimension to $R^2$.
Suppose $\V{v}$ is an item vector with dimension $R$,
the extension vector, assumed as $\V{v}^e$,
is generated following the rule below:
\[
    \V{v}^e(r+r'R) = \V{v}(r)\V{v}(r')\ r,r' = {1,2,...,R}
\]
After this expansion, we get $N$ expanded factor matrices
\[
    \M{E}^{(n)}=[\VnC{e}{n}{1},\ldots,\VnC{e}{n}{R^2}]
\]
of each $\M{A}^{(n)}$. 
And it is easy to notice that
\[
    \VnC{e}{n}{*l} = \sum\nolimits_{i_n}\Sca{a}{n}{r}\Sca{a}{n}{r'},l=r+r'R
\]
Then use the origin core sampling to process these expanded factor matrices will make the probability of coordinate proportioning to $x_{\V{i}}^2$.
\begin{proof}
Suppose all factor matrices are nonnegative. According to central sampling, we first sample the indexes pair $(r,r')$ in which $l = r+r'R$ with probability $w_l/\norm{\V{w}}{1}$. And sample the other nodes in partition $V_n$ with probability $e^{(n)}_{i_nl}/\norm{\VnC{e}{n}{*l}}{1}$. By definition, we know
\[
w_l = \norm{\VnC{e}{1}{*l}}{1}\cdots \norm{\VnC{e}{N}{l}}{1}, l = 1,\ldots,R^2
\]
and
\[
e^{(n)}_{i_nl} = \Sca{a}{n}{r}\Sca{a}{n}{r'}, l = r+r'R
\]
So, let $l = r+r'R$
\begin{align*}
Pr(\epsilon_{\V{i}}) &= \sum_{l} Pr(\epsilon_{\V{i},l})= \sum_{r,r'} Pr(\epsilon_{\V{i},r,r'})\\
&= \sum_{l} \frac{w_l}{\V{w}}
\frac{|e^{(1)}_{i_1l}|}{\norm{\VnC{e}{1}{*l}}{1}}\cdots
\frac{|e^{(N)}_{i_Nl}|}{\norm{\VnC{e}{N}{*l}}{1}}\\
& = \sum_{r,r'} \frac{|\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}\Sca{a}{1}{r'}\cdots\Sca{a}{N}{r'}|}
{\norm{\V{W}}{1}}\\
& = \frac{(\sum_{r}|\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}|)^2}{\norm{\V{W}}{1}}\\
& = \frac{x_{\V{i}}^2}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}


We can make the probability of coordinate proportioning to $n$-the power the its value by extending the feature vector to $R^N$. However, the cost the computing cost and storage for the extension matrix will increase exponentially.

\section{Theoretical Analysis}

Until now, we just give the result without any prove.
In this section, we will show the probabilities of coordinates and the expectation of scores.
Then some usefull error bounds will be given.
We suppose all factor matrices are nonnegative which is premise of compact conclusions.

\subsection{Probability of Coordinates}

The probability of coordinate $\V{i}$ to be sampled in core sampling is
$x_{\V{i}}/\norm{\V{w}}{1}$, even with extra sampling in core partition.

\begin{proof}
When we just sample one index in core partition, the probability of $\V{i}$ is:
\begin{align*}
Pr(\epsilon_{\V{i}}) 
& = \sum_{r=1}^{R} Pr(\epsilon_{\V{i},r}) \\
& = \sum_{r=1}^{R} \frac{\WreightR}{\norm{\V{W}}{1}}
    \frac{|\Sca{a}{1}{r}|}{\Nrocl{1}{r}}\ldots\frac{|\Sca{a}{N}{r}|}{\Nrocl{N}{r}}\\
& = \sum_{r=1}^{R} \frac{\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}}{\norm{\V{W}}{1}} 
  = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
Because each extra sampling in core partition is independent, we have 
\[
    Pr(\epsilon_{\V{i},r_1,\ldots,r_n}) = 
    Pr(\epsilon_{\V{i},r_1,\ldots,r_{n-1}})w_{r_n}/\norm{\V{w}}{1}
\]
So when we do $n$ extra sampling, then the occurrence probability of coordinate $\V{i}$ is:
\begin{align*}
Pr(\epsilon_{\V{i}}) 
&= \sum_{r_1,\ldots,r_n} Pr(\epsilon_{\V{i},r_1,\ldots,r_n})\\
&= \sum_{r_1,\ldots,r_n} Pr(\epsilon_{\V{i},r_1,\ldots,r_{n-1}})
                         \frac{w_{r_n}}{\norm{\V{w}}{1}} \\
&= \sum_{r_1,\ldots,r_{n-1}} Pr(\epsilon_{\V{i},r_1,\ldots,r_{n-1}}) \\
& \cdots\\
&= \sum_{r_1} Pr(\epsilon_{\V{i},r_1})
 = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
\end{proof}

\subsection{Expectation of Score}
When we do $n$ extra sampling, the expectation score of $n$ order estimation is
$sx_{\V{i}}^n/\norm{\V{w}}{1}^n$.
\begin{lemma}\label{lem:Expectation}
The expectation of $\widehat{x}_{\V{i},n}$ equals to $sx_{\V{i}}^n/\norm{\V{w}}{1}^n$.
\end{lemma}
\begin{proof} The expectation of first order estimation is:
\begin{align*}
\mathbb{E}[\widehat{x}_{\V{i}}]& =\sum_{\ell=1}^{s}\sum_{r=1}^{R} Pr(\epsilon_{\V{i},r})\M{X}_{\V{i},\ell}\\
& = \sum_{\ell=1}^{s}\sum_{r=1}^{R} \frac{|\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}|}{\norm{\V{W}}{1}}sgn(\Sca{a}{1}{r}\cdots\Sca{a}{N}{r})\\
& = \sum_{\ell=1}^{s}\sum_{r=1}^{R} \frac{\Sca{a}{1}{r}\cdots\Sca{a}{N}{r}}{\norm{\V{W}}{1}}\\
& = \frac{sx_{\V{i}}}{\norm{\V{w}}{1}}
\end{align*}
The expectation of score for $n$-th power estimation in $\ell$-th turn satisfies
\[
\M{X}_{\V{i},\ell,n}  = \frac{|\Sca{a}{1}{r_n}\cdots\Sca{a}{N}{r_n}|}{\Nrocl{1}{r_n}\ldots\Nrocl{N}{r_n}}
\M{X}_{\V{i},\ell,n-1}
\]
Then the expectation score of $n$-th power estimation is:

\begin{align*}
& \mathbb{E}[\widehat{x}_{\V{i},n}/s] 
= \sum_{r_1,\ldots,r_n}Pr(\epsilon_{\V{i},r_1,\cdots,r_n})\M{X}_{\V{i},\ell,n}\\
& = \sum_{r_1,\ldots,r_n}
\frac{Pr(\epsilon_{\V{i},r_1,\cdots,r_{n-1}})w_{r_n}}{\norm{\V{w}}{1}}\M{X}_{\V{i},\ell,n}\\
& = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}\sum_{r_1,\cdots,r_{n-1}}
Pr(\epsilon_{\V{i},r_1,\ldots,r_{n-1}})\M{X}_{\V{i},\ell,n-1}\\
& = \sum_{r_1,\ldots,r_{n-1}}
\frac{|\Sca{a}{1}{r_n}\cdots\Sca{a}{N}{r_n}|}{\norm{\V{w}}{1}}
Pr(\epsilon_{\V{i},r_1,\cdots,r_{n-1}})\M{X}_{\V{i},\ell,n-1}\\
& = \frac{x_{\V{i}}}{\norm{\V{w}}{1}}\mathbb{E}[\widehat{x}_{\V{i},n-1}/s]
\end{align*}
\end{proof}
Then we get
\begin{equation}
\mathbb{E}[\widehat{x}_{\V{i},n}] = sx^n_{\V{i}}/\norm{\V{w}}{1}^n
\end{equation}

\subsection{Error Bounds}
\begin{lemma}\label{lem:Bound}
Fix $\varepsilon > 0$ and error probability $\sigma \in (0,1)$. Assuming all entries in factor matrices are nonnegative and at most $K$. If the number of samples
\[
s \geq 3\norm{\V{w}}{1}^n\log{(2/\sigma)}/(\epsilon x^n_{\V{i}})
\]
then
\[
Pr(|\frac{\widehat{x}_\V{i}\norm{\V{w}}{1}^n}{s} - x^n_{\V{i}}| < \epsilon x^n_{\V{i}}) \leq \sigma
\]
\end{lemma}

\begin{proof}[Proof:]
Let
\[
    y_{\V{i}} = \sum_{\ell}\M{Y}_{\V{i},\ell} = \sum_{\ell}\M{X}_{\V{i},\ell}/K^{N-1}
\]
Where $\M{Y}_{\V{i},\ell}$ is in $[0,1]$ for $\M{X}_{\V{i},\ell}$ is in $[0,K^{N-1}]$ and $y_{\V{i}}$ is a sum of random variables in $[0,1]$.
Applying the Chernoff bound,
\[
Pr[y_{\V{i}} \geq \mathbb{E}[y_{\V{i}}]] < \exp{(-\epsilon^2\mathbb{E}[y_{\V{i}}]/3)}
\]
By ~\Lem{Expectation}
\[
\mathbb{E}[y_{\V{i}}] = \frac{sx^2_{\V{i}}}{K^{N-1}\norm{\V{W}}{1}}
\]
By the choice of $s$ we have $\mathbb{E}[y_{\V{i}}]=(sx^2_{\V{i}})/(K^{N-1}\norm{\V{W}}{1}) \leq 3\log{(2/\sigma)}$. Then
\[
Pr[y_{\V{i}} \geq \mathbb{E}[y_{\V{i}}]] < \sigma/2
\]
By the substitution of $y_{\V{i}}=\sum_{\ell}\M{X}_{\V{i},\ell}/K^{N-1}=\widehat{x}_{\V{i}}/K^{N-1}$
\[
Pr[\widehat{x}_{\V{i}}\cdot\norm{\V{W}}{1} \geq s\cdot x_{\V{i}}] < \sigma/2
\]

Using the Chernoff lower tail bound and identical reasoning. We get
\[
Pr[\widehat{x}_{\V{i}}\cdot\norm{\V{W}}{1}/s \leq (1-\epsilon)x_{\V{i}}] \leq \sigma/2
\]
\end{proof}

\begin{theorem}\label{theo:Order}
Given the threshold $\tau$ and error probability $\sigma$. 
Assume all entries in factor matrices are nonnegative. Suppose
\[
    s \geq 12\norm{\V{w^c}}{1}^n\log{(2/\sigma)}/\tau^n
\]
Then with probability at least $1-\sigma$,
the following holds for all coordinates $\V{i}:(i_1,i_2,\ldots,i_N)$ 
and $\V{i'}:(i'_1,i'_2,\ldots,i'_N)$:
\[
    if\ x_{\V{i}} > \tau\ and \ x_{\V{i'}} < \tau/\sqrt[n]{4e}, \ then\ \widehat{x_{\V{i}}} > \widehat{x_{\V{i'}}}
\]
\end{theorem}

\begin{proof}

\end{proof}
So the more $r$ we sampled, the better order will keep. 
We call $d=\sqrt[n]{4e}$ the distinguishability.
To get high distinguishability, we need more sample number to reach the same error bound.

\section{Core Sampling for Queries}
In recommendation system, it is demanded to find the most $k$ relevant items for a set of users. For consistency, say $\M{A}^{(1)}$ stands for the querying users, and the rest $N-1$ matrices $\M{A}^{(n)},n=2,\ldots,N$ are items. Each row in matrix $\M{A}^{(1)}$ represents a particular user $\V{u}$. And the $N-1$ order tenor in ~\Eqn{RankTensorCP} represents the predicted ranking score of this user for all combinations.

\begin{equation}\label{eq:RankTensorCP}
\T{X}_{\V{u}}= \KT{ \V{u},\Mn{A}{2}\dots,\Mn{A}{N}} =
\sum_{r=1}^{R} u_r \cdot \VnC{A}{2}{r}\circ \cdots \circ \VnC{A}{N}{r}
\end{equation}

For one use, it is a exception to reduce one factor matrix into a vector in previous algorithm. However, when we have so many queries, the previous user will have many useful information for the next user we process. The most noted one in sampling processing is picking the nodes in partitions $(V_2,\ldots,V_n)$. And the biggest difference between two users in the probability for picking nodes in partition $\overline{V}$, in other word, the frequency number $(c_1,c_2,\ldots,c_R)$ of each $\overline{v}_r$ that is expected to be sampled.

The probabilities of indexes that from other partition $(V_2,\ldots,V_N)$ is only depend on the items matrices, and this is the useful information that can be utilized for the next user. We define the sub-path $(i_2,\ldots,i_N)$ and lists $\V{g}_1,\V{g}_2,\ldots,\V{g}_R$ to save the sub-paths for convenience.

The algorithm for query sampling is show in ~\Alg{QuerySampling}.
In ~\AlgLine{QuerySampling}{Indexes}, the user-oriented indexes $r$ is depending on the
specific sampling algorithm. For central sampling, and we notice that for one user, the probability of nodes $\overline{v}_r$ is the same.



\begin{algorithm}[t]
    \caption{Finding k-NN for a query}\label{alg:QuerySampling}
        Given factor matrix $\M{A}^{(n)}\in R^{L_n\times R}, n = 1,2,\ldots,N$.$\M{A}^{(1)}$ is the query matrix.\\
        Let $s$ be the number of samples, $t'$ be the budget.
    \begin{algorithmic}[1]
    \State Initialize $R$ empty sub-path lists $\V{g}_1,\V{g}_2,\ldots,\V{g}_R$
    \State Initialize $c_r = 0,r= 1,\ldots,R$
    \For {$i_1 =1,2,\ldots,L_1$}
    \State Let query $\V{u}=\VnC{a}{1}{i_1*}$
    \For {$\ell = 1,\ldots,s$}
    \State Sample the user-oriented indexes $r$. \label{line:Indexes}
    \State  Increment $c_r$.
    \EndFor
    \For {$r= 1,\ldots,R$}
    \If {$c_r\leq |\V{g}_r|$ }
    \State Use the $c_r$ sub-paths in $\V{g}_r$ pool.
    \Else
    \State Sample $(c_r - |\V{g}_r|)$ sub-paths and append into $\V{g}_r$.
    \EndIf
    \State Get the score each sub-path of $\V{u}$.
    \State Use the method in ~\Alg{Topt} to find the top-$k$ largest value for query $\V{u}$.
    \EndFor
    \EndFor
    \end{algorithmic}
\end{algorithm}



\section{Implementing details}
In section, we introduce some tricks get the improvement for algorithm.

\subsection{Data Structure}
The first one is the matrix structure, we use the row major order, and the rest factor matrices in column major order, since it is efficient for row index sampling. And the absolute sum of each column  will be computed saved previously. Particularly, we use the transposition of the user matrix when diamond sampling is in processing.

\subsection{Picking Core Nodes}
In central sampling when we sample the $r$ and in ~\AlgLine{QuerySampling}{Indexes}, instead of sample $r$ once a time, we compute the $c_r$ with expectation $sw_r/\norm{\V{w}}{1}$. $\V{w}$ is determined in use of sampling algorithm.
\begin{equation*}c_r=
    \left\{
      \begin{array}{ll}
        \lfloor sw_r/\norm{w}{1} \rfloor,
        & \hbox{$p=\lceil sw_r/\norm{w}{1} \rceil - sw_r/\norm{w}{1}$} \\\\
        \lceil sw_r/\norm{w}{1} \rceil,
        & \hbox{$p=\lfloor sw_r/\norm{w}{1} \rfloor - sw_r/\norm{w}{1}$}
      \end{array}
    \right.
\end{equation*}

We use Vose Alias's method to sample $c_r$ times $i_n$ wih probability ${\Sca{a}{n}{r}}/\Nrocl{n}{r}$.

\section{Experiments Results}
We will analysis the recall and time consuming  between different method. Then we evaluate the ability to keep order of different method.

\subsection{Data and Preprocessing}
We evaluate our algorithms on some real datesets:
DeliciousBookmarks\footnote{http://www.delicious.com
}(delicious for short),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens\footnote{http://www.grouplens.org}+IMDb\footnote{http://www.imdb.com }/Rotten Tomatoes\footnote{http://www.imdb.com}(ml-2k).
Those data sets are released in the framework of the 2nd International Workshop (HetRec2011)\cite{Cantador:RecSys2011}.
And two more larger data sets of MovieLens\cite{Harper2015}:ml-10m and ml-20m.

We remove the minority until all user, item and tag occurred in at least 5 times. The statistics of reduced tuple is shown in ~\Table{Data}.

\begin{table}[ht]
  \label{table:Data}
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    DataSet & User & Item & Tag \\
    \hline
    ml-2k       & 456  &  1973   &  1222  \\
    ml-10m      & 993  &  3298   &  2555  \\
    lastfm      & 1348 &  6927   &  2132  \\
    ml-20m      & 3432 &  8971   &  7979  \\
    delicious   & 1681 &  29540  &  7251  \\
    \hline
  \end{tabular}
  \caption{Data Statistics}
\end{table}
Use the algorithm in \cite{Rendle_RTF} to train each data set under CP decomposition model,
after witch, we have three factor matrices for user, item and tag.

\subsection{Accuracy and Time Consuming}
We evaluate those different methods' performance on time and accuracy. We both use first order score for central sampling and extension version.
When we use the maximum budget witch is using actual values of all sampled coordinates, the recall of each data set is show in ~\Fig{RecallMaxBudget}. And the accuracy of using budget $t'=10t$ is drawn in ~\Fig{RecallBudget}. And the time consuming for each algorithms in shown in ~\Fig{Time}.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Accuracy for different methods in different data sets in witch we use a maximum budget, and the number of samples varies from $10^3$ to $10^7$.}
  \label{fig:RecallMaxBudget}
\end{figure}

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Accuracy for different methods in different data sets in witch we use a less budget $t'=10t$, and the number of samples varies from $10^3$ to $10^7$.}
  \label{fig:RecallBudget}
\end{figure}

\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Time for different sampling approaches in different data sets. All methods use the maximum budget that equals to the number of samples}
  \label{fig:Time}
\end{figure}
\subsection{Isotonicity of Score}
The isotonicity of scores means the ability of score to keep the original order in actual value. And as we mentioned previously, we may sample some extra indexes and adjust the score to get a high distinguishability. Since the central sampling has the same probability of coordinate in different scores, they are comparable. ~\Fig{Isotonicity} shows the isotonicity of different we used under the same samples.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Accuracy for different scores. All use the same samples $10^6$ and the budget varies form $10^3$ to $10^6$.}
  \label{fig:Isotonicity}
\end{figure}

\subsection{Sampling for Queries}
We use the list of sub-path to save the computation of sampling for queries, and we will show the recall when we use the sub-indexes pool and not.
\begin{figure}[ht]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width= 8cm,height=4.5cm,draft]{draft}\\
  \caption{Accuracy for each queries. We use $t=100$ and $s=10^4$.}
  \label{fig:Queries}
\end{figure}

\section{Conclusion}

In this work, we extend the diamond sampling to handle $N$ factor matrices, which is the multi-items recommendation in practical. Then we proposed a new method called central sampling, and point out that, we can adjust this method to get the $n$-the order estimation and reach high distinguishability. However, for the exponential increase of demand sample number, we suggest the first and second order score. We also proposed the extension version of central sampling, it increase the probability of coordinate with the expending on pre-computing and storage. At the end, we modify the score to handle the other ranking functions.

\bibliographystyle{aaai}
\bibliography{IIP}
\end{document}
