%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.



\usepackage{amsthm,amsmath,amssymb}
\usepackage{stmaryrd}
\usepackage{paralist}
\usepackage{indentfirst}
\usepackage{graphicx}
%\usepackage{subfigure}

% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  %% IEEE Computer Society needs nocompress option
  %% requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
%% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath




% TOD: Use algorithmic to rewrite all algorithms
% *** SPECIALIZED LIST PACKAGES ***
%
% \usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx
\usepackage{algorithm}
\usepackage{algpseudocode}



% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


%% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
%% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
%% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% user commands
%% -------
%% Scalar
%% -------
\newcommand{\Sca}[3]{{#1}^{(#2)}_{i_#2#3}}%Scalar with superscript and subscript
% entries in factor matrices
\newcommand{\anr}[2]{\Sca{a}{#1}{#2}}
% entries in extension factor matrices
\newcommand{\enr}[2]{\Sca{e}{#1}{\V{#2}}}
% score for {#1}-turn
\newcommand{\score}[1]{\xi_{\V{i},#1}}
%% -------
%% Tensor
%% -------
\newcommand{\T}[1]{\mathcal{#1}}
\newcommand{\KT}[1]{\llbracket #1 \rrbracket}
%% -------
%% Vector
%% -------
\newcommand{\V}[1]{{\boldsymbol{{\MakeLowercase{#1}}}}}
% Vector with superscript and subscript
\newcommand{\ColVec}[3]{\V{#1}^{(#2)}_{#3}}
\newcommand{\NormColA}[2]{\norm{\ColVec{a}{#1}{*#2}}{1}}
\newcommand{\NormColE}[2]{\norm{\ColVec{e}{#1}{*\V{#2}}}{1}}
\newcommand{\RowVecA}[1]{\V{a}^{(#1)}_{i_#1*}}
\newcommand{\ColVecA}[1]{\V{a}^{(#1)}_{*r}}
% others
\newcommand{\coord}{(i_1,i_2,\ldots,i_N)}
\newcommand{\WeightR}{\NormColA{1}{r}\ldots\NormColA{N}{r}}
\newcommand{\predx}{\hat{x}_{\V{i}}}
\newcommand{\predxn}{\hat{x}_{\V{i},n}}

%% -------
%% Matrix
%% -------
\newcommand{\M}[1]{{\boldsymbol{{\MakeUppercase{#1}}}}}
\newcommand{\FacMat}[2]{\M{#1}^{(#2)}}
% p-norm
\newcommand{\norm}[2]{\|#1\|_{#2}}
%% ------------
%% reference
%% ------------
% reference:definition
\newcommand{\Def}[1]{Definition~\ref{def:#1}}
% reference:equation
\newcommand{\Eqn}[1]{Eq.(\ref{eq:#1})}
% reference:figure
\newcommand{\Fig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\Figs}[2]{Fig.~\ref{fig:#1}$\sim$\ref{fig:#2}}
% reference:table
\newcommand{\Table}[1]{Table~\ref{table:#1}}
% reference:lemma
\newcommand{\Lem}[1]{Lemma~\ref{lem:#1}}
% reference:theorem
\newcommand{\Theo}[1]{Theorem~\ref{theo:#1}}
% reference:property
\newcommand{\Prop}[1]{Property~\ref{prop:#1}}
% reference:algorithm
\newcommand{\Alg}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\AlgLine}[2]{line~\ref{line:#2} of Algorithm~\ref{alg:#1}}
\newcommand{\AlgLines}[3]{lines~\ref{line:#2}--\ref{line:#3} of Algorithm~\ref{alg:#1}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Sampling Technique for Approximate Maximum Search in Tensor}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{ ~Shell,~\IEEEmembership{Member,~IEEE,}
        John~Doe,~\IEEEmembership{Fellow,~OSA,}
        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell was with the Department
of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
GA, 30332.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: see http://www.michaelshell.org/contact.html
\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% for Computer Society papers, we must declare the abstract and index terms
%% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
Factorization models have been extensively used for
recovering the missing entries of a matrix or tensor.
However, direct computing all entries
using the learned factorization models is prohibitive
when the size of matrix/tensor is large.
On the other hand, in many applications,
such as collaborative filtering,
we are only interested in a few entries that are the largest among all.
In this work, we propose a sampling-based approach for finding the top entries of a tensor
which is decomposed by the CANDECOMP/PARAFAC model.
We develop an algorithm to sample the entries with probabilities proportional to their values.
We further extend it to make the sampling proportional to the $k$-th power of the values,
amplifying the focus on the top ones.
We provide theoretical analysis of the sampling algorithm and evaluate its performance on several real-world data sets.
Experimental results indicate that the proposed approach is orders of magnitude faster than exhaustive computing.
When applied to the special case of searching in a matrix,
it also requires fewer samples than the state-of-the-art method.

\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Maximum Search, Tensor Decomposition, Information Retrial
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle


\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.

\IEEEPARstart{M}{atrix} and tensor completion has received considerable attention in recent years.
Many problems in application can be formulated
as recovering the missing entries of a matrix or tensor.
In some scenarios, such as image and video in-painting~\cite{Ankita14},
all lost entries are needed to be filled in.
In some others, however, it is difficult and also unnecessary to recover them all.
For example, in recommender systems, the size of prediction matrix or tensor,
which is determined by the numbers of users and items, is usually rather large and needs to be reconstructed from latent representation of users and items.
It is computational expensive to compute all of the unknown values, even given one peculiar user.
On the other hand, for recommendation purpose,
we are only interested in a few entries that are the largest within a sub-array of the matrix or tensor.
% TODD: Complete the following citation
Moreover, a good recommendation system should avoid overspecialization[need a cite] and one simply way to increase the serendipity is to "inject a note of randomness"[need a cite].
The largest entries of a matrix/tensor are not only the central concerns for personalized recommendation, but also meaningful in many other cases.
In a similarity matrix, the top entries correspond to pairs of items that are most similar,
which are of interest for applications like link prediction in graph~\cite{LibenNowell07},
duplicate detection~\cite{Ke2010} as well as information retrieval~\cite{Salton03IR}.
And in neuroimage meta-analysis,
the largest matrix/tensor entries may suggest the most probable associations between brain functions and behaviors.
For those mentioned fields, the actual value is not critical and the minority owns relative high values.

In this work, we study the problem of efficiently identifying the top entries within a tensor without exhaustive computing. The tensor, as a multi-way generalization of the matrix, has been exploited more and more recently.
% TODO: more survey on recommender system
Take the recommender systems for example.
While traditional focus is on the user-item matrix/tensor
which is required for data representation in many emerging settings
such as context-aware recommendation~\cite{?},
where contextual information like time and location is considered,
and collaborative filtering recommender\cite{HuYiLa15,Rendle_PITF,KoYe09}
where the object to be recommended is a set of items that interact with each other.
Following the most common paradigm, we assume that the tensor can be decomposed into some factors,
which can be estimated from the observed entries by some learning algorithms.
Specifically, we focus on the CANDECOMP/PARAFAC decomposition model.

CP decomposition~\cite{KoBa09} is a widely used technique for exploring and
extracting the underlying structure of multi-way data.
Given a $N$-order tensor $\T{X}\in\mathbb{R}^{L_1\times \cdots\times L_N}$,
CP decomposition approximates it by $N$ factor matrices $\FacMat{A}{1},\FacMat{A}{2},\ldots,\FacMat{A}{N}$,
such that
\begin{align}
\label{eq:CPDecomposition}
\T{X}&\approx\KT{\FacMat{A}{1},\FacMat{A}{2},\cdots,\FacMat{A}{N}} \\ \notag
&=\sum_{r=1}^R\ColVecA{1}\circ\ColVecA{2}\circ\cdots\circ\ColVecA{N}
\end{align}
where each factor matrix
$\FacMat{A}{n}=[\ColVec{a}{n}{*1}\ColVec{a}{n}{*2}\cdots\ColVec{a}{n}{*R}], n=1,\ldots,N$
is of size $L_n\times R$ with $\ColVec{a}{n}{*r}\in\mathbb{R}^{L_n}$
being the $r$-th column.
The symbol ``$\circ$'' represents the vector outer product.
$R$ is the tensor rank, indicating the number of latent factors.
Element-wise, \Eqn{CPDecomposition} is written as
\begin{align}
\label{eq:CPValue}
x_\V{i} \approx \sum_{r=1}^{R}\anr{1}{r}\anr{2}{r}\cdots\anr{N}{r}
\end{align}
where $\V{i}$ is short for the index tuple $(i_1,i_2,\ldots,i_N)$.

The tensors models relationships for components from each dimensions(actual meaning like categories according to its application). In many interesting fields, they apply the decomposition to compute factor matrices with small $R$ instead of learning/computing the whole tensor. And the retrial stage calls for the reconstruction of whole tensor.

\subsection{Problem Reformation and Related Works}

Given $N$ factor matrices $\textbf{A}^{(1)},\ldots,\textbf{A}^{(N)}$ and a parameter $t$,
the explicit CP decomposition of an interaction tensor $\T{X}$, we would like to find $t$ index tuples $\{\V{i}_1,\ldots,\V{i}_t\}$ which correspond to the top-$t$ largest values $x_{\V{i}}$. This problem subsumes many existing problems in the literature.
When $N=2$, it is exactly the MAD (Maximum All-pairs Dot-product Search) problem~\cite{BaPiKoSe15}
that finds the largest entries in the product of two matrices.
And MAD contains the MIPS (Maximum Inner Product Search)~\cite{Cohen97} problem
as the special case with one matrix being a single column.

The most obvious approach is to compute the entries exhaustively via the reconstruction of whole tensor.
However, this becomes prohibitive as the sizes of the factor matrices grow.
There is some literature in approximate matrix multiplication.
But these methods are not suited even for MAD,
since only a few entries among the millions are of interest.
The more efficient solution is to directly search the top ones.
This has been extensively studied for the MIPS problem.
Popular approaches include LSH (Locality Sensing Hashing)~\cite{Andoni08,ALSH14},
space partition techniques like k-d tree
and some sampling-based approaches~\cite{Drineas2006,John15} are also used.
Recently, Ballard et al. proposed a randomized approach called diamond sampling~\cite{BaPiKoSe15} to the MAD problem.
They selected diamonds, i.e. four-cycles, from a weighted tripartite representation of the two factor matrices,
with the probability of selecting a diamond corresponding to index pair $(i_1,i_2)$
being proportional to $(\RowVecA{1}\cdot\RowVecA{2})^2$,
where we use $\RowVecA{n}$ to represent the $i_n$ row of the factor matrix.
For tensor, there hasn't been any study conducted yet.

Inspired by the work of diamond sampling,
we apply index sampling methods to the case of tensor,
whose entries are computed by the CP decomposition model.
We design a strategy to sample the index tuple $\V{i}$ proportional to the magnitude of the corresponding entries.
We further extend the basic to make the sampling proportional to the $k$-th power of the entries, amplifying the focus on largest ones.
For the application of recommender systems,
an algorithm that reuse the samples for different users is presented.
We provide theoretical analysis for our sampling algorithms,
and derive concentration bounds on the behavior.
We evaluate the sampling methods on several real-world data sets.
The results show that they are orders of magnitude faster than exact computing.
When compared to previous approach for matrix sampling, our methods require much fewer samples.
And we hope to see the improvement for serendipity in applications need to find the "unexpected" objects like a movie recommender.


% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%for IEEE Computer Society journal papers produced under \LaTeX\ using
%% IEEEtran.cls version 1.8b and later.
%% You must have at least 2 lines in the paragraph with the drop letter
%% (should never be an issue)
%I wish you the best of success.
%
%\hfill mds
% 
%\hfill August 26, 2015

%\subsection{Subsection Heading Here}
%Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrL_aheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{The Sampling Method}
In this section, we will propose a basic sampling for finding the top-$N$ entries in tensor, after which, a advanced sampling approach will be introduced .

% TODO: More explanation figures high dimension
% TODO: More detialed captions
\begin{figure*}[!ht]
	\centering
	\subfloat[]{
		\includegraphics{img/fig_graph_factor_matrices_a}
		\label{fig:GraphMatrices:a}
	}
	\qquad
	\subfloat[]{
		\includegraphics{img/fig_graph_factor_matrices_b}
		\label{fig:GraphMatrices:b}
	}
	\subfloat[]{
		\includegraphics{img/fig_graph_factor_matrices_c}
		\label{fig:GraphMatrices:c}
	}
	\caption{Graph representation of factor matrices and sampling mechanism.}
	\label{fig:GraphMatrices}
\end{figure*}

%\begin{figure}[!ht]
%	\centering
%	% Requires \usepackage{graphicx}
%	\includegraphics[viewport = 0 20 240 130]{./img/fig_graph_factor_matrices}\\
%	\caption{Graph representation of factor matrices}
%	\label{fig:GraphMatrices}
%\end{figure}

\subsection{Basic Sampling Method}

The idea of randomized approach is to sample the entries in a tensor according to some probability distribution so that the larger one entry is the more probable it is picked. To achieve this, we use a (N+1)-partite graph to represent the factor matrices $\FacMat{A}{1},\ldots,\FacMat{A}{N}$ in \Eqn{CPDecomposition}. Consider one of the matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n \times R}$, it is represented by a bipartite graph shown within the dash-dot box in ~\Fig{GraphMatrices:a}.
We first use $R$ nodes indexed by $r\in[R]$ to denote the $R$ columns of it, $[R]$ denotes $\{1,\ldots,R\}$. These nodes are also shared by other factor matrices. They constitute the core partition. We then use $L_n$ nodes, indexed by $i_n\in [L_n]$, to represent the rows of $\FacMat{A}{n}$. They constitute a peripheral partition. Edge $(i_n,r)$ exists if the entry $\anr{n}{r}$ is nonzero.

To sample an entry, we sequentially pick each of its indices starting from the core partition. We first assign some weights to the nodes in core partition. For $r\in[R]$, let
\begin{equation}
w_r = \WeightR
\end{equation}
where $\|\cdot\|$ denotes 1-norm of the vector.
We sample the nodes in core partition with probability $w_r/\norm{\V{W}}{1}$.
After one core node is chosen, shown in ~\Fig{GraphMatrices:b},
$N$ peripheral nodes, one from each peripheral partition, are picked,
where the index $i_n$ is drawn with probability $|\anr{n}{r}|/\norm{\ColVecA{n}}{1}$.
After an index tuple $\V{i}=\coord$, as depicted shematically in ~\Fig{GraphMatrices:c}, is obtained,
a score is computed that
\begin{align}
\score{\ell}  = sgn(\anr{1}{r}\cdot\anr{2}{r}\cdots\anr{N}{r})
\end{align}
where $\ell$ denotes the $\ell$-th sample.
If the indices $\V{i}$ has not been sampled before,
a container is created with $\predx = \score{\ell}$.
Otherwise, we increase $\predx$ by $\score{\ell}$.
The procedure is shown in \Alg{CoreSampling}.

Let $\Psi_p = \{\V{i}_p|p = 1,2,\ldots,P\}$
denote the set of unique indices that have been sampled.
$P\leq s$ since some $\V{i}$ may be picked more than once.
There are two strategies for identifying the $t$ largest entries from the samples.
In the first one,
we directly compute the exact entry value for each sample in $\Psi_{p}$ using \Eqn{CPValue}
and then find the $t$ largest ones.
Inspired by previous works, in the second strategy,
we utilize the scores $\predx$ obtained during sampling.
Computing the exact entry value using \Eqn{CPValue} for all sampled indices
would be time consuming when $R$ and $N$ are large,
while the load for computing $\predx$ is much lighter.
Moreover, as we show through theoretical analysis later,
the expectation of $\predx$ is proportional to the exact value $x_{\V{i}}$.
Therefore, we can use these estimated scores to filter out the relatively small ones
and only compute the exact values for a subset of the sampled indices.
Specifically, we denote the subset by $\Psi_{t'},t'>t$,
that contains $t'$ indices corresponding to the $t'$ largest $\predx$, i.e.
$
\Psi_{t'} = \{ \V{i} | \predx \geq \hat{x}_{\V{i'}},
\forall i'\in \Psi_{p} \backslash \Psi_{t'}
\}
$.
Then the exact values  are computed only for $\V{i}$ in $\Psi_{t'}$.
And the $t$ indices with the largest $x_{\V{i}}$ are extracted.

The chose of $t'$ is a trade-off between accuracy and computation.
The recall of the first direct computing strategy is determined by the probability
that indices of the largest entries are sampled during each iteration.
The higher the probabilities for the top entries, the more likely they will be sampled
and thus the higher the recall.
The second strategy will save some computation.
However, since the scores are only estimations of the true entry values,
some orders between the entries may not be kept
that some top entries may be missed during filtering,
which will lead to a slightly lower recall than the first strategy.

\begin{algorithm}[!ht]
	\caption{The basic sampling method}
	\label{alg:CoreSampling}
	Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
	Let $s$ be the number of samples.
	\begin{algorithmic}[1]
		\For{$r\in{1,2,\ldots,R}$}
		\State $w_r \leftarrow \NormColA{1}{r}\NormColA{2}{r}\ldots\NormColA{N}{r}$
		\label{line:Weight}
		\EndFor
		\For{$ \ell = 1,\ldots,s$}
		\State Sample $r$ with probability $w_r/\norm{\V{W}}{1}$
		\label{line:CorePartition}
		\For {$n = 1,...,N$}
		\label{line:ItemPartitionFor}
		\State Sample $i_n$ with probability $|\anr{n}{r}|/\norm{\ColVecA{n}}{1}$
		\EndFor
		\label{line:ItemPartitionEnd}
		\State
		\label{line:Scoring}
		$\score{\ell} \leftarrow sgn(\anr{1}{r}\cdots\anr{N}{r})$
		\If {$\V{i}=\coord$ has not been sampled}
		\State  Create $\predx \leftarrow \score{\ell} $
		\Else
		\State $\predx \leftarrow \predx + \score{\ell}$
		\EndIf
		\EndFor
		\label{line:ScoringEnd}
	\end{algorithmic}
\end{algorithm}

\subsection{Theoretical Analysis}
In each iteration,
we sample one node, indexed by $r$,
from the core partition and $N$ nodes,
indexed by $\boldsymbol{i}$, from the peripheral partitions.
Let $\epsilon_{\boldsymbol{i},r}$
denote the event of choosing $r$
and $\boldsymbol{i}$, $\epsilon_{\boldsymbol{i}}$ denote that of choosing $\boldsymbol{i}$.
We first analyze the probability $p(\epsilon_{\V{i}})$ that an entry is sampled by the sampling algorithms and then the expectation of the scores we computed during sampling. 
We also prove error bound on our estimate.

\begin{lemma}\label{lem:Probability}
	Suppose that all elements of the factor matrix $\M{A}^{(n)}, n\in[N]$ are nonnegative. We have
	$p(\epsilon_{\V{i}})$ equals to $x_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
	The probability $p(\epsilon_{\V{i}})$ is the marginal distribution of $p(\epsilon_{\V{i},r})$,
	so we have:
	\begin{align*}
	p(\epsilon_{\V{i}})
	& = \sum_{r} p(\epsilon_{\V{i},r}) \\
	& = \sum_{r} \frac{w_{r}}{\norm{\V{W}}{1}}
	\frac{|\anr{1}{r}|}{\NormColA{1}{r}} \ldots \frac{|\anr{N}{r}|}{\NormColA{N}{r}}\\
	& = \sum_{r} \frac{\anr{1}{r}\cdots\anr{N}{r}}{\norm{\V{W}}{1}}
	= \frac{x_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{proof}
Let $c_{\V{i},\ell}$ be a random variable that if $\V{i}$ is the index tuple
being picked in the $\ell$-th iteration, $c_{\V{i},\ell}=\score{\ell}$.
Otherwise $c_{\V{i},\ell}=0$.
The final score $\predx$ can be written as
\begin{equation}\label{eq:score}
\predx = \sum_{\ell=1}^{s} c_{\V{i},\ell}
\end{equation}

\begin{lemma}\label{lem:Expectation}
	The expectation of $\predx$ equals to $sx_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
	$c_{\V{i},\ell}$ are i.i.d for fixed $\V{i}$ and varing $\ell$.
	Thus the expectation of $\predx$ is:
	\begin{align*}
	\mathbb{E}[\predx]
	& = \sum_{\ell=1}^{s}\mathbb{E}[c_{\V{i},\ell}] = \sum_{\ell=1}^{s}\sum_{r} p(\epsilon_{\V{i},r})\score{\ell} \\
	& = s\sum_{r} \frac{|\anr{1}{r}\cdots\anr{N}{r}|}{\norm{\V{W}}{1}}
	sgn(\anr{1}{r}\cdots\anr{N}{r})\\
	& = s\sum_{r} \frac{\anr{1}{r}\cdots\anr{N}{r}}{\norm{\V{W}}{1}}
	= \frac{sx_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{proof}

\subsubsection{Error Bounds}
% TODO: delete theorem 1
We derive two error bounds, which suggest the number of samples required to get reasonable results.
\begin{theorem}\label{theo:ObservationBound}
	Fix error probability $\sigma \in (0,1)$.
	Assuming all entries in the factor matrices are nonnegative.
	The number of samples required for the index tuple $\V{i}$ to be picked as least once is
	\[
	s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})
	\]
\end{theorem}
\begin{proof}
	This is a binomial distribution${\sim}B(s,p(\epsilon_{\V{i}}))$,
	in which $p(\epsilon_{\V{i}})$ is relative small and $s$ large.
	So the Poisson distribution with $\lambda = sp(\epsilon_{\V{i}})$
	can be used to approximate $Pr(x\geq1)$.
	That is $Pr(x\geq1) = 1-Pr(x=0)\approx 1-e^{-sp(\epsilon_{\V{i}})} \geq 1-\sigma$,
	which gives $s \geq \ln(1/\sigma)/p(\epsilon_{\V{i}})$.
\end{proof}

\begin{theorem}\label{theo:Bound}
	Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
	Assuming all entries in the factor matrices are nonnegative.
	If the number of samples
	\[
	s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x_{\V{i}})
	\]
	then
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
	\]
\end{theorem}

\begin{proof}
	We follow the proof of Lemma 3 in~\cite{BaPiKoSe15}.
	Since  $c_{\V{i},1},\cdots,c_{\V{i},s}$
	are independent random variables taking values in $\{0,1\}$.
	So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are the special case).
	The Chernoff bounds on the sum of Poisson trials shows, for any $0 <\delta <1 $:
	\[
	Pr(|\predx - \mu|\geq\delta\mu) \leq 2\exp{(-\mu\delta^2/3)}
	\]
	where $\mu=\mathbb{E}[\predx]=sx_{\V{i}}/\norm{\V{W}}{1}$.
	And by the choice of $s$, we have
	$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
	Then
	\[
	Pr(|\predx-sx_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx_{\V{i}}/\norm{\V{W}}{1}) \leq \sigma
	\]
	multiplying $\norm{\V{W}}{1}/s$ inside ${\rm Pr}[\cdot]$ gives
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x_{\V{i}}| > \delta x_{\V{i}}) \leq \sigma
	\]
\end{proof}


\section{Extension of the Basic Sampling Method} 

% TODO: Figure for the Extension of Core Partitions

The recall we can obtain heavily depends on the probability distribution for sampling the entries. To achieve better performance, we should improve this distribution by amplifying the odds for the top entries.
\begin{figure}[tph!]
	\centering
	\includegraphics[]{img/ExtensionOfCorePartition}
	\caption{Extension of core partition}
	\label{fig:ExtensionOfCorePartition}
\end{figure}

Instead of sampling one node $r$ from the core partition,
we can still pick one but compound node $(r,r')$ from some extended core partition,
in which the number of nodes is expanded to $R^2$.
Then conditioned on $(r,r')$, we sample $N$ nodes  
indexed by $(i_1,\ldots,i_N)$ from the peripheral partitions as depicted in ~\Fig{ExtensionOfCorePartition}. However the weight or probability $p(i_n,(r,r'))$ of the edge $((r,r'),i_n)$ connecting the node $(r,r')$ and $i_n$ need to be computed so that the expectation will meet actual value.

More generally, we can pick $k$ compound nodes $(r_1,r_2,\ldots,r_k)$ from the extended core partition
and then draw $N$ peripheral nodes.
We name this strategy Core$^k$ sampling.
The method discussed in previous section is a special case with $k=1$.

By sampling $k$ nodes from the core partition,
we virtually create $R^k$ compound nodes,
indexed by $\V{r}=(r_1,\ldots,r_k)$.
These compound nodes constitute a new partition called Core$^k$ partition.
To assign probabilities for picking the compound nodes and probabilities for picking a peripheral node given a compound nodes, we propose formula for the weight of edge that connecting $\V{r}$ and $i_n$ as follows:
% will be we extend each factor matrix $\FacMat{A}{n}$ with size $L_n\times R$ to a new matrix $\M{E}^{(n)}$ with size $L_n\times R^k$.
% The entries of $\M{E}^{(n)}$ are obtained by
\begin{align}
\enr{n}{r} = \anr{n}{r_1}\cdots\anr{n}{r_k}
\end{align}
and immediately, the weight for a compound nodes $\V{r}$ is:
%where we use $\V{r}$ to index the columns of $\M{E}^{(n)}$. And the weight we assign to the compound node $\V{r}$ is
\begin{align}
w_{\V{r}} = \NormColE{1}{r}\cdots\NormColE{N}{r}
\end{align}
where $\V{e}_{*\V{r}}^{(n)}$ is the weight vector drawn for the $n$-th peripheral partition and its  1-norm is:
%with $\V{e}_{*\V{r}}^{(n)}$ being the column of $\M{E}^{(n)}$ indexed by $\V{r}$ and
\begin{align}
\NormColE{n}{r} = \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|
\end{align}
If we straighten the nodes in Core$^k$ partition to one dimension with $b(\V{r})$ indicating the position, then Core$^k$ sampling strategy will be the same as basic algorithm acting on a set of matrices, presumably, called $\M{E}^{(n)}$.
The $b(\V{r})$-th column of $\M{E}^{(n)}$ is $\V{e}_{*b(\V{r})}^{(n)} = [e_{1,\V{r}}^{(n)},\cdots,e_{L_n,\V{r}}^{(n)}]^T$.
The alternative explanation help us to implement the sampling approach.

\subsection{Theoretical Analysis}

The sampling procedure is similar with \Alg{CoreSampling}
and we summarize it in \Alg{CoreExtensionSampling}.
In this part, we prove that in Core$^k$ sampling,
the probability that entry $\V{i}$ is sampled is proportional to $x_{\V{i}}^k$
and the expectation of $\predx$ is also proportional to the $k$-th power of $x_{\V{i}}$.
Therefore, in Core$^k$ extension, more focus will be put on the top entries.

In each iteration, 
we sample one compound node, indexed by $\boldsymbol{r}$, 
from the Core$^k$ partition and $N$ nodes, 
indexed by $\boldsymbol{i}$, from the peripheral partitions. 
Let $\epsilon_{\boldsymbol{i},\boldsymbol{r}}$ 
denote the event of choosing $\boldsymbol{r}$ 
and $\boldsymbol{i}$, $\epsilon_{\boldsymbol{i}}$ denote that of choosing $\boldsymbol{i}$.
We then analyze the probability $p(\epsilon_{\V{i}})$ and the expectation of the scores in Core$^k$ sampling, we also prove error bound on Core$^k$ sampling.

\subsubsection{Probability of Coordinates}
\begin{lemma}\label{lem:Probability}
	Suppose that all elements of the factor matrix $\M{A}^{(n)}, n\in[N]$ are nonnegative. 
	In Core$^k$ sampling, We have $p(\epsilon_{\V{i}})$ equals to $x^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
	The probability $p(\epsilon_{\V{i}})$ is the marginal distribution of $p(\epsilon_{\V{i},\V{r}})$,
	so we have:
	\begin{align*}
	p(\epsilon_{\V{i}})
	& = \sum_{\V{r}} p(\epsilon_{\V{i},\V{r}}) \\
	%& = \sum_{\V{r}} p({\rm pick\ }\V{r})p({\rm pick\ }i_1|\V{r})\cdots p({\rm pick\ }i_N|\V{r})\\
	& = \sum_{\V{r}} \frac{w_{\V{r}}}{\norm{\V{W}}{1}}
	\frac{|\Sca{e}{1}{\V{r}}|}{\norm{\ColVec{e}{n}{*\V{r}}}{1}}\ldots\frac{|\Sca{e}{N}{\V{r}}|}{\norm{\ColVec{e}{N}{*\V{r}}}{1}}\\
	& = \sum_{\V{r}} \frac{\Sca{e}{1}{\V{r}}\cdots\Sca{e}{N}{\V{r}}}{\norm{\V{W}}{1}}\\
	& = \frac{(\sum_{r}\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
	= \frac{x^k_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{proof}

Let $c_{\V{i},\ell}$ be a random variable that if $\V{i}$ is the index tuple 
being picked in the l-th iteration, $c_{\V{i},\ell}=\score{\ell}$. 
Otherwise $c_{\V{i},\ell}=0$. 
The final score $\predx$ can be written as
\begin{equation}\label{eq:score}
\predx = \sum_{\ell=1}^{s} c_{\V{i},\ell}
\end{equation}
\begin{lemma}\label{lem:Expectation}
	The expectation of approximation $\hat{x}_{\V{i}}$ in Core$^k$ sampling equals to $sx^k_{\V{i}}/\norm{\V{w}}{1}$.
\end{lemma}
\begin{proof}
	$c_{\V{i},\ell}$ are i.i.d for fixed $\V{i}$ and varing $\ell$. 
	The expectation of $\predx$ is:
	\begin{align*}
	\mathbb{E}[\predx]
	& = \sum_{\ell=1}^{s}\mathbb{E}[c_{\V{i},\ell}] = \sum_{\ell=1}^{s}\sum_{\V{r}} p(\epsilon_{\V{i},\V{r}})\score{\ell} \\
	& = s\sum_{\V{r}} \frac{|\enr{1}{r}\cdots\enr{N}{r}|}{\norm{\V{W}}{1}}
	sgn(\enr{1}{r}\cdots\enr{N}{r})\\
	& = s\sum_{\V{r}} \frac{\enr{1}{r}\cdots\enr{N}{r}}{\norm{\V{W}}{1}}\\
	& = s\frac{(\sum_{r}\anr{1}{r}\cdots\anr{N}{r})^k}{\norm{\V{W}}{1}}
	= \frac{sx^k_{\V{i}}}{\norm{\V{w}}{1}}
	\end{align*}
\end{proof}

\subsubsection{Error Bounds}
\begin{theorem}\label{theo:Bound}
	Fix $\delta > 0$ and error probability $\sigma \in (0,1)$.
	Assuming all entries in factor matrices are nonnegative.
	In Core$^k$ sampling, if the number of samples
	\[
	s \geq 3\norm{\V{w}}{1}\log{(2/\sigma)}/(\delta^2 x^k_{\V{i}})
	\]
	then
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
	\]
\end{theorem}

\begin{proof}
	Since  $c_{\V{i},1},\cdots,c_{\V{i},s}$
	are independent random variables taking values in $\{0,1\}$.
	So $\predx$ is the sum of independent Poisson trials(Bernoulli trials are the special case).
	The Chernoff bounds on the sum of Poisson trials shows, for any $0 <\delta <1 $:
	\[
	Pr(|\predx - \mu|\geq\delta\mu) \leq 2\exp{(-\mu\delta^2/3)}
	\]
	where $\mu=\mathbb{E}[\predx]=sx^k_{\V{i}}/\norm{\V{W}}{1}$ in Core$^k$ sampling.
	And by the choice of $s$, we have
	$\mu\leq 3\log{(2/\sigma)/\delta^2}$.
	Then
	\[
	Pr(|\predx-sx^k_{\V{i}}/\norm{\V{W}}{1}|\geq\delta sx^k_{\V{i}}/\norm{\V{W}}{1}) \leq \sigma
	\]
	multiplying $\norm{\V{W}}{1}/s$ inside ${\rm Pr}[\cdot]$ gives
	\[
	Pr(|\predx\norm{\V{W}}{1}/s - x^k_{\V{i}}| > \delta x^k_{\V{i}}) \leq \sigma
	\]
\end{proof}

Note that in the diamond sampling method for matrix~\cite{BaPiKoSe15},
they also sampled two nodes from the core partition.
However, the second node $r'$ was sampled after the indices $i_1$ and $i_2$ were picked
and none extension of the factor matrices were introduced.
As a result, only the estimation $\predx$ was improved
but not the probability of picking the top entries.

\begin{algorithm}[!ht]
	\caption{Core$^k$ sampling}
	\label{alg:CoreExtensionSampling}
	Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.\\
	Let $s$ be the number of samples, $k$ for the extension order.$[R]^k$ denotes all the
	\begin{algorithmic}[1]
		\For{$\V{r}\in{\underbrace{[R]\times \cdots \times [R]}_{k}}$}
		\For{$n = 1,...,N$}
		\State $\NormColE{n}{r} \leftarrow \sum_{i_n}|\anr{n}{r_1}\cdots\anr{n}{r_k}|$
		\EndFor
		\State $w_{\V{r}} \leftarrow \NormColE{1}{r} \cdots \NormColE{N}{r} $
		\EndFor
		\For{$ \ell = 1,\ldots,s$}
		\State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$
		\label{line:nodes}
		\For {$n = 1,...,N$}
		\State Sample $i_n$ with probability $|\enr{n}{r}|/\NormColE{n}{r}$
		\EndFor
		\State
		$\score{\ell} \leftarrow sgn(\enr{1}{r}\cdots\enr{N}{r})$
		\If {$\V{i}=\coord$ has not been sampled}
		\State  Create $\predx \leftarrow \score{\ell} $
		\Else
		\State $\predx \leftarrow \predx + \score{\ell}$
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

\subsection{Sampling for Multiple Users}
In this subjection,
we present an algorithm for efficiently identifying top entries in sub-arrays of a tensor,
which is of particular interest for recommender systems.
Without loss of generality,
we assume that the first factor matrix $\FacMat{A}{1}$ corresponds to users,
with each row characterizing a single user.
For user $\V{u} = \RowVecA{1}$,
we are interested in top entries in the (N-1)-order tensor $\T{X}_\V{u}$,
whose CP decomposition is
\begin{align}
\T{X}_{\V{u}} \approx \KT{\V{u},\FacMat{A}{2},\cdots,\FacMat{A}{N}}
\end{align}

To generate recommendations for multiple users,
an naive approach is to run the above sampling algorithm independently for different users.
However, this is totally unnecessary.
We find that for different users,
only the probabilities for sampling nodes from the core partition are different.
In the following step, they share the same distribution for sampling the peripheral nodes.
Based on this observation, for each node in the core partition,
we build a pool containing indices $\V{i}=(i_2,\ldots,i_N)$ that have been picked given that core node.
For a new user, when a core node is chosen,
%we can directly use the indices kept in its pool and only sample new $\V{i}$ when necessary.
By sharing among users, a huge number of samples can be saved.
This procedure is illustrated in~\Alg{QuerySampling}.

\begin{algorithm}[!ht]
	\caption{Finding top-$t$ entries for multiple users}
	\label{alg:QuerySampling}
	Given factor matrix $\FacMat{A}{n}\in \mathbb{R}^{L_n\times R}, n = 1,2,\ldots,N$.
	$\FacMat{A}{1}$ is the set of querying users.
	Let $s$ be the number of samples, and $m=R^k$.
	\begin{algorithmic}[1]
		\State Initialize $m$ empty pools $\V{g}_1,\V{g}_2,\ldots,\V{g}_m$
		\State Initialize $f_{\V{r}} = 0$ for all compound nodes $\V{r}$.
		\For {$i_1 =1,2,\ldots,L_1$}
		\State Let the current user be $\V{u}=\RowVecA{1}$
		\ForAll{$\V{r}=(r_1,\ldots,r_k)$}
		\State $w_\V{r} \leftarrow |u_{r_1}\cdots u_{r_k}|\NormColE{2}{r} \cdots \NormColE{N}{r}$
		\EndFor
		\For {$\ell = 1,\ldots,s$}
		\State Sample $\V{r}$ with probability $w_\V{r}/\norm{\V{W}}{1}$.
		\label{line:Indexes}
		\State  Increment $f_{\V{r}}$.
		\EndFor
		\ForAll {$\V{r}$}
		\If {$f_\V{r} > |\V{g}_\V{r}|$ }
		\State Sample $f_{\V{r}} - |\V{g}_{\V{r}}|$ index tuples $\V{i}$ into $\V{g}_{\V{r}}$.
		\EndIf
		\State Use $f_{\V{r}}$ index tuples $\V{i}$ in $\V{g}_{\V{r}}$ and compute scores.
		\EndFor
		\State Post-processing for finding top-$t$ entries of $\V{u}$.
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Implementation Details}
In this section, we discuss some implementation details that could improve the performance.

Instead of first picking a node from the Core/Core$^k$ partition
and then choosing $N$ peripheral nodes,
in implementation, we first directly get $s$ core nodes in one loop.
Specifically, we compute the expected number of occurrence of the nodes.
Let $\mu_{\V{r}}=sw_\V{r}/\norm{\V{w}}{1}$, the count for $\V{r}$ is:
\begin{equation}f_\V{r}=
\left\{
\begin{array}{ll}
	\lfloor \mu_{\V{r}} \rfloor, & \hbox {with probability $p=\lceil \mu_{\V{r}} \rceil - \mu_{\V{r}}$}  \\
	\lceil \mu_{\V{r}} \rceil,   & \hbox{with probability $p=\lfloor \mu_{\V{r}} \rfloor - \mu_{\V{r}}$}
\end{array}
\right.
\end{equation}
This requires $O(R^k)$ work while sampling $\V{r}$ $s$ times requires $O(sk\log R)$ work.

To conduct Core$^k$ sampling,
we extend the factor matrices to $\FacMat{E}{n}$,
which requires $O(L_nR^k)$ space for each $n\in[N]$.
This will be costly in practice when $k$ is large.
Using the above strategy, we have obtained the number of times each core node appears.
Given a core node, the sampling of the peripheral nodes
will only depend on one column in $\FacMat{E}{n}$ that is indexed by $\boldsymbol{r}$.
Therefore, instead of storing the whole $\FacMat{E}{n}$,
we only keep one column of it at a time.
This requires $O(L_1+\cdots+L_N)$ space in total.
When k is small, we can directly store $\FacMat{E}{n}$ in exchange for speed.
\section{Experiments}
%
%
%
%
%
We evaluate our algorithms on four real-world data sets:
Delicious Bookmarks\footnote{http://www.delicious.com}(delicious),
Last.FM\footnote{http://www.lastfm.com}(lastfm),
MovieLens\footnote{http://www.grouplens.org}+IMDb\footnote{http://www.imdb.com}/Rotten Tomatoes\footnote{http://www.rottentomatoes.com}(ml-2k),
and a larger MovieLens data(ml-10m).
The first three are from~\cite{Cantador:RecSys2011} and the forth one is from ~\cite{Harper2015}.
Following previous practice, users, items and tags that occur at least 5 times are used.
Some statistics of the data sets after preprocessing are shown in \Table{Data}.


\subsection{Data and Preprocessing}

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		 DataSet  & User & Item  & Tag  &  top - 1   & top - 1000 \\ \hline
		  ml-2k   & 456  & 1973  & 1222 & 81.5643  &  43.3850   \\
		 ml-10m   & 993  & 3298  & 2555 & 88.7368  &  50.5948   \\
		 lastfm   & 1348 & 6927  & 2132 & 234.7302 &  101.7950  \\
		delicious & 1681 & 29540 & 7251 & 95.4723  &  40.8289   \\ \hline
	\end{tabular}
	\caption{Data Statistics}
	\label{table:Data}
\end{table}
To learn the factor matrices of CP decomposition,
we optimize the pair-wise
ranking function Bayesian Personalized Ranking (BPR)~\cite{Rendle_BPR,Rendle_RTF}.
The factor matrices are learned so that the observed entries have ranking scores that are higher than the unobserved ones. 
$R$ is set to 64. 
After getting the factor matrices, 
we run different algorithms to find the top entries of the tensor.

\subsection{Results}
\subsubsection{Accuracy and Time}

\begin{figure}[!]
	\centering
	\includegraphics[width=3in]{./img/fig_recall}\\
	\caption{Recall of Core$^k$ sampling for $k\in\{1,2,3\}$.}
	\label{fig:recall}
\end{figure}
\begin{figure}[!]
	\centering
	\includegraphics[width=3in,viewport = 10 10 600 400]{./img/fig_times}\\
	\caption{Running time for the experiments in~\Fig{recall}.}
	\label{fig:time}
\end{figure}
We present time and accuracy results for the four data sets in
~\Fig{recall} and~\Fig{time}.
In~\Fig{recall}, we plot recall, i.e. the percentage of the top-$t$ entries identified,
versus the number of samples.
The results for $t\in\{1,10,100,1000\}$ are shown.
~\Fig{time} plots the computation time versus the number of samples.
The time for exhaustive computing is also shown.
Here we set $t'=s$ and focus on the performance of the sampling step.
For each $s$, we run the sampling algorithms 10 times and the average of the recalls is reported.

The results show that by extending the core partition to Core$^2$ and Core$^3$,
we can get the same level of recalls with much fewer samples.
In Core$^2$ we store the extension matrices for speeding up.
This is consistent with our theoretical analysis that in Core$^k$ sampling,
the focus on top entries is amplified,
making them more probable to be sampled.
All extensions are orders of magnitude faster than exhaustive computing.

\subsubsection{Use Prefiltering}
We then evaluate the performance of using $\predx$ for prefiltering with $t'=s/10$.
Due to space limit, we only show results on the lastfm data set in~\Fig{lastfm_budget}.
Compared with the results in~\Fig{recall} and~\Fig{time},
we notice that prefiltering leads to a slightly lower recall but less computation.
\begin{figure}[H]
	\centering
	\subfloat[a]{\includegraphics[width=3in]{./img/fig_lastfm_recall_budget}}
	\qquad
	\subfloat[b]{\includegraphics[width=2.7in]{./img/fig_lastfm_times_budget}}
	\caption{Recall and time of using prefiltering.
		(a) Recall (b) Time.
		We show the ratio of time used by $t'=s/10$ to that used by $t'=s$.}
	\label{fig:lastfm_budget}
\end{figure}
\subsubsection{Sampling for Collaborative Filtering}
In~\Fig{Queries}, we show the performance of~\Alg{QuerySampling}
applied for collaborative filtering,
where the samples are shared among different users.
Here Core$^2$ sampling is used to find the top-100 largest entries for the lastfm data set
and $s$ is set to $10^6$.
We can see that only for the first few users we need some time to get the samples
and build the sample pools.
After that, the computation time for a new user is much reduced.
\begin{figure}[!]
	\centering
	\includegraphics[width=2.7in]{./img/fig_lastfm_queries}\\
	\caption{Recall and time results of~\Alg{QuerySampling} for the lastfm data set.
		For better visualization, we show one result for every $10$ users.}
	\label{fig:Queries}
\end{figure}
\subsubsection{Comparison to Diamond Sampling}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=3in,viewport = 00 20 460 265]{./img/fig_lastfm_comparision_recall}\\
	\caption{Comparison with diamond sampling for the lastfm data set.}
	\label{fig:Comparison_recall}
\end{figure}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=2.7in,viewport = 0 10 450 180]{./img/fig_lastfm_comparison_times}\\
	\caption{Running time for the experiments in~\Fig{Comparison_recall}.}
	\label{fig:Comparison_time}
\end{figure}
We also compare our algorithms with diamond sampling~\cite{BaPiKoSe15}
for finding the top entries in the product of two matrices.
For each user, we multiply the vector characterizing it into the item matrix
to get a user-oriented item matrix.
The problem is to identify the top entries in the product of the item matrix and the tag matrix.
Note that the searchings for different users are conducted independently,
i.e. we run~\Alg{CoreExtensionSampling} instead of~\Alg{QuerySampling} for this experiment.
We show the recalls and the time consumed by the sampling algorithms for the lastfm data set
in~\Fig{Comparison_recall} and~\Fig{Comparison_time}.
$t'$ is set to $s/10$.
The performance of diamond sampling is better than Core$^1$ sampling.
This is reasonable since the score $\hat{x}_{\V{i}}$ it computed is proportional to the square of the entry magnitude.
However, Core$^2$ and Core$^3$ extensions achieve higher recalls than diamond sampling with the same number of samples.
This illustrates the advantage of augmenting the sampling probability to higher power of the entry magnitude.

\subsubsection{Accuracy of the Scores}
The $\predx$ is an estimation of $sx^k_{\V{i}}/\norm{\V{w}}{1}$ in Core$^k$ extension.
We experimentally validate this analysis.
We test on the lastfm data set with $k=3$ and $s=10^8$.
The $x$ and $y$ axes correspond to $\norm{\V{w}}{1}\hat{x}_{\V{i}}/s$ and $x^3_{\V{i}}$ respectively. 
Only the $10^4$ pairs with largest actual values that be sampled are shown in~\Figs{ml_2k_est}{delicious_est}.
\begin{figure}[H]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=3in]{./img/fig_ml_2k_est}\\
	\caption{Plot of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$.
		The deshed line is the reference for equality.}
	\label{fig:ml_2k_est}
\end{figure}
\begin{figure}[H]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=3in]{./img/fig_ml_10m_est}\\
	\caption{Plot of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$.
		The deshed line is the reference for equality.}
	\label{fig:ml_10m_est}
\end{figure}
\begin{figure}[H]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=3in]{./img/fig_lastfm_est}\\
	\caption{Plot of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$.
		The deshed line is the reference for equality.}
	\label{fig:lastfm_est}
\end{figure}
\begin{figure}[H]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=3in]{./img/fig_delicious_est}\\
	\caption{Plot of pairs $(\norm{\V{w}}{1}\hat{x}_{\V{i}}/s,x^3_{\V{i}})$.
		The deshed line is the reference for equality.}
	\label{fig:delicious_est}
\end{figure}

As expected, the points concentrate around the diagonal, 
which confirmed with the theoretical result.


\section{}
\section{Conclusion}




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\bibliography{IIP}
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}
%
%% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
%
%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage
%
%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


